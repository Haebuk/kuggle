{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pubg modeling.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haebuk/kuggle/blob/main/pubg_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LSUqzNEmUmE"
      },
      "source": [
        "### Reference\n",
        "- [competition](https://www.kaggle.com/c/pubg-finish-placement-prediction/code?competitionId=10335&sortBy=voteCount\n",
        ")\n",
        "- [1st place discussion](https://www.kaggle.com/c/pubg-finish-placement-prediction/discussion/79161)\n",
        "- [LigthGBM Baseline](https://www.kaggle.com/chocozzz/lightgbm-baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6e2UQhqtmQ3"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJOppHQwMgKx",
        "outputId": "eb16b19a-9c12-4ab5-a846-3fd76bf94137"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxj5Z_vXmUmG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import gc, sys\n",
        "gc.enable()\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCHbwJPthPK"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmwRHXjgmUmH"
      },
      "source": [
        "def feature_engineering(is_train=True, debug=True):\n",
        "    test_idx = None\n",
        "    if is_train:\n",
        "        print('preprocessing train.csv')\n",
        "        df = pd.read_csv('/content/drive/MyDrive/input/pubg/train_V2.csv')\n",
        "        df = df[df['maxPlace'] > 1]\n",
        "    else:\n",
        "        print('processing test.csv')\n",
        "        df = pd.read_csv('/content/drive/MyDrive/input/pubg/test_V2.csv')\n",
        "        test_idx = df.Id\n",
        "\n",
        "    target = 'winPlacePerc'\n",
        "\n",
        "    print('Adding Features')\n",
        "\n",
        "    df['headshotrate'] = df['kills']/df['headshotKills'] #  \n",
        "    df['killStreakrate'] = df['killStreaks']/df['kills'] # 킬스트릭의 비율\n",
        "    df['healthitems'] = df['heals'] + df['boosts'] # 힐 아이템을 사용한 개수\n",
        "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"] # 총 이동 거리\n",
        "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace'] # maxPlace 대비 killPlace\n",
        "    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills'] # 헤드샷으로 kill을 낸 비율\n",
        "    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired'] # 획득한 무기 대비 이동 거리\n",
        "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals'] # 힐 아이템 사용 대비 걸은 거리\n",
        "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills'] # 킬 대비 걸은 거리\n",
        "    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance'] # 걸은 거리 대비 킬\n",
        "    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"] # 헤드샷 킬 + 로드 킬\n",
        "\n",
        "    df[df == np.Inf] = np.NaN\n",
        "    df[df == np.NINF] = np.NaN\n",
        "\n",
        "    print(\"Removing Na's From DF\")\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    features = list(df.columns)\n",
        "    features.remove('Id')\n",
        "    features.remove('matchId')\n",
        "    features.remove('groupId')\n",
        "    features.remove('matchType')\n",
        "\n",
        "    y = None\n",
        "\n",
        "    if is_train:\n",
        "        print('get target') # target = 'winPlacePerc'\n",
        "        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n",
        "        features.remove(target)\n",
        "\n",
        "    print('get group mean feature')\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    \n",
        "    if is_train:\n",
        "        df_out = agg.reset_index()[['matchId', 'groupId']]\n",
        "    else:\n",
        "        df_out = df[['matchId', 'groupId']]\n",
        "\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank,  suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "    print('get group max feature')\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].agg('max')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "    print('get group min feature')\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].agg('min')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "    print('get group size feature')\n",
        "    agg = df.groupby(['matchId', 'groupId']).size().reset_index(name='group_size')\n",
        "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "    print('get match mean feature')\n",
        "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
        "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
        "\n",
        "    print('get match size feature')\n",
        "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
        "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
        "\n",
        "    df_out.drop(['matchId', 'groupId'], axis=1, inplace=True)\n",
        "\n",
        "    X = df_out\n",
        "\n",
        "    feature_names = list(df_out.columns)\n",
        "\n",
        "    del df, df_out, agg, agg_rank\n",
        "    gc.collect()\n",
        "\n",
        "    return X, y, feature_names, test_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fSffymLmUmI",
        "outputId": "2cba739f-91b5-4ed3-90a2-9ca1bd9a5f55"
      },
      "source": [
        "x_train, y_train, train_columns, _ = feature_engineering(True, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing train.csv\n",
            "Adding Features\n",
            "Removing Na's From DF\n",
            "get target\n",
            "get group mean feature\n",
            "get group max feature\n",
            "get group min feature\n",
            "get group size feature\n",
            "get match mean feature\n",
            "get match size feature\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T96C-8p8mUmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f429da3a-d516-4606-f1e9-c0e4cd07fe9c"
      },
      "source": [
        "x_test, _, _, test_idx = feature_engineering(False, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing test.csv\n",
            "Adding Features\n",
            "Removing Na's From DF\n",
            "get group mean feature\n",
            "get group max feature\n",
            "get group min feature\n",
            "get group size feature\n",
            "get match mean feature\n",
            "get match size feature\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87c8bAsqmUmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e5ec5e-c313-4b65-e44b-5b80f70d4289"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum()\n",
        "    print(\"Memory usage of dataframe in {:.2f} MB\",format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                # np.iinfo: 정수형 타입의 데이터에 명시한 데이터 타입만큼의 메모리 할당\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "                # np.finfo\n",
        "                elif c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "            else:\n",
        "                df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum()\n",
        "    print('Memory Usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df\n",
        "\n",
        "x_train = reduce_mem_usage(x_train)\n",
        "x_test = reduce_mem_usage(x_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe in {:.2f} MB 4021060096\n",
            "Memory Usage after optimization is: 1348624632.00 MB\n",
            "Decreased by 66.5%\n",
            "Memory usage of dataframe in {:.2f} MB 3837401216\n",
            "Memory Usage after optimization is: 1113111432.00 MB\n",
            "Decreased by 71.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idWEE1iWH46b"
      },
      "source": [
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFyv05J33EJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d48e0f-a2b7-4a4e-da5d-03ffa4db5005"
      },
      "source": [
        "train_index = round(int(x_train.shape[0] * 0.8))\n",
        "tr_X = x_train[:train_index]\n",
        "val_X= x_train[train_index:]\n",
        "tr_y = y_train[:train_index]\n",
        "val_y = y_train[train_index:]\n",
        "gc.collect()\n",
        "\n",
        "# lightgbm model custom\n",
        "def run_lgb(train_X, train_y, val_X, val_y, x_Ztest):\n",
        "    params = {'objective': 'regression',\n",
        "              'metric': 'mae',\n",
        "              'n_estimators': 20000,\n",
        "              'early_stopping_rounds': 200,\n",
        "              'num_leaves': 31,\n",
        "              'learning_rate': 0.05,\n",
        "              'bagging_fraction': 0.7,\n",
        "              'bagging_seed': 0,\n",
        "              'num_threads': 4,\n",
        "              'colsample_bytree': 0.7}\n",
        "\n",
        "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
        "    lgval = lgb.Dataset(val_X, label=val_y)\n",
        "    model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval],\n",
        "                      early_stopping_rounds=200, verbose_eval=1000)\n",
        "    pred_test_y = model.predict(x_test, num_iteration=model.best_iteration)\n",
        "    return pred_test_y, model\n",
        "\n",
        "# model training\n",
        "pred_test, model = run_lgb(tr_X, tr_y, val_X, val_y, x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds.\n",
            "[1000]\ttraining's l1: 0.0324754\tvalid_1's l1: 0.03865\n",
            "[2000]\ttraining's l1: 0.0283006\tvalid_1's l1: 0.0381183\n",
            "[3000]\ttraining's l1: 0.025347\tvalid_1's l1: 0.0378768\n",
            "[4000]\ttraining's l1: 0.0230365\tvalid_1's l1: 0.0376937\n",
            "Early stopping, best iteration is:\n",
            "[4144]\ttraining's l1: 0.0226953\tvalid_1's l1: 0.0376596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwOKmNup6Yys"
      },
      "source": [
        "df_sub = pd.read_csv('/content/drive/MyDrive/input/pubg/sample_submission_V2.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/input/pubg/test_V2.csv')\n",
        "df_sub['winPlacePerc'] = pred_test\n",
        "# 몇 가지 열 복원\n",
        "df_sub = df_sub.merge(df_test[['Id', 'matchId', 'groupId', 'maxPlace', 'numGroups']],\n",
        "                      on='Id', how='left')\n",
        "\n",
        "#\n",
        "df_sub_group = df_sub.groupby(['matchId', 'groupId']).first().reset_index()\n",
        "df_sub_group['rank'] = df_sub.groupby(['matchId'])['winPlacePerc'].rank()\n",
        "df_sub_group = df_sub_group.merge(\n",
        "    df_sub_group.groupby('matchId')['rank'].max().to_frame('max_rank').reset_index(),\n",
        "    on='matchId', how='left'\n",
        ")\n",
        "df_sub_group['adjusted_perc'] = (df_sub_group['rank'] - 1) / (df_sub_group['numGroups'] - 1)\n",
        "\n",
        "df_sub = df_sub.merge(df_sub_group[['adjusted_perc', 'matchId', 'groupId']],\n",
        "                      on = ['matchId', 'groupId'], how='left')\n",
        "df_sub['winPlacePerc'] = df_sub['adjusted_perc']\n",
        "\n",
        "df_sub.loc[df_sub.maxPlace == 0, 'winPlacePerc'] = 0\n",
        "df_sub.loc[df_sub.maxPlace == 1, 'winPlacePerc'] = 1\n",
        "\n",
        "subset = df_sub.loc[df_sub.maxPlace > 1]\n",
        "gap = 1.0 / (subset.maxPlace.values - 1)\n",
        "new_perc = np.around(subset.winPlacePerc.values / gap) * gap\n",
        "df_sub.loc[df_sub.maxPlace > 1, 'winPlacePerc'] = new_perc\n",
        "\n",
        "df_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), 'winPlacePerc'] = 0\n",
        "assert df_sub['winPlacePerc'].isnull().sum() == 0\n",
        "\n",
        "df_sub[['Id', 'winPlacePerc']].to_csv('submission_adjusted.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSIMet3ew4XZ"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/68543150/119942983-62b28680-bfcd-11eb-8894-e61e85cdf9e5.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQAtNTt7MucM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}