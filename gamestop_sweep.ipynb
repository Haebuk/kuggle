{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gamestop_sweep.ipynb",
      "provenance": [],
      "mount_file_id": "1a83B0yCBlvhZMEo4JR4YwX8O5z3qbWji",
      "authorship_tag": "ABX9TyOJqAo3VkSEpH1njNbQPKSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haebuk/kuggle/blob/main/gamestop_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB0meTHJItzV",
        "outputId": "ecf35b84-32c6-4abd-a4d6-a9bb70d79624"
      },
      "source": [
        "%pip install wandb -qqq"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6 MB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 56.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cev3goNIwrD",
        "outputId": "0ace454c-25fb-4850-9ac3-f64e8bb6719a"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6CSy3R4Hp44"
      },
      "source": [
        "def train():\n",
        "    import wandb \n",
        "    from wandb.keras import WandbCallback\n",
        "    import tensorflow as tf\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import LSTM, Dense, Dropout\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/input/'\n",
        "    df = pd.read_csv(PATH + 'GME_scaled.csv')\n",
        "\n",
        "    default_configs = {\n",
        "        'learning_rate': 0.001,\n",
        "        'dropout_rate1': 0.2,\n",
        "        'dropout_rate2': 0.2,\n",
        "        'dropout_rate3': 0.2,\n",
        "        'hidden1': 50,\n",
        "        'hidden2': 50,\n",
        "        'hidden3': 50,\n",
        "        'time_step': 60,\n",
        "    }\n",
        "    wandb.init(project='gamestop_prediction', config=default_configs, magic=True)\n",
        "    config = wandb.config\n",
        "\n",
        "    X = df.drop('date', axis=1).values\n",
        "    y = df['open_price'].values\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "    time_step = config.time_step\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "    for i in range(time_step,len(X_tr)): # train\n",
        "        X_train.append(X_tr[i-time_step:i,:])\n",
        "        y_train.append(y_tr[i])\n",
        "    for i in range(time_step, len(X_te)): # test\n",
        "        X_test.append(X_te[i-time_step:i,:])\n",
        "        y_test.append(y_te[i])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "    # LSTM input shape 조건(batch_size, timestep, feature_num)에 맞게 reshape\n",
        "    X_train = np.reshape(X_train, (-1,X_train.shape[1],2))\n",
        "    X_test = np.reshape(X_test, (-1,X_test.shape[1],2))\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "    LSTM(config.hidden1, return_sequences=True, input_shape=(config.time_step, 2)),\n",
        "    Dropout(config.dropout_rate1),\n",
        "    LSTM(config.hidden2, return_sequences=True),\n",
        "    Dropout(config.dropout_rate2),\n",
        "    LSTM(config.hidden3),\n",
        "    Dropout(config.dropout_rate3),\n",
        "    Dense(1),\n",
        "    ])\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    model.compile(optimizer=opt,\n",
        "                loss='mse',\n",
        "                metrics=['mse'])\n",
        "    \n",
        "    model.fit(X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=200, callbacks=[WandbCallback(), early_stopping])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHDzW4x4IfXJ"
      },
      "source": [
        "with open('train.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import wandb \n",
        "from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "PATH = '/content/drive/MyDrive/input/'\n",
        "df = pd.read_csv(PATH + 'GME_scaled.csv')\n",
        "\n",
        "default_configs = {\n",
        "    'learning_rate': 0.001,\n",
        "    'dropout_rate1': 0.2,\n",
        "    'dropout_rate2': 0.2,\n",
        "    'dropout_rate3': 0.2,\n",
        "    'hidden1': 50,\n",
        "    'hidden2': 50,\n",
        "    'hidden3': 50,\n",
        "    'time_step': 60,\n",
        "}\n",
        "wandb.init(project='gamestop_prediction', config=default_configs, magic=True)\n",
        "config = wandb.config\n",
        "\n",
        "X = df.drop('date', axis=1).values\n",
        "y = df['open_price'].values\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "time_step = config.time_step\n",
        "X_train, y_train, X_test, y_test = [], [], [], []\n",
        "for i in range(time_step,len(X_tr)): # train\n",
        "    X_train.append(X_tr[i-time_step:i,:])\n",
        "    y_train.append(y_tr[i])\n",
        "for i in range(time_step, len(X_te)): # test\n",
        "    X_test.append(X_te[i-time_step:i,:])\n",
        "    y_test.append(y_te[i])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "# LSTM input shape 조건(batch_size, timestep, feature_num)에 맞게 reshape\n",
        "X_train = np.reshape(X_train, (-1,X_train.shape[1],2))\n",
        "X_test = np.reshape(X_test, (-1,X_test.shape[1],2))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "LSTM(config.hidden1, return_sequences=True, input_shape=(config.time_step, 2)),\n",
        "Dropout(config.dropout_rate1),\n",
        "LSTM(config.hidden2, return_sequences=True),\n",
        "Dropout(config.dropout_rate2),\n",
        "LSTM(config.hidden3),\n",
        "Dropout(config.dropout_rate3),\n",
        "Dense(1),\n",
        "])\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "            loss='mse',\n",
        "            metrics=['mse'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=200, callbacks=[WandbCallback(), early_stopping])\n",
        "    \"\"\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmZV6rSNJK8P",
        "outputId": "5270bd98-f893-41a1-ff1d-78f3326e54a4"
      },
      "source": [
        "!head train.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "import wandb \n",
            "from wandb.keras import WandbCallback\n",
            "import tensorflow as tf\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from keras.models import Sequential\n",
            "from keras.layers import LSTM, Dense, Dropout\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oG_tPIuJLnN",
        "outputId": "b4feede2-410f-43b3-9aae-db928f4e7125"
      },
      "source": [
        "!wandb agent kade/gamestop_prediction/47ty4gib"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "\tdropout_rate1: 0.16478566479409007\n",
            "\tdropout_rate2: 0.056896766413966106\n",
            "\tdropout_rate3: 0.38967783541929096\n",
            "\thidden1: 81\n",
            "\thidden2: 89\n",
            "\thidden3: 100\n",
            "\tlearning_rate: 0.007916615730729864\n",
            "\ttime_step: 25\n",
            "2021-08-19 14:15:46,348 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.16478566479409007 --dropout_rate2=0.056896766413966106 --dropout_rate3=0.38967783541929096 --hidden1=81 --hidden2=89 --hidden3=100 --learning_rate=0.007916615730729864 --time_step=25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-sweep-583\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/b69srkog\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_141548-b69srkog\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:15:51.005466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51,354 - wandb.wandb_agent - INFO - Running runs: ['b69srkog']\n",
            "2021-08-19 14:15:51.014165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.014871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.016140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.016852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.017522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.680297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.681283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.682221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:15:51.683076: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:15:51.683147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:15:51.723520: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:15:51.723556: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:15:51.723597: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:15:51.867554: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:15:51.867721: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:15:52.063011: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:15:56.045840: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 8.6106 - mse: 8.61062021-08-19 14:15:56.679541: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:15:56.679584: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 4.4195 - mse: 4.4195 2021-08-19 14:15:56.947445: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:15:56.948086: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:15:57.102807: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 647 callback api events and 644 activity events. \n",
            "2021-08-19 14:15:57.111273: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:15:57.124150: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57\n",
            "\n",
            "2021-08-19 14:15:57.133155: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:15:57.149989: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57\n",
            "\n",
            "2021-08-19 14:15:57.153120: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:15:57.153674: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_141548-b69srkog/files/train/plugins/profile/2021_08_19_14_15_57/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 5.1464 - mse: 5.1464WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_begin` time: 0.0423s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.0582s). Check your callbacks.\n",
            "76/82 [==========================>...] - ETA: 0s - loss: 0.6793 - mse: 0.6793\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.6694 - mse: 0.6694 - val_loss: 2.2401 - val_mse: 2.2401\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5149 - mse: 0.5149 - val_loss: 2.3299 - val_mse: 2.3299\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4481 - mse: 0.4481 - val_loss: 2.6886 - val_mse: 2.6886\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4502 - mse: 0.4502 - val_loss: 2.2504 - val_mse: 2.2504\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2869 - mse: 0.2869 - val_loss: 1.0533 - val_mse: 1.0533\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.1047 - mse: 0.1047 - val_loss: 0.8317 - val_mse: 0.8317\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.0938 - mse: 0.0938 - val_loss: 1.0957 - val_mse: 1.0957\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0899 - mse: 0.0899 - val_loss: 0.9134 - val_mse: 0.9134\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0725 - mse: 0.0725 - val_loss: 0.6843 - val_mse: 0.6843\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0710 - mse: 0.0710 - val_loss: 0.8694 - val_mse: 0.8694\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0674 - mse: 0.0674 - val_loss: 0.9057 - val_mse: 0.9057\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0611 - mse: 0.0611 - val_loss: 0.9359 - val_mse: 0.9359\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0601 - mse: 0.0601 - val_loss: 0.7420 - val_mse: 0.7420\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0540 - mse: 0.0540 - val_loss: 0.8398 - val_mse: 0.8398\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.9136 - val_mse: 0.9136\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.8413 - val_mse: 0.8413\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.7644 - val_mse: 0.7644\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.7978 - val_mse: 0.7978\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.8560 - val_mse: 0.8560\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 133720\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_141548-b69srkog/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_141548-b69srkog/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.03697\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.03697\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.85602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.85602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382570\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.6843\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▆▆▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▆▆▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▆▇█▆▂▂▂▂▁▂▂▂▁▂▂▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▆▇█▆▂▂▂▂▁▂▂▂▁▂▂▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▂▃▃▃▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▂▃▃▃▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mearnest-sweep-583\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/b69srkog\u001b[0m\n",
            "2021-08-19 14:16:21,874 - wandb.wandb_agent - INFO - Cleaning up finished run: b69srkog\n",
            "2021-08-19 14:16:22,249 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:16:22,249 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.06913616081086814\n",
            "\tdropout_rate2: 0.4116195844295091\n",
            "\tdropout_rate3: 0.49171026600165235\n",
            "\thidden1: 87\n",
            "\thidden2: 110\n",
            "\thidden3: 93\n",
            "\tlearning_rate: 0.010422004378517358\n",
            "\ttime_step: 11\n",
            "2021-08-19 14:16:22,251 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.06913616081086814 --dropout_rate2=0.4116195844295091 --dropout_rate3=0.49171026600165235 --hidden1=87 --hidden2=110 --hidden3=93 --learning_rate=0.010422004378517358 --time_step=11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-sweep-584\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/7n9657ie\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_141624-7n9657ie\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:16:27.013089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27,262 - wandb.wandb_agent - INFO - Running runs: ['7n9657ie']\n",
            "2021-08-19 14:16:27.022229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.023146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.024863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.026405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.027254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.685302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.685989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.686594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:16:27.687114: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:16:27.687172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:16:27.716483: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:16:27.716514: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:16:27.716547: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:16:27.851152: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:16:27.851311: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:16:28.036603: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:16:32.002427: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:00 - loss: 9.7719 - mse: 9.77192021-08-19 14:16:32.645039: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:16:32.645082: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.0686 - mse: 5.0686 2021-08-19 14:16:32.907291: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:16:32.907796: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:16:33.056805: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 460 callback api events and 457 activity events. \n",
            "2021-08-19 14:16:33.064661: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:16:33.075124: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33\n",
            "\n",
            "2021-08-19 14:16:33.082661: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:16:33.097908: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33\n",
            "\n",
            "2021-08-19 14:16:33.100941: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:16:33.101910: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_141624-7n9657ie/files/train/plugins/profile/2021_08_19_14_16_33/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 4.8233 - mse: 4.8233WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_begin` time: 0.0417s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0582s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 0.7641 - mse: 0.7641 - val_loss: 2.2080 - val_mse: 2.2080\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.5180 - mse: 0.5180 - val_loss: 2.4562 - val_mse: 2.4562\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3196 - mse: 0.3196 - val_loss: 0.8845 - val_mse: 0.8845\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1560 - mse: 0.1560 - val_loss: 0.8947 - val_mse: 0.8947\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1239 - mse: 0.1239 - val_loss: 0.7631 - val_mse: 0.7631\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1114 - mse: 0.1114 - val_loss: 0.8792 - val_mse: 0.8792\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.9225 - val_mse: 0.9225\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0823 - mse: 0.0823 - val_loss: 0.6918 - val_mse: 0.6918\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0726 - mse: 0.0726 - val_loss: 0.8506 - val_mse: 0.8506\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0634 - mse: 0.0634 - val_loss: 0.6334 - val_mse: 0.6334\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.7615 - val_mse: 0.7615\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0478 - mse: 0.0478 - val_loss: 0.7757 - val_mse: 0.7757\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.7298 - val_mse: 0.7298\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.6577 - val_mse: 0.6577\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.8293 - val_mse: 0.8293\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.6232 - val_mse: 0.6232\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.5842 - val_mse: 0.5842\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.6814 - val_mse: 0.6814\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.5520 - val_mse: 0.5520\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.5826 - val_mse: 0.5826\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.6015 - val_mse: 0.6015\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.5440 - val_mse: 0.5440\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.5510 - val_mse: 0.5510\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.5850 - val_mse: 0.5850\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.6701 - val_mse: 0.6701\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.5568 - val_mse: 0.5568\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.6315 - val_mse: 0.6315\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.6070 - val_mse: 0.6070\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.6400 - val_mse: 0.6400\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.5561 - val_mse: 0.5561\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.5532 - val_mse: 0.5532\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.5401 - val_mse: 0.5401\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.5672 - val_mse: 0.5672\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.5964 - val_mse: 0.5964\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.5690 - val_mse: 0.5690\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.5541 - val_mse: 0.5541\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.6213 - val_mse: 0.6213\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.5037 - val_mse: 0.5037\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.5377 - val_mse: 0.5377\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.5350 - val_mse: 0.5350\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.5176 - val_mse: 0.5176\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.5129 - val_mse: 0.5129\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.5272 - val_mse: 0.5272\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.5021 - val_mse: 0.5021\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.5872 - val_mse: 0.5872\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.5254 - val_mse: 0.5254\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.4477 - val_mse: 0.4477\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.4833 - val_mse: 0.4833\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.4499 - val_mse: 0.4499\n",
            "Epoch 50/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.4089 - val_mse: 0.4089\n",
            "Epoch 51/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.4315 - val_mse: 0.4315\n",
            "Epoch 52/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.3810 - val_mse: 0.3810\n",
            "Epoch 53/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.3937 - val_mse: 0.3937\n",
            "Epoch 54/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.4427 - val_mse: 0.4427\n",
            "Epoch 55/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.4280 - val_mse: 0.4280\n",
            "Epoch 56/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.4564 - val_mse: 0.4564\n",
            "Epoch 57/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.4685 - val_mse: 0.4685\n",
            "Epoch 58/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.4514 - val_mse: 0.4514\n",
            "Epoch 59/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.3495 - val_mse: 0.3495\n",
            "Epoch 60/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.5165 - val_mse: 0.5165\n",
            "Epoch 61/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.4678 - val_mse: 0.4678\n",
            "Epoch 62/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.4816 - val_mse: 0.4816\n",
            "Epoch 63/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.4949 - val_mse: 0.4949\n",
            "Epoch 64/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.4776 - val_mse: 0.4776\n",
            "Epoch 65/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.4742 - val_mse: 0.4742\n",
            "Epoch 66/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.5004 - val_mse: 0.5004\n",
            "Epoch 67/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.4953 - val_mse: 0.4953\n",
            "Epoch 68/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.6258 - val_mse: 0.6258\n",
            "Epoch 69/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.5269 - val_mse: 0.5269\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 133901\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_141624-7n9657ie/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_141624-7n9657ie/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 68\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.00775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.00775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.52689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.52689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 68\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.34951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▆▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▆▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▇█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▇█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mefficient-sweep-584\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/7n9657ie\u001b[0m\n",
            "2021-08-19 14:17:58,668 - wandb.wandb_agent - INFO - Cleaning up finished run: 7n9657ie\n",
            "2021-08-19 14:17:59,216 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:17:59,216 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3288428394772016\n",
            "\tdropout_rate2: 0.24248108330815543\n",
            "\tdropout_rate3: 0.40344547126890573\n",
            "\thidden1: 86\n",
            "\thidden2: 27\n",
            "\thidden3: 76\n",
            "\tlearning_rate: 0.0024768437272104857\n",
            "\ttime_step: 5\n",
            "2021-08-19 14:17:59,218 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3288428394772016 --dropout_rate2=0.24248108330815543 --dropout_rate3=0.40344547126890573 --hidden1=86 --hidden2=27 --hidden3=76 --learning_rate=0.0024768437272104857 --time_step=5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-sweep-585\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/mn4b07zu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_141801-mn4b07zu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:18:03.963511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:04,229 - wandb.wandb_agent - INFO - Running runs: ['mn4b07zu']\n",
            "2021-08-19 14:18:03.971321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:03.971982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:03.973485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:03.974102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:03.974784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:04.614635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:04.615264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:04.615906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:04.616439: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:18:04.616494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:18:04.647091: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:18:04.647124: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:18:04.647169: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:18:04.784131: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:18:04.784342: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:18:04.966347: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:18:08.954616: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 10.4260 - mse: 10.42602021-08-19 14:18:09.587018: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:18:09.587065: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 10.0101 - mse: 10.0101 2021-08-19 14:18:09.847040: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:18:09.847501: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:18:10.005449: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 377 callback api events and 374 activity events. \n",
            "2021-08-19 14:18:10.012929: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:18:10.023694: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10\n",
            "\n",
            "2021-08-19 14:18:10.031857: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:18:10.049643: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10\n",
            "\n",
            "2021-08-19 14:18:10.053895: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:18:10.054717: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_141801-mn4b07zu/files/train/plugins/profile/2021_08_19_14_18_10/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 9.4812 - mse: 9.4812  WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_begin` time: 0.0414s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0580s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 27ms/step - loss: 1.0249 - mse: 1.0249 - val_loss: 0.8450 - val_mse: 0.8450\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1541 - mse: 0.1541 - val_loss: 0.4956 - val_mse: 0.4956\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1386 - mse: 0.1386 - val_loss: 0.5196 - val_mse: 0.5196\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1180 - mse: 0.1180 - val_loss: 0.4827 - val_mse: 0.4827\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1157 - mse: 0.1157 - val_loss: 0.4594 - val_mse: 0.4594\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1088 - mse: 0.1088 - val_loss: 0.5244 - val_mse: 0.5244\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1049 - mse: 0.1049 - val_loss: 0.3715 - val_mse: 0.3715\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.5192 - val_mse: 0.5192\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.3800 - val_mse: 0.3800\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0949 - mse: 0.0949 - val_loss: 0.4725 - val_mse: 0.4725\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0942 - mse: 0.0942 - val_loss: 0.4017 - val_mse: 0.4017\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.6077 - val_mse: 0.6077\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0882 - mse: 0.0882 - val_loss: 0.5183 - val_mse: 0.5183\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0912 - mse: 0.0912 - val_loss: 0.5795 - val_mse: 0.5795\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.4393 - val_mse: 0.4393\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0881 - mse: 0.0881 - val_loss: 0.5008 - val_mse: 0.5008\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.7012 - val_mse: 0.7012\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 134336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_141801-mn4b07zu/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_141801-mn4b07zu/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.08364\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.08364\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.70116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.70116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382700\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.37149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▃▃▃▂▃▁▃▁▂▁▄▃▄▂▃▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▃▃▃▂▃▁▃▁▂▁▄▃▄▂▃▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▃▃▃▃▄▄▅▅▆▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▃▃▃▃▄▄▅▅▆▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdevoted-sweep-585\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/mn4b07zu\u001b[0m\n",
            "2021-08-19 14:18:34,723 - wandb.wandb_agent - INFO - Cleaning up finished run: mn4b07zu\n",
            "2021-08-19 14:18:35,416 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:18:35,416 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.16358089816467766\n",
            "\tdropout_rate2: 0.48495419657094724\n",
            "\tdropout_rate3: 0.17590283219472125\n",
            "\thidden1: 113\n",
            "\thidden2: 80\n",
            "\thidden3: 63\n",
            "\tlearning_rate: 0.0942370828193193\n",
            "\ttime_step: 75\n",
            "2021-08-19 14:18:35,418 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.16358089816467766 --dropout_rate2=0.48495419657094724 --dropout_rate3=0.17590283219472125 --hidden1=113 --hidden2=80 --hidden3=63 --learning_rate=0.0942370828193193 --time_step=75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-sweep-586\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/fmcyn570\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_141838-fmcyn570\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:18:40.190206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40,428 - wandb.wandb_agent - INFO - Running runs: ['fmcyn570']\n",
            "2021-08-19 14:18:40.197773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.198411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.199592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.200189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.200777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.835831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.836540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.837125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:18:40.837695: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:18:40.837746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:18:40.866834: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:18:40.866873: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:18:40.866961: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:18:41.002060: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:18:41.002230: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:18:41.182920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:18:45.120259: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/80 [..............................] - ETA: 5:50 - loss: 9.6128 - mse: 9.61282021-08-19 14:18:45.774734: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:18:45.774778: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/80 [..............................] - ETA: 31s - loss: 15.5437 - mse: 15.54372021-08-19 14:18:46.043900: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:18:46.044808: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:18:46.199244: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1303 callback api events and 1300 activity events. \n",
            "2021-08-19 14:18:46.213985: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:18:46.237961: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46\n",
            "\n",
            "2021-08-19 14:18:46.252981: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:18:46.273782: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46\n",
            "\n",
            "2021-08-19 14:18:46.276587: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:18:46.277215: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_141838-fmcyn570/files/train/plugins/profile/2021_08_19_14_18_46/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/80 [>.............................] - ETA: 25s - loss: 11.1516 - mse: 11.1516WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_begin` time: 0.0408s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0666s). Check your callbacks.\n",
            "80/80 [==============================] - 7s 33ms/step - loss: 1.2057 - mse: 1.2057 - val_loss: 2.2008 - val_mse: 2.2008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4391 - mse: 0.4391 - val_loss: 2.4064 - val_mse: 2.4064\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4463 - mse: 0.4463 - val_loss: 2.6055 - val_mse: 2.6055\n",
            "Epoch 4/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4171 - mse: 0.4171 - val_loss: 2.0148 - val_mse: 2.0148\n",
            "Epoch 5/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4136 - mse: 0.4136 - val_loss: 3.1162 - val_mse: 3.1162\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4016 - mse: 0.4016 - val_loss: 2.4209 - val_mse: 2.4209\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3936 - mse: 0.3936 - val_loss: 2.4691 - val_mse: 2.4691\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3870 - mse: 0.3870 - val_loss: 2.2634 - val_mse: 2.2634\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3665 - mse: 0.3665 - val_loss: 2.1619 - val_mse: 2.1619\n",
            "Epoch 10/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3907 - mse: 0.3907 - val_loss: 2.8776 - val_mse: 2.8776\n",
            "Epoch 11/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3857 - mse: 0.3857 - val_loss: 3.5265 - val_mse: 3.5265\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4002 - mse: 0.4002 - val_loss: 2.5106 - val_mse: 2.5106\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4642 - mse: 0.4642 - val_loss: 2.5889 - val_mse: 2.5889\n",
            "Epoch 14/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3905 - mse: 0.3905 - val_loss: 2.9865 - val_mse: 2.9865\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 134507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_141838-fmcyn570/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_141838-fmcyn570/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.39047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.39047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.98646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.98646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382742\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.01483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▂▂▃▃▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▂▂▁▁▁▁▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▂▃▄▁▆▃▃▂▂▅█▃▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▂▃▄▁▆▃▃▂▂▅█▃▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▃▃▃▄▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▃▃▃▄▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▂▂▃▃▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mpleasant-sweep-586\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/fmcyn570\u001b[0m\n",
            "2021-08-19 14:19:10,929 - wandb.wandb_agent - INFO - Cleaning up finished run: fmcyn570\n",
            "2021-08-19 14:19:11,401 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:19:11,401 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.4844993544008065\n",
            "\tdropout_rate2: 0.21218458710489319\n",
            "\tdropout_rate3: 0.3534856728384131\n",
            "\thidden1: 10\n",
            "\thidden2: 95\n",
            "\thidden3: 22\n",
            "\tlearning_rate: 0.014316971086531318\n",
            "\ttime_step: 7\n",
            "2021-08-19 14:19:11,402 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.4844993544008065 --dropout_rate2=0.21218458710489319 --dropout_rate3=0.3534856728384131 --hidden1=10 --hidden2=95 --hidden3=22 --learning_rate=0.014316971086531318 --time_step=7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-sweep-587\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/dvjdeh0b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_141913-dvjdeh0b\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:19:16.017600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.024692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.025291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16,412 - wandb.wandb_agent - INFO - Running runs: ['dvjdeh0b']\n",
            "2021-08-19 14:19:16.026513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.027137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.027735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.645670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.646699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.647578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:19:16.648338: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:19:16.648405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:19:16.693601: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:19:16.693675: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:19:16.693730: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:19:16.839381: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:19:16.839550: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:19:17.025768: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:19:20.995714: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:00 - loss: 11.1449 - mse: 11.14492021-08-19 14:19:21.633869: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:19:21.633913: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 8.8463 - mse: 8.8463   2021-08-19 14:19:21.893913: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:19:21.894384: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:19:22.043931: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 406 callback api events and 403 activity events. \n",
            "2021-08-19 14:19:22.050256: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:19:22.060103: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22\n",
            "\n",
            "2021-08-19 14:19:22.067020: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:19:22.084910: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22\n",
            "\n",
            "2021-08-19 14:19:22.089009: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:19:22.089667: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_141913-dvjdeh0b/files/train/plugins/profile/2021_08_19_14_19_22/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 6.6561 - mse: 6.6561WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0075s vs `on_train_batch_begin` time: 0.0414s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_train_batch_end` time: 0.0572s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 0.9294 - mse: 0.9294 - val_loss: 2.2675 - val_mse: 2.2675\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4698 - mse: 0.4698 - val_loss: 1.5730 - val_mse: 1.5730\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2508 - mse: 0.2508 - val_loss: 1.2276 - val_mse: 1.2276\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.9588 - val_mse: 0.9588\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1496 - mse: 0.1496 - val_loss: 0.8903 - val_mse: 0.8903\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1133 - mse: 0.1133 - val_loss: 0.7656 - val_mse: 0.7656\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.6817 - val_mse: 0.6817\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0780 - mse: 0.0780 - val_loss: 0.7994 - val_mse: 0.7994\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0645 - mse: 0.0645 - val_loss: 1.0232 - val_mse: 1.0232\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0571 - mse: 0.0571 - val_loss: 0.7207 - val_mse: 0.7207\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.7992 - val_mse: 0.7992\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.8147 - val_mse: 0.8147\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.7590 - val_mse: 0.7590\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.7126 - val_mse: 0.7126\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.7580 - val_mse: 0.7580\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.6592 - val_mse: 0.6592\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.8160 - val_mse: 0.8160\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.7038 - val_mse: 0.7038\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.7159 - val_mse: 0.7159\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.6508 - val_mse: 0.6508\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.7750 - val_mse: 0.7750\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.6764 - val_mse: 0.6764\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.6558 - val_mse: 0.6558\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.7223 - val_mse: 0.7223\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.6485 - val_mse: 0.6485\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.5814 - val_mse: 0.5814\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.5946 - val_mse: 0.5946\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.6171 - val_mse: 0.6171\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.6332 - val_mse: 0.6332\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.6705 - val_mse: 0.6705\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.5519 - val_mse: 0.5519\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.6920 - val_mse: 0.6920\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.5820 - val_mse: 0.5820\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.6168 - val_mse: 0.6168\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.6582 - val_mse: 0.6582\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.6474 - val_mse: 0.6474\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5809 - val_mse: 0.5809\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.6255 - val_mse: 0.6255\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.6173 - val_mse: 0.6173\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.6678 - val_mse: 0.6678\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.6594 - val_mse: 0.6594\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 134663\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_141913-dvjdeh0b/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_141913-dvjdeh0b/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01776\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01776\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.65938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.65938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.55189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▅▄▃▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▅▄▃▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdandy-sweep-587\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/dvjdeh0b\u001b[0m\n",
            "2021-08-19 14:19:56,198 - wandb.wandb_agent - INFO - Cleaning up finished run: dvjdeh0b\n",
            "2021-08-19 14:19:56,585 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:19:56,585 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.0755792651809551\n",
            "\tdropout_rate2: 0.34099184256377607\n",
            "\tdropout_rate3: 0.3318665235752542\n",
            "\thidden1: 26\n",
            "\thidden2: 79\n",
            "\thidden3: 80\n",
            "\tlearning_rate: 0.046213855417462\n",
            "\ttime_step: 11\n",
            "2021-08-19 14:19:56,587 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.0755792651809551 --dropout_rate2=0.34099184256377607 --dropout_rate3=0.3318665235752542 --hidden1=26 --hidden2=79 --hidden3=80 --learning_rate=0.046213855417462 --time_step=11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33measy-sweep-588\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/egmfmban\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_141959-egmfmban\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:20:01.248323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.255545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.256179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01,596 - wandb.wandb_agent - INFO - Running runs: ['egmfmban']\n",
            "2021-08-19 14:20:01.257461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.258054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.258631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.910345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.911086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.911711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:01.912244: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:20:01.912295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:20:01.943696: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:20:01.943727: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:20:01.943767: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:20:02.079697: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:20:02.079943: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:20:02.247827: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:20:06.207554: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:59 - loss: 10.1206 - mse: 10.12062021-08-19 14:20:06.827129: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:20:06.827177: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 14.0663 - mse: 14.0663 2021-08-19 14:20:07.108313: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:20:07.110581: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:20:07.264884: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 443 callback api events and 440 activity events. \n",
            "2021-08-19 14:20:07.273964: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:20:07.284228: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07\n",
            "\n",
            "2021-08-19 14:20:07.291329: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:20:07.306904: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07\n",
            "\n",
            "2021-08-19 14:20:07.309658: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:20:07.310194: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_141959-egmfmban/files/train/plugins/profile/2021_08_19_14_20_07/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 14.5542 - mse: 14.5542WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_begin` time: 0.0448s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_end` time: 0.0559s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 1.1575 - mse: 1.1575 - val_loss: 2.5724 - val_mse: 2.5724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4781 - mse: 0.4781 - val_loss: 2.5679 - val_mse: 2.5679\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.4814 - mse: 0.4814 - val_loss: 2.7527 - val_mse: 2.7527\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4724 - mse: 0.4724 - val_loss: 2.2319 - val_mse: 2.2319\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4505 - mse: 0.4505 - val_loss: 2.9370 - val_mse: 2.9370\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4290 - mse: 0.4290 - val_loss: 2.3814 - val_mse: 2.3814\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4040 - mse: 0.4040 - val_loss: 2.2800 - val_mse: 2.2800\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2734 - mse: 0.2734 - val_loss: 1.2816 - val_mse: 1.2816\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1705 - mse: 0.1705 - val_loss: 0.7738 - val_mse: 0.7738\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 1.3506 - val_mse: 1.3506\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1477 - mse: 0.1477 - val_loss: 1.1676 - val_mse: 1.1676\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.9915 - val_mse: 0.9915\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0526 - mse: 0.0526 - val_loss: 1.0626 - val_mse: 1.0626\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 1.0349 - val_mse: 1.0349\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.8602 - val_mse: 0.8602\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.9249 - val_mse: 0.9249\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.9169 - val_mse: 0.9169\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.8741 - val_mse: 0.8741\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.9060 - val_mse: 0.9060\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 134956\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_141959-egmfmban/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_141959-egmfmban/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.90597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.90597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.77383\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▇▇▇▆█▆▆▃▁▃▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▇▇▇▆█▆▆▃▁▃▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33measy-sweep-588\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/egmfmban\u001b[0m\n",
            "2021-08-19 14:20:32,075 - wandb.wandb_agent - INFO - Cleaning up finished run: egmfmban\n",
            "2021-08-19 14:20:32,511 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:20:32,511 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.18059720614499536\n",
            "\tdropout_rate2: 0.26907966411351003\n",
            "\tdropout_rate3: 0.36480772180246057\n",
            "\thidden1: 24\n",
            "\thidden2: 101\n",
            "\thidden3: 105\n",
            "\tlearning_rate: 0.0218102778349456\n",
            "\ttime_step: 12\n",
            "2021-08-19 14:20:32,513 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.18059720614499536 --dropout_rate2=0.26907966411351003 --dropout_rate3=0.36480772180246057 --hidden1=24 --hidden2=101 --hidden3=105 --learning_rate=0.0218102778349456 --time_step=12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-sweep-589\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/qkskbsum\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142035-qkskbsum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:20:37.238526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37,521 - wandb.wandb_agent - INFO - Running runs: ['qkskbsum']\n",
            "2021-08-19 14:20:37.245806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.246426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.247516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.248109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.248694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.925944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.926640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.927305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:20:37.927919: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:20:37.927983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:20:37.959104: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:20:37.959141: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:20:37.959243: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:20:38.099979: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:20:38.100439: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:20:38.286972: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:20:42.312567: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:04 - loss: 10.6259 - mse: 10.62592021-08-19 14:20:42.942175: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:20:42.942223: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 10.1168 - mse: 10.1168 2021-08-19 14:20:43.206678: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:20:43.207196: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:20:43.375155: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 463 callback api events and 460 activity events. \n",
            "2021-08-19 14:20:43.382878: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:20:43.393033: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43\n",
            "\n",
            "2021-08-19 14:20:43.400507: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:20:43.415565: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43\n",
            "\n",
            "2021-08-19 14:20:43.418498: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:20:43.418985: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142035-qkskbsum/files/train/plugins/profile/2021_08_19_14_20_43/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 12.4449 - mse: 12.4449WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_begin` time: 0.0414s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_end` time: 0.0592s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 1.1543 - mse: 1.1543 - val_loss: 1.2722 - val_mse: 1.2722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1531 - mse: 0.1531 - val_loss: 0.9021 - val_mse: 0.9021\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1648 - mse: 0.1648 - val_loss: 1.1347 - val_mse: 1.1347\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1112 - mse: 0.1112 - val_loss: 0.7803 - val_mse: 0.7803\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.9694 - val_mse: 0.9694\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.9627 - val_mse: 0.9627\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.9081 - val_mse: 0.9081\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0813 - mse: 0.0813 - val_loss: 0.8903 - val_mse: 0.8903\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0574 - mse: 0.0574 - val_loss: 1.0820 - val_mse: 1.0820\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0698 - mse: 0.0698 - val_loss: 1.0335 - val_mse: 1.0335\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0564 - mse: 0.0564 - val_loss: 0.6648 - val_mse: 0.6648\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.9282 - val_mse: 0.9282\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.7384 - val_mse: 0.7384\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.6913 - val_mse: 0.6913\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.8741 - val_mse: 0.8741\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.9269 - val_mse: 0.9269\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.9540 - val_mse: 0.9540\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.7331 - val_mse: 0.7331\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.8051 - val_mse: 0.8051\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.7111 - val_mse: 0.7111\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.6682 - val_mse: 0.6682\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 135139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142035-qkskbsum/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142035-qkskbsum/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0231\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.0231\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.66818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.66818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.66484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▄▆▂▅▄▄▄▆▅▁▄▂▁▃▄▄▂▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▄▆▂▅▄▄▄▆▅▁▄▂▁▃▄▄▂▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▂▃▃▃▃▄▅▅▅▅▆▆▆▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▂▃▃▃▃▄▅▅▅▅▆▆▆▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfresh-sweep-589\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/qkskbsum\u001b[0m\n",
            "2021-08-19 14:21:02,951 - wandb.wandb_agent - INFO - Cleaning up finished run: qkskbsum\n",
            "2021-08-19 14:21:03,680 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:21:03,680 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.12129103594716512\n",
            "\tdropout_rate2: 0.09351272571219722\n",
            "\tdropout_rate3: 0.4109925576636812\n",
            "\thidden1: 27\n",
            "\thidden2: 68\n",
            "\thidden3: 86\n",
            "\tlearning_rate: 0.0010228165766770657\n",
            "\ttime_step: 7\n",
            "2021-08-19 14:21:03,682 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.12129103594716512 --dropout_rate2=0.09351272571219722 --dropout_rate3=0.4109925576636812 --hidden1=27 --hidden2=68 --hidden3=86 --learning_rate=0.0010228165766770657 --time_step=7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlucky-sweep-590\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/vf8jsb40\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142106-vf8jsb40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:21:08.315552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.324989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.325892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08,692 - wandb.wandb_agent - INFO - Running runs: ['vf8jsb40']\n",
            "2021-08-19 14:21:08.327720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.328572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.329383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.978099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.978766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.979334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:08.979861: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:21:08.979910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:21:09.009090: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:21:09.009123: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:21:09.009156: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:21:09.142576: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:21:09.142746: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:21:09.328681: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:21:13.406814: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:09 - loss: 9.2166 - mse: 9.21662021-08-19 14:21:14.044948: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:21:14.044992: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 9.5616 - mse: 9.5616 2021-08-19 14:21:14.311154: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:21:14.311847: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:21:14.466192: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 401 callback api events and 398 activity events. \n",
            "2021-08-19 14:21:14.472742: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:21:14.481738: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14\n",
            "\n",
            "2021-08-19 14:21:14.488871: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:21:14.503415: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14\n",
            "\n",
            "2021-08-19 14:21:14.506373: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:21:14.506913: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142106-vf8jsb40/files/train/plugins/profile/2021_08_19_14_21_14/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 9.2950 - mse: 9.2950WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0059s vs `on_train_batch_begin` time: 0.0423s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0059s vs `on_train_batch_end` time: 0.0563s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 25ms/step - loss: 1.1136 - mse: 1.1136 - val_loss: 1.1024 - val_mse: 1.1024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1481 - mse: 0.1481 - val_loss: 0.3869 - val_mse: 0.3869\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1225 - mse: 0.1225 - val_loss: 0.3531 - val_mse: 0.3531\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1147 - mse: 0.1147 - val_loss: 0.5293 - val_mse: 0.5293\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1065 - mse: 0.1065 - val_loss: 0.6525 - val_mse: 0.6525\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1083 - mse: 0.1083 - val_loss: 0.3702 - val_mse: 0.3702\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1032 - mse: 0.1032 - val_loss: 0.3535 - val_mse: 0.3535\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.4439 - val_mse: 0.4439\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0942 - mse: 0.0942 - val_loss: 0.3509 - val_mse: 0.3509\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.3923 - val_mse: 0.3923\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.5392 - val_mse: 0.5392\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.4613 - val_mse: 0.4613\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.3949 - val_mse: 0.3949\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.3383 - val_mse: 0.3383\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0911 - mse: 0.0911 - val_loss: 0.2969 - val_mse: 0.2969\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.2586 - val_mse: 0.2586\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0905 - mse: 0.0905 - val_loss: 0.2939 - val_mse: 0.2939\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0891 - mse: 0.0891 - val_loss: 0.3068 - val_mse: 0.3068\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.2916 - val_mse: 0.2916\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.3001 - val_mse: 0.3001\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.3542 - val_mse: 0.3542\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0933 - mse: 0.0933 - val_loss: 0.2924 - val_mse: 0.2924\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.3481 - val_mse: 0.3481\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0840 - mse: 0.0840 - val_loss: 0.3500 - val_mse: 0.3500\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0878 - mse: 0.0878 - val_loss: 0.3219 - val_mse: 0.3219\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.3975 - val_mse: 0.3975\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 135330\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142106-vf8jsb40/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142106-vf8jsb40/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.08059\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.08059\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.39754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.39754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382889\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.25863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▂▂▃▄▂▂▃▂▂▃▃▂▂▁▁▁▁▁▁▂▁▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▂▂▃▄▂▂▃▂▂▃▃▂▂▁▁▁▁▁▁▂▁▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlucky-sweep-590\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/vf8jsb40\u001b[0m\n",
            "2021-08-19 14:21:39,283 - wandb.wandb_agent - INFO - Cleaning up finished run: vf8jsb40\n",
            "2021-08-19 14:21:39,877 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:21:39,877 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.21037220582771687\n",
            "\tdropout_rate2: 0.25336347036869616\n",
            "\tdropout_rate3: 0.2521475580109012\n",
            "\thidden1: 25\n",
            "\thidden2: 51\n",
            "\thidden3: 61\n",
            "\tlearning_rate: 0.010471585104255176\n",
            "\ttime_step: 8\n",
            "2021-08-19 14:21:39,878 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.21037220582771687 --dropout_rate2=0.25336347036869616 --dropout_rate3=0.2521475580109012 --hidden1=25 --hidden2=51 --hidden3=61 --learning_rate=0.010471585104255176 --time_step=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mserene-sweep-591\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/f3r7ct78\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142142-f3r7ct78\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:21:44.544408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:44,889 - wandb.wandb_agent - INFO - Running runs: ['f3r7ct78']\n",
            "2021-08-19 14:21:44.551552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:44.552152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:44.553268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:44.553887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:44.554454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:45.177996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:45.178843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:45.179580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:21:45.180114: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:21:45.180165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:21:45.208975: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:21:45.209007: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:21:45.209095: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:21:45.346533: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:21:45.346695: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:21:45.531068: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:21:49.559587: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:06 - loss: 9.0131 - mse: 9.01312021-08-19 14:21:50.219744: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:21:50.219788: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 7.1926 - mse: 7.1926 2021-08-19 14:21:50.485384: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:21:50.485837: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:21:50.640166: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 418 callback api events and 415 activity events. \n",
            "2021-08-19 14:21:50.646901: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:21:50.656419: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50\n",
            "\n",
            "2021-08-19 14:21:50.663314: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:21:50.679095: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50\n",
            "\n",
            "2021-08-19 14:21:50.682072: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:21:50.682609: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142142-f3r7ct78/files/train/plugins/profile/2021_08_19_14_21_50/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 4.8967 - mse: 4.8967WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_begin` time: 0.0422s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_end` time: 0.0580s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 0.7545 - mse: 0.7545 - val_loss: 2.1011 - val_mse: 2.1011\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1602 - mse: 0.1602 - val_loss: 0.8756 - val_mse: 0.8756\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.6708 - val_mse: 0.6708\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0728 - mse: 0.0728 - val_loss: 0.5434 - val_mse: 0.5434\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0753 - mse: 0.0753 - val_loss: 0.5515 - val_mse: 0.5515\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0637 - mse: 0.0637 - val_loss: 0.4746 - val_mse: 0.4746\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.5125 - val_mse: 0.5125\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0598 - mse: 0.0598 - val_loss: 0.5680 - val_mse: 0.5680\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0567 - mse: 0.0567 - val_loss: 0.5136 - val_mse: 0.5136\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0546 - mse: 0.0546 - val_loss: 0.6312 - val_mse: 0.6312\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.4431 - val_mse: 0.4431\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.5465 - val_mse: 0.5465\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.5449 - val_mse: 0.5449\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.4232 - val_mse: 0.4232\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.4619 - val_mse: 0.4619\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.4287 - val_mse: 0.4287\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.5218 - val_mse: 0.5218\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.6068 - val_mse: 0.6068\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.5113 - val_mse: 0.5113\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.4122 - val_mse: 0.4122\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.5865 - val_mse: 0.5865\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.5102 - val_mse: 0.5102\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.4447 - val_mse: 0.4447\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.5374 - val_mse: 0.5374\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.5273 - val_mse: 0.5273\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.4275 - val_mse: 0.4275\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.4597 - val_mse: 0.4597\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.4508 - val_mse: 0.4508\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.4217 - val_mse: 0.4217\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.4600 - val_mse: 0.4600\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 135546\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142142-f3r7ct78/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142142-f3r7ct78/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.46002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.46002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629382928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.41218\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▃▂▂▂▁▁▂▁▂▁▂▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▃▂▂▂▁▁▂▁▂▁▂▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mserene-sweep-591\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/f3r7ct78\u001b[0m\n",
            "2021-08-19 14:22:15,371 - wandb.wandb_agent - INFO - Cleaning up finished run: f3r7ct78\n",
            "2021-08-19 14:22:42,503 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:22:42,503 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3355685927613347\n",
            "\tdropout_rate2: 0.059329776351085634\n",
            "\tdropout_rate3: 0.4414674308058839\n",
            "\thidden1: 31\n",
            "\thidden2: 15\n",
            "\thidden3: 32\n",
            "\tlearning_rate: 0.003254783669403712\n",
            "\ttime_step: 22\n",
            "2021-08-19 14:22:42,505 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3355685927613347 --dropout_rate2=0.059329776351085634 --dropout_rate3=0.4414674308058839 --hidden1=31 --hidden2=15 --hidden3=32 --learning_rate=0.003254783669403712 --time_step=22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-sweep-592\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/ut0f5v3g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142244-ut0f5v3g\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:22:47.141680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.149159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.149792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47,514 - wandb.wandb_agent - INFO - Running runs: ['ut0f5v3g']\n",
            "2021-08-19 14:22:47.151036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.151643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.152174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.766060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.766719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.767284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:22:47.767810: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:22:47.767859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:22:47.810300: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:22:47.810336: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:22:47.810396: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:22:47.960922: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:22:47.961107: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:22:48.141815: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:22:52.092174: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:59 - loss: 8.9727 - mse: 8.97272021-08-19 14:22:52.736473: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:22:52.736516: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 9.1056 - mse: 9.1056 2021-08-19 14:22:53.003000: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:22:53.003616: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:22:53.153145: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 593 callback api events and 590 activity events. \n",
            "2021-08-19 14:22:53.161973: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:22:53.174238: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53\n",
            "\n",
            "2021-08-19 14:22:53.182754: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:22:53.198112: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53\n",
            "\n",
            "2021-08-19 14:22:53.201013: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:22:53.202126: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142244-ut0f5v3g/files/train/plugins/profile/2021_08_19_14_22_53/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 8.6995 - mse: 8.6995WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_begin` time: 0.0421s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0578s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 1.2439 - mse: 1.2439 - val_loss: 2.5134 - val_mse: 2.5134\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4614 - mse: 0.4614 - val_loss: 1.2342 - val_mse: 1.2342\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2776 - mse: 0.2776 - val_loss: 1.0884 - val_mse: 1.0884\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2634 - mse: 0.2634 - val_loss: 0.8379 - val_mse: 0.8379\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2485 - mse: 0.2485 - val_loss: 1.0833 - val_mse: 1.0833\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2331 - mse: 0.2331 - val_loss: 1.0977 - val_mse: 1.0977\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2160 - mse: 0.2160 - val_loss: 0.8184 - val_mse: 0.8184\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2016 - mse: 0.2016 - val_loss: 1.0431 - val_mse: 1.0431\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.8873 - val_mse: 0.8873\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2026 - mse: 0.2026 - val_loss: 1.1423 - val_mse: 1.1423\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1775 - mse: 0.1775 - val_loss: 0.9485 - val_mse: 0.9485\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 1.0636 - val_mse: 1.0636\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1670 - mse: 0.1670 - val_loss: 0.8073 - val_mse: 0.8073\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1626 - mse: 0.1626 - val_loss: 1.0004 - val_mse: 1.0004\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1456 - mse: 0.1456 - val_loss: 0.9021 - val_mse: 0.9021\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1325 - mse: 0.1325 - val_loss: 0.8366 - val_mse: 0.8366\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1263 - mse: 0.1263 - val_loss: 0.7236 - val_mse: 0.7236\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1251 - mse: 0.1251 - val_loss: 0.9605 - val_mse: 0.9605\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1105 - mse: 0.1105 - val_loss: 0.7980 - val_mse: 0.7980\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 0.8315 - val_mse: 0.8315\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.7635 - val_mse: 0.7635\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0876 - mse: 0.0876 - val_loss: 0.6907 - val_mse: 0.6907\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.7738 - val_mse: 0.7738\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0817 - mse: 0.0817 - val_loss: 0.7226 - val_mse: 0.7226\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0724 - mse: 0.0724 - val_loss: 0.6949 - val_mse: 0.6949\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0664 - mse: 0.0664 - val_loss: 0.7406 - val_mse: 0.7406\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0622 - mse: 0.0622 - val_loss: 0.7600 - val_mse: 0.7600\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0592 - mse: 0.0592 - val_loss: 0.6936 - val_mse: 0.6936\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0543 - mse: 0.0543 - val_loss: 0.7334 - val_mse: 0.7334\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0486 - mse: 0.0486 - val_loss: 0.6873 - val_mse: 0.6873\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.6815 - val_mse: 0.6815\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.6658 - val_mse: 0.6658\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.7050 - val_mse: 0.7050\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.6096 - val_mse: 0.6096\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.6304 - val_mse: 0.6304\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.6237 - val_mse: 0.6237\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.6297 - val_mse: 0.6297\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.6307 - val_mse: 0.6307\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.5996 - val_mse: 0.5996\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.5979 - val_mse: 0.5979\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.5845 - val_mse: 0.5845\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.5628 - val_mse: 0.5628\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.5519 - val_mse: 0.5519\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.5601 - val_mse: 0.5601\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.5673 - val_mse: 0.5673\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.5232 - val_mse: 0.5232\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.5631 - val_mse: 0.5631\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.5073 - val_mse: 0.5073\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5051 - val_mse: 0.5051\n",
            "Epoch 50/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.4968 - val_mse: 0.4968\n",
            "Epoch 51/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.4900 - val_mse: 0.4900\n",
            "Epoch 52/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.4645 - val_mse: 0.4645\n",
            "Epoch 53/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.4835 - val_mse: 0.4835\n",
            "Epoch 54/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.4694 - val_mse: 0.4694\n",
            "Epoch 55/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.4974 - val_mse: 0.4974\n",
            "Epoch 56/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.4947 - val_mse: 0.4947\n",
            "Epoch 57/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.4753 - val_mse: 0.4753\n",
            "Epoch 58/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.4573 - val_mse: 0.4573\n",
            "Epoch 59/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.4998 - val_mse: 0.4998\n",
            "Epoch 60/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.5031 - val_mse: 0.5031\n",
            "Epoch 61/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.4904 - val_mse: 0.4904\n",
            "Epoch 62/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.4485 - val_mse: 0.4485\n",
            "Epoch 63/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.4506 - val_mse: 0.4506\n",
            "Epoch 64/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.4547 - val_mse: 0.4547\n",
            "Epoch 65/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.4414 - val_mse: 0.4414\n",
            "Epoch 66/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.4516 - val_mse: 0.4516\n",
            "Epoch 67/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.4187 - val_mse: 0.4187\n",
            "Epoch 68/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.4306 - val_mse: 0.4306\n",
            "Epoch 69/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.4331 - val_mse: 0.4331\n",
            "Epoch 70/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.4134 - val_mse: 0.4134\n",
            "Epoch 71/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.4530 - val_mse: 0.4530\n",
            "Epoch 72/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.4413 - val_mse: 0.4413\n",
            "Epoch 73/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.4318 - val_mse: 0.4318\n",
            "Epoch 74/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.4803 - val_mse: 0.4803\n",
            "Epoch 75/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.4582 - val_mse: 0.4582\n",
            "Epoch 76/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.4538 - val_mse: 0.4538\n",
            "Epoch 77/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.4292 - val_mse: 0.4292\n",
            "Epoch 78/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.4250 - val_mse: 0.4250\n",
            "Epoch 79/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.4585 - val_mse: 0.4585\n",
            "Epoch 80/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.4768 - val_mse: 0.4768\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 135784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142244-ut0f5v3g/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142244-ut0f5v3g/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 79\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01297\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01297\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.47683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.47683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383023\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 79\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.41341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▃▃▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▃▃▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mearnest-sweep-592\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/ut0f5v3g\u001b[0m\n",
            "2021-08-19 14:24:18,960 - wandb.wandb_agent - INFO - Cleaning up finished run: ut0f5v3g\n",
            "2021-08-19 14:24:19,406 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:24:19,406 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.13818721566255246\n",
            "\tdropout_rate2: 0.2871673487798793\n",
            "\tdropout_rate3: 0.3988522455981727\n",
            "\thidden1: 68\n",
            "\thidden2: 113\n",
            "\thidden3: 100\n",
            "\tlearning_rate: 0.011156049639116192\n",
            "\ttime_step: 18\n",
            "2021-08-19 14:24:19,407 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.13818721566255246 --dropout_rate2=0.2871673487798793 --dropout_rate3=0.3988522455981727 --hidden1=68 --hidden2=113 --hidden3=100 --learning_rate=0.011156049639116192 --time_step=18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgiddy-sweep-593\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/758w0smo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142421-758w0smo\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:24:24.078485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.085966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.086589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24,413 - wandb.wandb_agent - INFO - Running runs: ['758w0smo']\n",
            "2021-08-19 14:24:24.087822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.088434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.089002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.719796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.720760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.721673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:24:24.722441: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:24:24.722502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:24:24.768642: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:24:24.768719: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:24:24.768776: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:24:24.915186: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:24:24.915351: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:24:25.093144: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:24:29.074082: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:00 - loss: 9.4325 - mse: 9.43252021-08-19 14:24:29.698539: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:24:29.698581: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.9798 - mse: 5.9798 2021-08-19 14:24:29.974632: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:24:29.975186: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:24:30.128549: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 546 callback api events and 543 activity events. \n",
            "2021-08-19 14:24:30.136385: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:24:30.147814: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30\n",
            "\n",
            "2021-08-19 14:24:30.157276: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:24:30.172102: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30\n",
            "\n",
            "2021-08-19 14:24:30.175006: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:24:30.175913: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142421-758w0smo/files/train/plugins/profile/2021_08_19_14_24_30/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 8.5254 - mse: 8.5254WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_begin` time: 0.0433s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_end` time: 0.0575s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.8753 - mse: 0.8753 - val_loss: 2.2477 - val_mse: 2.2477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2427 - mse: 0.2427 - val_loss: 0.6597 - val_mse: 0.6597\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1439 - mse: 0.1439 - val_loss: 1.0168 - val_mse: 1.0168\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.8878 - val_mse: 0.8878\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1050 - mse: 0.1050 - val_loss: 0.8619 - val_mse: 0.8619\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0868 - mse: 0.0868 - val_loss: 0.6054 - val_mse: 0.6054\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.6601 - val_mse: 0.6601\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0698 - mse: 0.0698 - val_loss: 0.6136 - val_mse: 0.6136\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0608 - mse: 0.0608 - val_loss: 0.5996 - val_mse: 0.5996\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0604 - mse: 0.0604 - val_loss: 0.6643 - val_mse: 0.6643\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0543 - mse: 0.0543 - val_loss: 0.7270 - val_mse: 0.7270\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0567 - mse: 0.0567 - val_loss: 0.7513 - val_mse: 0.7513\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.8956 - val_mse: 0.8956\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0469 - mse: 0.0469 - val_loss: 0.5886 - val_mse: 0.5886\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.6245 - val_mse: 0.6245\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.7171 - val_mse: 0.7171\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.4862 - val_mse: 0.4862\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.5462 - val_mse: 0.5462\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.6567 - val_mse: 0.6567\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.5942 - val_mse: 0.5942\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.7315 - val_mse: 0.7315\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.6232 - val_mse: 0.6232\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.6290 - val_mse: 0.6290\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.5720 - val_mse: 0.5720\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.5780 - val_mse: 0.5780\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.6177 - val_mse: 0.6177\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.6064 - val_mse: 0.6064\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 136274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142421-758w0smo/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142421-758w0smo/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.60638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.60638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.48616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▂▃▃▂▁▂▂▁▂▂▂▃▁▂▂▁▁▂▁▂▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▂▃▃▂▁▂▂▁▂▂▂▃▁▂▂▁▁▂▁▂▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgiddy-sweep-593\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/758w0smo\u001b[0m\n",
            "2021-08-19 14:24:54,888 - wandb.wandb_agent - INFO - Cleaning up finished run: 758w0smo\n",
            "2021-08-19 14:24:55,377 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:24:55,377 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3079423744889477\n",
            "\tdropout_rate2: 0.0685995417521971\n",
            "\tdropout_rate3: 0.12854327192938703\n",
            "\thidden1: 16\n",
            "\thidden2: 33\n",
            "\thidden3: 80\n",
            "\tlearning_rate: 0.03971578633320436\n",
            "\ttime_step: 35\n",
            "2021-08-19 14:24:55,378 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3079423744889477 --dropout_rate2=0.0685995417521971 --dropout_rate3=0.12854327192938703 --hidden1=16 --hidden2=33 --hidden3=80 --learning_rate=0.03971578633320436 --time_step=35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mazure-sweep-594\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/mgxe6k0r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142457-mgxe6k0r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:25:00.016804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.024012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.024636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00,384 - wandb.wandb_agent - INFO - Running runs: ['mgxe6k0r']\n",
            "2021-08-19 14:25:00.025862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.026495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.027063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.657560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.658523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.659392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:00.660215: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:25:00.660281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:25:00.704169: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:25:00.704201: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:25:00.704236: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:25:00.841961: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:25:00.842129: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:25:01.008894: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:25:04.978793: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:57 - loss: 9.5130 - mse: 9.51302021-08-19 14:25:05.620299: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:25:05.620345: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 31s - loss: 7.3886 - mse: 7.3886 2021-08-19 14:25:05.885425: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:25:05.886219: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:25:06.041166: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 746 callback api events and 743 activity events. \n",
            "2021-08-19 14:25:06.051645: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:25:06.067066: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06\n",
            "\n",
            "2021-08-19 14:25:06.077246: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:25:06.095347: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06\n",
            "\n",
            "2021-08-19 14:25:06.101444: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:25:06.102913: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142457-mgxe6k0r/files/train/plugins/profile/2021_08_19_14_25_06/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 24s - loss: 11.9032 - mse: 11.9032WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0111s vs `on_train_batch_begin` time: 0.0418s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0111s vs `on_train_batch_end` time: 0.0598s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 27ms/step - loss: 1.1403 - mse: 1.1403 - val_loss: 2.4872 - val_mse: 2.4872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.4511 - mse: 0.4511 - val_loss: 2.4224 - val_mse: 2.4224\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4057 - mse: 0.4057 - val_loss: 2.3973 - val_mse: 2.3973\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.4012 - mse: 0.4012 - val_loss: 2.4668 - val_mse: 2.4668\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.3872 - mse: 0.3872 - val_loss: 2.5106 - val_mse: 2.5106\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3867 - mse: 0.3867 - val_loss: 2.3462 - val_mse: 2.3462\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4342 - mse: 0.4342 - val_loss: 2.7513 - val_mse: 2.7513\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.4027 - mse: 0.4027 - val_loss: 2.4415 - val_mse: 2.4415\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3868 - mse: 0.3868 - val_loss: 2.3860 - val_mse: 2.3860\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.3959 - mse: 0.3959 - val_loss: 2.5014 - val_mse: 2.5014\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.3793 - mse: 0.3793 - val_loss: 2.2160 - val_mse: 2.2160\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.3028 - mse: 0.3028 - val_loss: 1.8720 - val_mse: 1.8720\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.1831 - mse: 0.1831 - val_loss: 1.5264 - val_mse: 1.5264\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0590 - mse: 0.0590 - val_loss: 1.2283 - val_mse: 1.2283\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0510 - mse: 0.0510 - val_loss: 1.1498 - val_mse: 1.1498\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0553 - mse: 0.0553 - val_loss: 1.1437 - val_mse: 1.1437\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 1.2221 - val_mse: 1.2221\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 1.0497 - val_mse: 1.0497\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 1.1436 - val_mse: 1.1436\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.7545 - val_mse: 0.7545\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.9751 - val_mse: 0.9751\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 1.2640 - val_mse: 1.2640\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 1.0863 - val_mse: 1.0863\n",
            "Epoch 24/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 1.4224 - val_mse: 1.4224\n",
            "Epoch 25/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.9738 - val_mse: 0.9738\n",
            "Epoch 26/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 1.2875 - val_mse: 1.2875\n",
            "Epoch 27/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 1.0662 - val_mse: 1.0662\n",
            "Epoch 28/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 1.0596 - val_mse: 1.0596\n",
            "Epoch 29/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.9908 - val_mse: 0.9908\n",
            "Epoch 30/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 1.0542 - val_mse: 1.0542\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 136495\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142457-mgxe6k0r/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142457-mgxe6k0r/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02482\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02482\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 1.05423\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 1.05423\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.75452\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▃▃▃▃▄▃▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▃▃▃▃▄▃▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▇▇▇▇▇▇█▇▇▇▆▅▄▃▂▂▃▂▂▁▂▃▂▃▂▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▇▇▇▇▇▇█▇▇▇▆▅▄▃▂▂▃▂▂▁▂▃▂▃▂▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mazure-sweep-594\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/mgxe6k0r\u001b[0m\n",
            "2021-08-19 14:25:46,685 - wandb.wandb_agent - INFO - Cleaning up finished run: mgxe6k0r\n",
            "2021-08-19 14:25:47,509 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:25:47,509 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.2318406080601359\n",
            "\tdropout_rate2: 0.2163745886448617\n",
            "\tdropout_rate3: 0.08114631450147708\n",
            "\thidden1: 26\n",
            "\thidden2: 69\n",
            "\thidden3: 85\n",
            "\tlearning_rate: 0.02263930955050833\n",
            "\ttime_step: 21\n",
            "2021-08-19 14:25:47,511 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.2318406080601359 --dropout_rate2=0.2163745886448617 --dropout_rate3=0.08114631450147708 --hidden1=26 --hidden2=69 --hidden3=85 --learning_rate=0.02263930955050833 --time_step=21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcharmed-sweep-595\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/ds88apax\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142549-ds88apax\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:25:52.166691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.175543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.180509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52,521 - wandb.wandb_agent - INFO - Running runs: ['ds88apax']\n",
            "2021-08-19 14:25:52.182057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.182980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.183797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.822774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.823470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.824052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:25:52.824620: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:25:52.824676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:25:52.857532: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:25:52.857562: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:25:52.857656: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:25:52.995376: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:25:52.995565: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:25:53.178880: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:25:57.100691: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:56 - loss: 10.8599 - mse: 10.85992021-08-19 14:25:57.741198: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:25:57.741239: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.5929 - mse: 5.5929   2021-08-19 14:25:58.006200: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:25:58.006796: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:25:58.160094: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 598 callback api events and 595 activity events. \n",
            "2021-08-19 14:25:58.168101: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:25:58.180784: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58\n",
            "\n",
            "2021-08-19 14:25:58.189729: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:25:58.205081: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58\n",
            "\n",
            "2021-08-19 14:25:58.208035: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:25:58.208739: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142549-ds88apax/files/train/plugins/profile/2021_08_19_14_25_58/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 8.3756 - mse: 8.3756WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0094s vs `on_train_batch_begin` time: 0.0415s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0094s vs `on_train_batch_end` time: 0.0586s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 0.7628 - mse: 0.7628 - val_loss: 1.5319 - val_mse: 1.5319\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4174 - mse: 0.4174 - val_loss: 2.3731 - val_mse: 2.3731\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1701 - mse: 0.1701 - val_loss: 1.0462 - val_mse: 1.0462\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0521 - mse: 0.0521 - val_loss: 0.7964 - val_mse: 0.7964\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.8643 - val_mse: 0.8643\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.8220 - val_mse: 0.8220\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.8742 - val_mse: 0.8742\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.8513 - val_mse: 0.8513\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.7590 - val_mse: 0.7590\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.7608 - val_mse: 0.7608\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.7160 - val_mse: 0.7160\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.8534 - val_mse: 0.8534\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.6560 - val_mse: 0.6560\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.7951 - val_mse: 0.7951\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.7692 - val_mse: 0.7692\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.7924 - val_mse: 0.7924\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.7653 - val_mse: 0.7653\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.7693 - val_mse: 0.7693\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.7264 - val_mse: 0.7264\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.5274 - val_mse: 0.5274\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.8255 - val_mse: 0.8255\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.7159 - val_mse: 0.7159\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.9230 - val_mse: 0.9230\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.6582 - val_mse: 0.6582\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.6322 - val_mse: 0.6322\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.6396 - val_mse: 0.6396\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.6575 - val_mse: 0.6575\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.6017 - val_mse: 0.6017\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.6355 - val_mse: 0.6355\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.5869 - val_mse: 0.5869\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 136733\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142549-ds88apax/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142549-ds88apax/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.00913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.00913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.5869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.5869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.52737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▅█▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▃▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▅█▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▃▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcharmed-sweep-595\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/ds88apax\u001b[0m\n",
            "2021-08-19 14:26:43,470 - wandb.wandb_agent - INFO - Cleaning up finished run: ds88apax\n",
            "2021-08-19 14:26:43,921 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:26:43,922 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.22531018744750775\n",
            "\tdropout_rate2: 0.08494576445823034\n",
            "\tdropout_rate3: 0.12046330900212653\n",
            "\thidden1: 88\n",
            "\thidden2: 91\n",
            "\thidden3: 95\n",
            "\tlearning_rate: 0.018201829024037216\n",
            "\ttime_step: 21\n",
            "2021-08-19 14:26:43,923 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.22531018744750775 --dropout_rate2=0.08494576445823034 --dropout_rate3=0.12046330900212653 --hidden1=88 --hidden2=91 --hidden3=95 --learning_rate=0.018201829024037216 --time_step=21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgrateful-sweep-596\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/krzvpwy3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142646-krzvpwy3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:26:48.618252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:48.625639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:48.626244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:48,933 - wandb.wandb_agent - INFO - Running runs: ['krzvpwy3']\n",
            "2021-08-19 14:26:48.627422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:48.627992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:48.628578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:49.277298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:49.278217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:49.279019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:26:49.280687: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:26:49.281230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:26:49.309942: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:26:49.309974: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:26:49.310012: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:26:49.443827: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:26:49.444034: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:26:49.624515: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:26:53.724911: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:10 - loss: 9.2883 - mse: 9.28832021-08-19 14:26:54.371049: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:26:54.371096: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 33s - loss: 6.0131 - mse: 6.0131 2021-08-19 14:26:54.645958: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:26:54.646553: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:26:54.798398: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 589 callback api events and 586 activity events. \n",
            "2021-08-19 14:26:54.806483: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:26:54.818598: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54\n",
            "\n",
            "2021-08-19 14:26:54.827014: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:26:54.843080: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54\n",
            "\n",
            "2021-08-19 14:26:54.845991: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:26:54.846528: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142646-krzvpwy3/files/train/plugins/profile/2021_08_19_14_26_54/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 25s - loss: 10.5768 - mse: 10.5768WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0087s vs `on_train_batch_begin` time: 0.0433s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0087s vs `on_train_batch_end` time: 0.0597s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.8772 - mse: 0.8772 - val_loss: 2.5155 - val_mse: 2.5155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4157 - mse: 0.4157 - val_loss: 2.2246 - val_mse: 2.2246\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4009 - mse: 0.4009 - val_loss: 2.3992 - val_mse: 2.3992\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3876 - mse: 0.3876 - val_loss: 2.4875 - val_mse: 2.4875\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3965 - mse: 0.3965 - val_loss: 2.3870 - val_mse: 2.3870\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3970 - mse: 0.3970 - val_loss: 2.3471 - val_mse: 2.3471\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3850 - mse: 0.3850 - val_loss: 2.2602 - val_mse: 2.2602\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3725 - mse: 0.3725 - val_loss: 2.1079 - val_mse: 2.1079\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3046 - mse: 0.3046 - val_loss: 1.4746 - val_mse: 1.4746\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2859 - mse: 0.2859 - val_loss: 1.6088 - val_mse: 1.6088\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1141 - mse: 0.1141 - val_loss: 1.4095 - val_mse: 1.4095\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0647 - mse: 0.0647 - val_loss: 1.0883 - val_mse: 1.0883\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0528 - mse: 0.0528 - val_loss: 1.2245 - val_mse: 1.2245\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 1.1957 - val_mse: 1.1957\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.9507 - val_mse: 0.9507\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 1.0498 - val_mse: 1.0498\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.8562 - val_mse: 0.8562\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 1.0764 - val_mse: 1.0764\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.8580 - val_mse: 0.8580\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.9708 - val_mse: 0.9708\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.7880 - val_mse: 0.7880\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.9331 - val_mse: 0.9331\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.8135 - val_mse: 0.8135\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 1.0284 - val_mse: 1.0284\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.8584 - val_mse: 0.8584\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.8942 - val_mse: 0.8942\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.7315 - val_mse: 0.7315\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.8881 - val_mse: 0.8881\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.8737 - val_mse: 0.8737\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.7434 - val_mse: 0.7434\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.7253 - val_mse: 0.7253\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.7525 - val_mse: 0.7525\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.8945 - val_mse: 0.8945\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.7689 - val_mse: 0.7689\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.6852 - val_mse: 0.6852\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.7558 - val_mse: 0.7558\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.6077 - val_mse: 0.6077\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.6123 - val_mse: 0.6123\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.7689 - val_mse: 0.7689\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.7108 - val_mse: 0.7108\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.6765 - val_mse: 0.6765\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.6791 - val_mse: 0.6791\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.6416 - val_mse: 0.6416\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.6739 - val_mse: 0.6739\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.6087 - val_mse: 0.6087\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.6088 - val_mse: 0.6088\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.5690 - val_mse: 0.5690\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.6601 - val_mse: 0.6601\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.6898 - val_mse: 0.6898\n",
            "Epoch 50/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.5447 - val_mse: 0.5447\n",
            "Epoch 51/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.7154 - val_mse: 0.7154\n",
            "Epoch 52/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.5865 - val_mse: 0.5865\n",
            "Epoch 53/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.5449 - val_mse: 0.5449\n",
            "Epoch 54/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.6492 - val_mse: 0.6492\n",
            "Epoch 55/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.5782 - val_mse: 0.5782\n",
            "Epoch 56/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.6682 - val_mse: 0.6682\n",
            "Epoch 57/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.6308 - val_mse: 0.6308\n",
            "Epoch 58/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.5781 - val_mse: 0.5781\n",
            "Epoch 59/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.5705 - val_mse: 0.5705\n",
            "Epoch 60/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.5441 - val_mse: 0.5441\n",
            "Epoch 61/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.6919 - val_mse: 0.6919\n",
            "Epoch 62/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.5527 - val_mse: 0.5527\n",
            "Epoch 63/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.8162 - val_mse: 0.8162\n",
            "Epoch 64/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.6097 - val_mse: 0.6097\n",
            "Epoch 65/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.6987 - val_mse: 0.6987\n",
            "Epoch 66/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.6763 - val_mse: 0.6763\n",
            "Epoch 67/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.6114 - val_mse: 0.6114\n",
            "Epoch 68/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.5873 - val_mse: 0.5873\n",
            "Epoch 69/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.8590 - val_mse: 0.8590\n",
            "Epoch 70/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.7177 - val_mse: 0.7177\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 136971\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142646-krzvpwy3/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142646-krzvpwy3/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.00647\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.00647\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.71768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.71768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 54\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383260\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.54413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▄▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▄▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▇█▇▇▄▄▃▂▃▃▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▇█▇▇▄▄▃▂▃▃▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgrateful-sweep-596\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/krzvpwy3\u001b[0m\n",
            "2021-08-19 14:28:20,454 - wandb.wandb_agent - INFO - Cleaning up finished run: krzvpwy3\n",
            "2021-08-19 14:28:20,907 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:28:20,907 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.13554997830035828\n",
            "\tdropout_rate2: 0.43637576015770124\n",
            "\tdropout_rate3: 0.15979311849047495\n",
            "\thidden1: 14\n",
            "\thidden2: 41\n",
            "\thidden3: 82\n",
            "\tlearning_rate: 0.003973380043329586\n",
            "\ttime_step: 36\n",
            "2021-08-19 14:28:20,909 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.13554997830035828 --dropout_rate2=0.43637576015770124 --dropout_rate3=0.15979311849047495 --hidden1=14 --hidden2=41 --hidden3=82 --learning_rate=0.003973380043329586 --time_step=36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgentle-sweep-597\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/es25ixkz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142823-es25ixkz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:28:25.632134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:25,919 - wandb.wandb_agent - INFO - Running runs: ['es25ixkz']\n",
            "2021-08-19 14:28:25.640171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:25.640785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:25.642220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:25.642878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:25.643441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:26.285133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:26.285808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:26.286401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:26.286946: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:28:26.286999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:28:26.315192: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:28:26.315225: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:28:26.315261: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:28:26.449971: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:28:26.450141: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:28:26.636200: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:28:30.604741: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:58 - loss: 11.6662 - mse: 11.66622021-08-19 14:28:31.282501: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:28:31.282545: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 35s - loss: 9.2786 - mse: 9.2786   2021-08-19 14:28:31.581041: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:28:31.581710: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:28:31.739810: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 796 callback api events and 793 activity events. \n",
            "2021-08-19 14:28:31.750951: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:28:31.769007: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31\n",
            "\n",
            "2021-08-19 14:28:31.781209: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:28:31.808500: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31\n",
            "\n",
            "2021-08-19 14:28:31.813217: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:28:31.814029: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142823-es25ixkz/files/train/plugins/profile/2021_08_19_14_28_31/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 27s - loss: 7.1154 - mse: 7.1154WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0105s vs `on_train_batch_begin` time: 0.0470s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0105s vs `on_train_batch_end` time: 0.0682s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 30ms/step - loss: 0.6639 - mse: 0.6639 - val_loss: 1.1724 - val_mse: 1.1724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 1.0190 - val_mse: 1.0190\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.6650 - val_mse: 0.6650\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.8148 - val_mse: 0.8148\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.8329 - val_mse: 0.8329\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.8527 - val_mse: 0.8527\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.8572 - val_mse: 0.8572\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.8458 - val_mse: 0.8458\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.7984 - val_mse: 0.7984\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.7606 - val_mse: 0.7606\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.7471 - val_mse: 0.7471\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.6676 - val_mse: 0.6676\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.8388 - val_mse: 0.8388\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 137409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142823-es25ixkz/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142823-es25ixkz/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.03815\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.03815\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.83883\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.83883\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383322\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▆▁▃▃▄▄▃▃▂▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▆▁▃▃▄▄▃▃▂▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▃▃▄▄▅▆▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▃▃▄▄▅▆▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgentle-sweep-597\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/es25ixkz\u001b[0m\n",
            "2021-08-19 14:28:51,320 - wandb.wandb_agent - INFO - Cleaning up finished run: es25ixkz\n",
            "2021-08-19 14:28:51,711 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:28:51,711 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.16413974986510554\n",
            "\tdropout_rate2: 0.21863522101745392\n",
            "\tdropout_rate3: 0.06305829261117316\n",
            "\thidden1: 54\n",
            "\thidden2: 49\n",
            "\thidden3: 81\n",
            "\tlearning_rate: 0.00857961549536675\n",
            "\ttime_step: 17\n",
            "2021-08-19 14:28:51,713 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.16413974986510554 --dropout_rate2=0.21863522101745392 --dropout_rate3=0.06305829261117316 --hidden1=54 --hidden2=49 --hidden3=81 --learning_rate=0.00857961549536675 --time_step=17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mazure-sweep-598\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/e5bbpyxa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142854-e5bbpyxa\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:28:56.360588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.367766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.368387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56,721 - wandb.wandb_agent - INFO - Running runs: ['e5bbpyxa']\n",
            "2021-08-19 14:28:56.369591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.370218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.370820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.976753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.977405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.977973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:28:56.978514: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:28:56.978564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:28:57.009693: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:28:57.009731: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:28:57.009764: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:28:57.178628: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:28:57.178806: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:28:57.350946: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:29:01.298990: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:59 - loss: 8.4630 - mse: 8.46302021-08-19 14:29:01.939288: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:29:01.939334: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.4028 - mse: 5.4028 2021-08-19 14:29:02.203940: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:29:02.204550: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:29:02.365720: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 544 callback api events and 541 activity events. \n",
            "2021-08-19 14:29:02.379638: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:29:02.396483: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02\n",
            "\n",
            "2021-08-19 14:29:02.404512: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:29:02.420645: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02\n",
            "\n",
            "2021-08-19 14:29:02.423656: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:29:02.424162: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142854-e5bbpyxa/files/train/plugins/profile/2021_08_19_14_29_02/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 25s - loss: 4.2719 - mse: 4.2719WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_begin` time: 0.0420s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0625s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.5189 - mse: 0.5189 - val_loss: 2.2952 - val_mse: 2.2952\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3487 - mse: 0.3487 - val_loss: 2.4524 - val_mse: 2.4524\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1999 - mse: 0.1999 - val_loss: 0.6362 - val_mse: 0.6362\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.8339 - val_mse: 0.8339\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.7573 - val_mse: 0.7573\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.7408 - val_mse: 0.7408\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.6319 - val_mse: 0.6319\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.6684 - val_mse: 0.6684\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.5825 - val_mse: 0.5825\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.6498 - val_mse: 0.6498\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.8130 - val_mse: 0.8130\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.7547 - val_mse: 0.7547\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.6466 - val_mse: 0.6466\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5772 - val_mse: 0.5772\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.6594 - val_mse: 0.6594\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.6304 - val_mse: 0.6304\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.6453 - val_mse: 0.6453\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5438 - val_mse: 0.5438\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.6180 - val_mse: 0.6180\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.7235 - val_mse: 0.7235\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.4951 - val_mse: 0.4951\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.6883 - val_mse: 0.6883\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.6256 - val_mse: 0.6256\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.7120 - val_mse: 0.7120\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5449 - val_mse: 0.5449\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.5519 - val_mse: 0.5519\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.6160 - val_mse: 0.6160\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.5518 - val_mse: 0.5518\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.6280 - val_mse: 0.6280\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.4864 - val_mse: 0.4864\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.5848 - val_mse: 0.5848\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.5382 - val_mse: 0.5382\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.5321 - val_mse: 0.5321\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.5495 - val_mse: 0.5495\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.4276 - val_mse: 0.4276\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.4814 - val_mse: 0.4814\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.4583 - val_mse: 0.4583\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.4392 - val_mse: 0.4392\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.4104 - val_mse: 0.4104\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.3899 - val_mse: 0.3899\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.4321 - val_mse: 0.4321\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.4038 - val_mse: 0.4038\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.4143 - val_mse: 0.4143\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.3535 - val_mse: 0.3535\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.3916 - val_mse: 0.3916\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.3981 - val_mse: 0.3981\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.3587 - val_mse: 0.3587\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.3694 - val_mse: 0.3694\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.3515 - val_mse: 0.3515\n",
            "Epoch 50/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.3110 - val_mse: 0.3110\n",
            "Epoch 51/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.3500 - val_mse: 0.3500\n",
            "Epoch 52/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.3355 - val_mse: 0.3355\n",
            "Epoch 53/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.3667 - val_mse: 0.3667\n",
            "Epoch 54/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.3882 - val_mse: 0.3882\n",
            "Epoch 55/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.3538 - val_mse: 0.3538\n",
            "Epoch 56/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.3415 - val_mse: 0.3415\n",
            "Epoch 57/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.3850 - val_mse: 0.3850\n",
            "Epoch 58/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.2912 - val_mse: 0.2912\n",
            "Epoch 59/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.3663 - val_mse: 0.3663\n",
            "Epoch 60/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.3810 - val_mse: 0.3810\n",
            "Epoch 61/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.2697 - val_mse: 0.2697\n",
            "Epoch 62/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.3318 - val_mse: 0.3318\n",
            "Epoch 63/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.3477 - val_mse: 0.3477\n",
            "Epoch 64/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.3664 - val_mse: 0.3664\n",
            "Epoch 65/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.3484 - val_mse: 0.3484\n",
            "Epoch 66/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.3000 - val_mse: 0.3000\n",
            "Epoch 67/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.3269 - val_mse: 0.3269\n",
            "Epoch 68/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.3313 - val_mse: 0.3313\n",
            "Epoch 69/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.3924 - val_mse: 0.3924\n",
            "Epoch 70/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.3214 - val_mse: 0.3214\n",
            "Epoch 71/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.2797 - val_mse: 0.2797\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 137560\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142854-e5bbpyxa/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142854-e5bbpyxa/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 70\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.00534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.00534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.27967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.27967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 70\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.26972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▇█▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▇█▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mazure-sweep-598\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/e5bbpyxa\u001b[0m\n",
            "2021-08-19 14:29:52,683 - wandb.wandb_agent - INFO - Cleaning up finished run: e5bbpyxa\n",
            "2021-08-19 14:29:53,049 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:29:53,049 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.26207399635324313\n",
            "\tdropout_rate2: 0.1407411026119852\n",
            "\tdropout_rate3: 0.08384934067957647\n",
            "\thidden1: 58\n",
            "\thidden2: 86\n",
            "\thidden3: 68\n",
            "\tlearning_rate: 0.04926016113471523\n",
            "\ttime_step: 11\n",
            "2021-08-19 14:29:53,050 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.26207399635324313 --dropout_rate2=0.1407411026119852 --dropout_rate3=0.08384934067957647 --hidden1=58 --hidden2=86 --hidden3=68 --learning_rate=0.04926016113471523 --time_step=11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpert-sweep-599\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/4k6i72fs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_142955-4k6i72fs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:29:57.678719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:57.686020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:57.686660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:58,057 - wandb.wandb_agent - INFO - Running runs: ['4k6i72fs']\n",
            "2021-08-19 14:29:57.687867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:57.688478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:57.689035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:58.317436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:58.318342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:58.319171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:29:58.319936: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:29:58.319989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:29:58.358861: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:29:58.358894: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:29:58.358985: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:29:58.495778: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:29:58.495948: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:29:58.673917: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:30:02.603954: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:59 - loss: 10.6045 - mse: 10.60452021-08-19 14:30:03.285127: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:30:03.285181: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 35s - loss: 9.1187 - mse: 9.1187   2021-08-19 14:30:03.564821: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:30:03.565305: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:30:03.717307: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 455 callback api events and 452 activity events. \n",
            "2021-08-19 14:30:03.724554: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:30:03.735496: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03\n",
            "\n",
            "2021-08-19 14:30:03.742873: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:30:03.757992: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03\n",
            "\n",
            "2021-08-19 14:30:03.761300: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:30:03.761897: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_142955-4k6i72fs/files/train/plugins/profile/2021_08_19_14_30_03/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 25s - loss: 6.5184 - mse: 6.5184WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_begin` time: 0.0446s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_train_batch_end` time: 0.0612s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 2.3801 - val_mse: 2.3801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3822 - mse: 0.3822 - val_loss: 2.4276 - val_mse: 2.4276\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3921 - mse: 0.3921 - val_loss: 2.8150 - val_mse: 2.8150\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3780 - mse: 0.3780 - val_loss: 2.2836 - val_mse: 2.2836\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4323 - mse: 0.4323 - val_loss: 2.2961 - val_mse: 2.2961\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4347 - mse: 0.4347 - val_loss: 2.7211 - val_mse: 2.7211\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3942 - mse: 0.3942 - val_loss: 2.3528 - val_mse: 2.3528\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3836 - mse: 0.3836 - val_loss: 2.6523 - val_mse: 2.6523\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4162 - mse: 0.4162 - val_loss: 2.5913 - val_mse: 2.5913\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3794 - mse: 0.3794 - val_loss: 2.5942 - val_mse: 2.5942\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4059 - mse: 0.4059 - val_loss: 2.5541 - val_mse: 2.5541\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3759 - mse: 0.3759 - val_loss: 2.5805 - val_mse: 2.5805\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3722 - mse: 0.3722 - val_loss: 2.4122 - val_mse: 2.4122\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3836 - mse: 0.3836 - val_loss: 2.5154 - val_mse: 2.5154\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 138005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_142955-4k6i72fs/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_142955-4k6i72fs/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.38356\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.38356\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.51541\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.51541\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.28363\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▂▂▃▃▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▂▁▃▃▂▁▂▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▂▁▃▃▂▁▂▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▂▃█▁▁▇▂▆▅▅▅▅▃▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▂▃█▁▁▇▂▆▅▅▅▅▃▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▃▄▄▅▅▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▃▄▄▅▅▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▂▂▃▃▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mexpert-sweep-599\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/4k6i72fs\u001b[0m\n",
            "2021-08-19 14:30:23,458 - wandb.wandb_agent - INFO - Cleaning up finished run: 4k6i72fs\n",
            "2021-08-19 14:30:23,825 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:30:23,825 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.06931108612827239\n",
            "\tdropout_rate2: 0.05342816010070187\n",
            "\tdropout_rate3: 0.18893045706129485\n",
            "\thidden1: 44\n",
            "\thidden2: 70\n",
            "\thidden3: 105\n",
            "\tlearning_rate: 0.0009379950260380097\n",
            "\ttime_step: 22\n",
            "2021-08-19 14:30:23,827 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.06931108612827239 --dropout_rate2=0.05342816010070187 --dropout_rate3=0.18893045706129485 --hidden1=44 --hidden2=70 --hidden3=105 --learning_rate=0.0009379950260380097 --time_step=22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdifferent-sweep-600\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/4pkuzcft\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143026-4pkuzcft\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:30:28.469841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:28,836 - wandb.wandb_agent - INFO - Running runs: ['4pkuzcft']\n",
            "2021-08-19 14:30:28.477127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:28.477754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:28.478851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:28.479466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:28.480024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:29.106105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:29.106811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:29.107411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:30:29.107941: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:30:29.107992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:30:29.140580: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:30:29.140618: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:30:29.140656: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:30:29.295348: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:30:29.295543: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:30:29.475548: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:30:33.490922: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:03 - loss: 10.9289 - mse: 10.92892021-08-19 14:30:34.112168: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:30:34.112208: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 10.1546 - mse: 10.1546 2021-08-19 14:30:34.382988: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:30:34.388831: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:30:34.541288: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 605 callback api events and 602 activity events. \n",
            "2021-08-19 14:30:34.550224: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:30:34.563844: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34\n",
            "\n",
            "2021-08-19 14:30:34.572461: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:30:34.588008: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34\n",
            "\n",
            "2021-08-19 14:30:34.590964: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:30:34.591505: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143026-4pkuzcft/files/train/plugins/profile/2021_08_19_14_30_34/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 9.1295 - mse: 9.1295  WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_begin` time: 0.0428s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0587s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.5395 - val_mse: 0.5395\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.4935 - val_mse: 0.4935\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0481 - mse: 0.0481 - val_loss: 0.3652 - val_mse: 0.3652\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.3265 - val_mse: 0.3265\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.3367 - val_mse: 0.3367\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3449 - val_mse: 0.3449\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.4831 - val_mse: 0.4831\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.4948 - val_mse: 0.4948\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.3781 - val_mse: 0.3781\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.3369 - val_mse: 0.3369\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.3797 - val_mse: 0.3797\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.3947 - val_mse: 0.3947\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.4306 - val_mse: 0.4306\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.3636 - val_mse: 0.3636\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 138163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143026-4pkuzcft/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143026-4pkuzcft/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.03165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.03165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.36361\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.36361\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.3265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▂▂▃▃▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▆▂▁▁▂▆▇▃▁▃▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▆▂▁▁▂▆▇▃▁▃▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▃▃▄▅▅▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▃▃▄▅▅▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▂▂▃▃▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdifferent-sweep-600\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/4pkuzcft\u001b[0m\n",
            "2021-08-19 14:30:59,339 - wandb.wandb_agent - INFO - Cleaning up finished run: 4pkuzcft\n",
            "2021-08-19 14:30:59,803 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:30:59,804 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.4031115763037398\n",
            "\tdropout_rate2: 0.0630919804127125\n",
            "\tdropout_rate3: 0.41035611911019004\n",
            "\thidden1: 21\n",
            "\thidden2: 102\n",
            "\thidden3: 32\n",
            "\tlearning_rate: 0.016717582949612616\n",
            "\ttime_step: 10\n",
            "2021-08-19 14:30:59,805 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.4031115763037398 --dropout_rate2=0.0630919804127125 --dropout_rate3=0.41035611911019004 --hidden1=21 --hidden2=102 --hidden3=32 --learning_rate=0.016717582949612616 --time_step=10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdazzling-sweep-601\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/9892mv5r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143102-9892mv5r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:31:04.482428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:04,815 - wandb.wandb_agent - INFO - Running runs: ['9892mv5r']\n",
            "2021-08-19 14:31:04.489578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:04.490166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:04.491382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:04.492085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:04.492695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:05.128718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:05.129404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:05.129969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:05.130494: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:31:05.130544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:31:05.157994: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:31:05.158027: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:31:05.158107: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:31:05.295026: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:31:05.295190: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:31:05.480343: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:31:09.444313: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:03 - loss: 9.9188 - mse: 9.91882021-08-19 14:31:10.115812: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:31:10.115866: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 5.6672 - mse: 5.6672 2021-08-19 14:31:10.376954: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:31:10.377651: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:31:10.530960: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 431 callback api events and 428 activity events. \n",
            "2021-08-19 14:31:10.537893: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:31:10.548004: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10\n",
            "\n",
            "2021-08-19 14:31:10.555516: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:31:10.571894: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10\n",
            "\n",
            "2021-08-19 14:31:10.575627: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:31:10.576316: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143102-9892mv5r/files/train/plugins/profile/2021_08_19_14_31_10/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 4.6302 - mse: 4.6302WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0097s vs `on_train_batch_begin` time: 0.0408s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0097s vs `on_train_batch_end` time: 0.0569s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 27ms/step - loss: 0.9015 - mse: 0.9015 - val_loss: 2.3394 - val_mse: 2.3394\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.4631 - mse: 0.4631 - val_loss: 1.3243 - val_mse: 1.3243\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2152 - mse: 0.2152 - val_loss: 0.8687 - val_mse: 0.8687\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1493 - mse: 0.1493 - val_loss: 0.7389 - val_mse: 0.7389\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1146 - mse: 0.1146 - val_loss: 0.6969 - val_mse: 0.6969\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0826 - mse: 0.0826 - val_loss: 0.9167 - val_mse: 0.9167\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.7717 - val_mse: 0.7717\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.8476 - val_mse: 0.8476\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.6994 - val_mse: 0.6994\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.7226 - val_mse: 0.7226\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.7683 - val_mse: 0.7683\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.6636 - val_mse: 0.6636\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.6881 - val_mse: 0.6881\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.7339 - val_mse: 0.7339\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.5977 - val_mse: 0.5977\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.6858 - val_mse: 0.6858\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.6985 - val_mse: 0.6985\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.6337 - val_mse: 0.6337\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5723 - val_mse: 0.5723\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.6855 - val_mse: 0.6855\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.6680 - val_mse: 0.6680\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5622 - val_mse: 0.5622\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5485 - val_mse: 0.5485\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5233 - val_mse: 0.5233\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5638 - val_mse: 0.5638\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.5804 - val_mse: 0.5804\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5863 - val_mse: 0.5863\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.5550 - val_mse: 0.5550\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5851 - val_mse: 0.5851\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.5185 - val_mse: 0.5185\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5526 - val_mse: 0.5526\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5412 - val_mse: 0.5412\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5679 - val_mse: 0.5679\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5146 - val_mse: 0.5146\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.6004 - val_mse: 0.6004\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5063 - val_mse: 0.5063\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.5645 - val_mse: 0.5645\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5218 - val_mse: 0.5218\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.4884 - val_mse: 0.4884\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5368 - val_mse: 0.5368\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.5724 - val_mse: 0.5724\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.6978 - val_mse: 0.6978\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5654 - val_mse: 0.5654\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.5086 - val_mse: 0.5086\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.6626 - val_mse: 0.6626\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.4896 - val_mse: 0.4896\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5942 - val_mse: 0.5942\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.4893 - val_mse: 0.4893\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5283 - val_mse: 0.5283\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 138319\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143102-9892mv5r/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143102-9892mv5r/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 48\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01572\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01572\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.52826\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.52826\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383499\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 48\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.48842\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▄▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▄▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdazzling-sweep-601\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/9892mv5r\u001b[0m\n",
            "2021-08-19 14:31:45,507 - wandb.wandb_agent - INFO - Cleaning up finished run: 9892mv5r\n",
            "2021-08-19 14:31:45,964 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:31:45,964 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.12583684749615753\n",
            "\tdropout_rate2: 0.3367458457215153\n",
            "\tdropout_rate3: 0.33427275559559355\n",
            "\thidden1: 18\n",
            "\thidden2: 91\n",
            "\thidden3: 23\n",
            "\tlearning_rate: 0.03632341052501544\n",
            "\ttime_step: 10\n",
            "2021-08-19 14:31:45,966 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.12583684749615753 --dropout_rate2=0.3367458457215153 --dropout_rate3=0.33427275559559355 --hidden1=18 --hidden2=91 --hidden3=23 --learning_rate=0.03632341052501544 --time_step=10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcosmic-sweep-602\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/qd33mrjm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143148-qd33mrjm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:31:50.694920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:50.702176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:50.702798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:50,975 - wandb.wandb_agent - INFO - Running runs: ['qd33mrjm']\n",
            "2021-08-19 14:31:50.703995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:50.704618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:50.705182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:51.363397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:51.364077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:51.364766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:31:51.365312: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:31:51.365375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:31:51.396255: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:31:51.396289: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:31:51.396330: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:31:51.533087: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:31:51.533252: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:31:51.704633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:31:55.711266: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:03 - loss: 9.4453 - mse: 9.44532021-08-19 14:31:56.332571: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:31:56.332615: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.3709 - mse: 5.3709 2021-08-19 14:31:56.603870: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:31:56.604380: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:31:56.753328: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 448 callback api events and 445 activity events. \n",
            "2021-08-19 14:31:56.760150: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:31:56.769777: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56\n",
            "\n",
            "2021-08-19 14:31:56.776994: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:31:56.792429: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56\n",
            "\n",
            "2021-08-19 14:31:56.795568: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:31:56.796076: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143148-qd33mrjm/files/train/plugins/profile/2021_08_19_14_31_56/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 4.0881 - mse: 4.0881WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_begin` time: 0.0426s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0553s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 0.8670 - mse: 0.8670 - val_loss: 2.5799 - val_mse: 2.5799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.5154 - mse: 0.5154 - val_loss: 2.5195 - val_mse: 2.5195\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4241 - mse: 0.4241 - val_loss: 2.5316 - val_mse: 2.5316\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3777 - mse: 0.3777 - val_loss: 2.4734 - val_mse: 2.4734\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3548 - mse: 0.3548 - val_loss: 2.3960 - val_mse: 2.3960\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3578 - mse: 0.3578 - val_loss: 2.4558 - val_mse: 2.4558\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3471 - mse: 0.3471 - val_loss: 2.4108 - val_mse: 2.4108\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3489 - mse: 0.3489 - val_loss: 2.2989 - val_mse: 2.2989\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.3509 - mse: 0.3509 - val_loss: 2.4508 - val_mse: 2.4508\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3415 - mse: 0.3415 - val_loss: 2.3606 - val_mse: 2.3606\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3384 - mse: 0.3384 - val_loss: 2.3479 - val_mse: 2.3479\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3406 - mse: 0.3406 - val_loss: 2.3713 - val_mse: 2.3713\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3431 - mse: 0.3431 - val_loss: 2.4759 - val_mse: 2.4759\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3406 - mse: 0.3406 - val_loss: 2.4861 - val_mse: 2.4861\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3437 - mse: 0.3437 - val_loss: 2.4103 - val_mse: 2.4103\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3409 - mse: 0.3409 - val_loss: 2.3894 - val_mse: 2.3894\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3408 - mse: 0.3408 - val_loss: 2.4093 - val_mse: 2.4093\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3380 - mse: 0.3380 - val_loss: 2.4233 - val_mse: 2.4233\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 138652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143148-qd33mrjm/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143148-qd33mrjm/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.33795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.33795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.42334\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.42334\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.29889\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▆▇▅▃▅▄▁▅▃▂▃▅▆▄▃▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▆▇▅▃▅▄▁▅▃▂▃▅▆▄▃▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcosmic-sweep-602\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/qd33mrjm\u001b[0m\n",
            "2021-08-19 14:32:16,403 - wandb.wandb_agent - INFO - Cleaning up finished run: qd33mrjm\n",
            "2021-08-19 14:32:16,769 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:32:16,769 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.4847996400141674\n",
            "\tdropout_rate2: 0.42135006238252154\n",
            "\tdropout_rate3: 0.06699316329939947\n",
            "\thidden1: 21\n",
            "\thidden2: 13\n",
            "\thidden3: 116\n",
            "\tlearning_rate: 0.08479700467161304\n",
            "\ttime_step: 45\n",
            "2021-08-19 14:32:16,771 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.4847996400141674 --dropout_rate2=0.42135006238252154 --dropout_rate3=0.06699316329939947 --hidden1=21 --hidden2=13 --hidden3=116 --learning_rate=0.08479700467161304 --time_step=45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwise-sweep-603\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/hanfqfls\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143219-hanfqfls\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:32:21.473909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:21.481331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:21.481959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:21,781 - wandb.wandb_agent - INFO - Running runs: ['hanfqfls']\n",
            "2021-08-19 14:32:21.483176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:21.483805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:21.484341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:22.153868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:22.154550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:22.155117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:22.155642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:32:22.155698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:32:22.191230: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:32:22.191268: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:32:22.191396: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:32:22.328447: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:32:22.328612: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:32:22.497468: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:32:26.504522: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 6:01 - loss: 11.1579 - mse: 11.15792021-08-19 14:32:27.168566: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:32:27.168609: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 32s - loss: 27.8540 - mse: 27.8540 2021-08-19 14:32:27.437511: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:32:27.438560: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:32:27.591095: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 917 callback api events and 914 activity events. \n",
            "2021-08-19 14:32:27.602833: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:32:27.619544: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27\n",
            "\n",
            "2021-08-19 14:32:27.632034: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:32:27.649310: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27\n",
            "\n",
            "2021-08-19 14:32:27.652185: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:32:27.652945: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143219-hanfqfls/files/train/plugins/profile/2021_08_19_14_32_27/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 24s - loss: 20.8844 - mse: 20.8844WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_begin` time: 0.0416s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_end` time: 0.0647s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 28ms/step - loss: 1.4795 - mse: 1.4795 - val_loss: 2.0305 - val_mse: 2.0305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4283 - mse: 0.4283 - val_loss: 2.3839 - val_mse: 2.3839\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4163 - mse: 0.4163 - val_loss: 3.1282 - val_mse: 3.1282\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.4107 - mse: 0.4107 - val_loss: 3.1957 - val_mse: 3.1957\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.4144 - mse: 0.4144 - val_loss: 1.9824 - val_mse: 1.9824\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.4508 - mse: 0.4508 - val_loss: 2.0329 - val_mse: 2.0329\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4173 - mse: 0.4173 - val_loss: 2.0401 - val_mse: 2.0401\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4015 - mse: 0.4015 - val_loss: 2.4712 - val_mse: 2.4712\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.4095 - mse: 0.4095 - val_loss: 2.3500 - val_mse: 2.3500\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.4029 - mse: 0.4029 - val_loss: 2.8481 - val_mse: 2.8481\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3917 - mse: 0.3917 - val_loss: 2.2592 - val_mse: 2.2592\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.3878 - mse: 0.3878 - val_loss: 1.9906 - val_mse: 1.9906\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3744 - mse: 0.3744 - val_loss: 3.1895 - val_mse: 3.1895\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.4373 - mse: 0.4373 - val_loss: 3.0816 - val_mse: 3.0816\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4174 - mse: 0.4174 - val_loss: 3.3093 - val_mse: 3.3093\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 138828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143219-hanfqfls/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143219-hanfqfls/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.41737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.41737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 3.30929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 3.30929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383560\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 1.98239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▁▃▇▇▁▁▁▄▃▆▂▁▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▁▃▇▇▁▁▁▄▃▆▂▁▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▂▃▄▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▂▃▄▄▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwise-sweep-603\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/hanfqfls\u001b[0m\n",
            "2021-08-19 14:32:47,200 - wandb.wandb_agent - INFO - Cleaning up finished run: hanfqfls\n",
            "2021-08-19 14:32:48,038 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:32:48,038 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3394143185299437\n",
            "\tdropout_rate2: 0.054734572089476435\n",
            "\tdropout_rate3: 0.2506795577124936\n",
            "\thidden1: 14\n",
            "\thidden2: 96\n",
            "\thidden3: 68\n",
            "\tlearning_rate: 0.011482939668972266\n",
            "\ttime_step: 24\n",
            "2021-08-19 14:32:48,040 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3394143185299437 --dropout_rate2=0.054734572089476435 --dropout_rate3=0.2506795577124936 --hidden1=14 --hidden2=96 --hidden3=68 --learning_rate=0.011482939668972266 --time_step=24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-604\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/sneeyyh5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143250-sneeyyh5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:32:53,050 - wandb.wandb_agent - INFO - Running runs: ['sneeyyh5']\n",
            "2021-08-19 14:32:52.751029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:52.758143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:52.758779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:52.760052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:52.760674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:52.761224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:53.416529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:53.417192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:53.417849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:32:53.418390: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:32:53.418438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:32:53.446899: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:32:53.446930: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:32:53.447006: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:32:53.582161: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:32:53.582329: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:32:53.758057: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:32:57.885162: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:13 - loss: 9.9746 - mse: 9.97462021-08-19 14:32:58.551291: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:32:58.551345: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 34s - loss: 5.7382 - mse: 5.7382 2021-08-19 14:32:58.814542: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:32:58.815168: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:32:58.965803: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 608 callback api events and 605 activity events. \n",
            "2021-08-19 14:32:58.974835: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:32:58.989269: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58\n",
            "\n",
            "2021-08-19 14:32:58.997587: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:32:59.013206: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58\n",
            "\n",
            "2021-08-19 14:32:59.015903: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:32:59.016451: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143250-sneeyyh5/files/train/plugins/profile/2021_08_19_14_32_58/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 25s - loss: 5.0135 - mse: 5.0135WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_begin` time: 0.0414s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0624s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 25ms/step - loss: 0.6888 - mse: 0.6888 - val_loss: 2.5108 - val_mse: 2.5108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2794 - mse: 0.2794 - val_loss: 0.8953 - val_mse: 0.8953\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1167 - mse: 0.1167 - val_loss: 1.3413 - val_mse: 1.3413\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.7893 - val_mse: 0.7893\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.9004 - val_mse: 0.9004\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0622 - mse: 0.0622 - val_loss: 0.9670 - val_mse: 0.9670\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.6996 - val_mse: 0.6996\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0597 - mse: 0.0597 - val_loss: 0.7326 - val_mse: 0.7326\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.7077 - val_mse: 0.7077\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.9728 - val_mse: 0.9728\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.6385 - val_mse: 0.6385\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.6775 - val_mse: 0.6775\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.8118 - val_mse: 0.8118\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.7949 - val_mse: 0.7949\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.6906 - val_mse: 0.6906\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.5861 - val_mse: 0.5861\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.7405 - val_mse: 0.7405\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.6880 - val_mse: 0.6880\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.7196 - val_mse: 0.7196\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.7191 - val_mse: 0.7191\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.7126 - val_mse: 0.7126\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.6385 - val_mse: 0.6385\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.7880 - val_mse: 0.7880\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.6666 - val_mse: 0.6666\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.8006 - val_mse: 0.8006\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.6415 - val_mse: 0.6415\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 138989\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143250-sneeyyh5/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143250-sneeyyh5/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.64149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.64149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.58605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▂▄▂▂▂▁▂▁▂▁▁▂▂▁▁▂▁▁▁▁▁▂▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▂▄▂▂▂▁▂▁▂▁▁▂▂▁▁▂▁▁▁▁▁▂▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msunny-sweep-604\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/sneeyyh5\u001b[0m\n",
            "2021-08-19 14:33:43,847 - wandb.wandb_agent - INFO - Cleaning up finished run: sneeyyh5\n",
            "2021-08-19 14:33:44,401 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:33:44,401 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3154403218396267\n",
            "\tdropout_rate2: 0.40993178872900904\n",
            "\tdropout_rate3: 0.06451800828548948\n",
            "\thidden1: 38\n",
            "\thidden2: 95\n",
            "\thidden3: 114\n",
            "\tlearning_rate: 0.018660744298339388\n",
            "\ttime_step: 5\n",
            "2021-08-19 14:33:44,403 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3154403218396267 --dropout_rate2=0.40993178872900904 --dropout_rate3=0.06451800828548948 --hidden1=38 --hidden2=95 --hidden3=114 --learning_rate=0.018660744298339388 --time_step=5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mzesty-sweep-605\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/xzur7j61\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143346-xzur7j61\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:33:49.131537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.138636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.139232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49,413 - wandb.wandb_agent - INFO - Running runs: ['xzur7j61']\n",
            "2021-08-19 14:33:49.140447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.141042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.141626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.775574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.776209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.776805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:33:49.777322: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:33:49.777385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:33:49.806889: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:33:49.806921: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:33:49.807009: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:33:49.940896: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:33:49.941064: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:33:50.127860: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:33:54.111206: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:03 - loss: 10.7481 - mse: 10.74812021-08-19 14:33:54.777178: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:33:54.777223: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 6.0761 - mse: 6.0761   2021-08-19 14:33:55.039693: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:33:55.040319: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:33:55.191282: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 382 callback api events and 379 activity events. \n",
            "2021-08-19 14:33:55.197666: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:33:55.206669: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55\n",
            "\n",
            "2021-08-19 14:33:55.213188: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:33:55.229279: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55\n",
            "\n",
            "2021-08-19 14:33:55.234851: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:33:55.235613: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143346-xzur7j61/files/train/plugins/profile/2021_08_19_14_33_55/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 17.8485 - mse: 17.8485WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_begin` time: 0.0418s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0577s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 0.8531 - val_mse: 0.8531\n",
            "Epoch 2/200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0529 - mse: 0.0529 - val_loss: 0.7034 - val_mse: 0.7034\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.5548 - val_mse: 0.5548\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.4371 - val_mse: 0.4371\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.3247 - val_mse: 0.3247\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.4465 - val_mse: 0.4465\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.3854 - val_mse: 0.3854\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.4587 - val_mse: 0.4587\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.3864 - val_mse: 0.3864\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.4356 - val_mse: 0.4356\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.5206 - val_mse: 0.5206\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.4629 - val_mse: 0.4629\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.5212 - val_mse: 0.5212\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.4473 - val_mse: 0.4473\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.4430 - val_mse: 0.4430\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 139207\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143346-xzur7j61/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143346-xzur7j61/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.0167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.44298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.44298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.32468\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▆▄▂▁▃▂▃▂▂▄▃▄▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▆▄▂▁▃▂▃▂▂▄▃▄▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mzesty-sweep-605\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/xzur7j61\u001b[0m\n",
            "2021-08-19 14:34:14,826 - wandb.wandb_agent - INFO - Cleaning up finished run: xzur7j61\n",
            "2021-08-19 14:34:15,544 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:34:15,544 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.09568844874242108\n",
            "\tdropout_rate2: 0.2512527642898935\n",
            "\tdropout_rate3: 0.10417488014246964\n",
            "\thidden1: 14\n",
            "\thidden2: 24\n",
            "\thidden3: 88\n",
            "\tlearning_rate: 0.021776670219420386\n",
            "\ttime_step: 36\n",
            "2021-08-19 14:34:15,546 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.09568844874242108 --dropout_rate2=0.2512527642898935 --dropout_rate3=0.10417488014246964 --hidden1=14 --hidden2=24 --hidden3=88 --learning_rate=0.021776670219420386 --time_step=36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mautumn-sweep-606\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/4on7wk1x\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143418-4on7wk1x\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:34:20,556 - wandb.wandb_agent - INFO - Running runs: ['4on7wk1x']\n",
            "2021-08-19 14:34:20.298545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.305904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.306528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.307797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.308402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.308957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.935900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.936609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.937182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:34:20.937730: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:34:20.937784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:34:20.967573: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:34:20.967606: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:34:20.967644: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:34:21.104058: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:34:21.104247: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:34:21.298119: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:34:25.297080: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:59 - loss: 9.6899 - mse: 9.68992021-08-19 14:34:25.951548: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:34:25.951592: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 32s - loss: 5.2132 - mse: 5.2132 2021-08-19 14:34:26.220617: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:34:26.221414: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:34:26.373541: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 778 callback api events and 775 activity events. \n",
            "2021-08-19 14:34:26.383184: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:34:26.397895: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26\n",
            "\n",
            "2021-08-19 14:34:26.408013: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:34:26.424642: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26\n",
            "\n",
            "2021-08-19 14:34:26.427327: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:34:26.427895: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143418-4on7wk1x/files/train/plugins/profile/2021_08_19_14_34_26/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 24s - loss: 7.0039 - mse: 7.0039WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0121s vs `on_train_batch_begin` time: 0.0426s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0121s vs `on_train_batch_end` time: 0.0596s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 27ms/step - loss: 0.7172 - mse: 0.7172 - val_loss: 2.3659 - val_mse: 2.3659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.2278 - mse: 0.2278 - val_loss: 1.2132 - val_mse: 1.2132\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 1.0088 - val_mse: 1.0088\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.8101 - val_mse: 0.8101\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 1.0705 - val_mse: 1.0705\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.9634 - val_mse: 0.9634\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.9087 - val_mse: 0.9087\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.8859 - val_mse: 0.8859\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.9082 - val_mse: 0.9082\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.7895 - val_mse: 0.7895\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.7508 - val_mse: 0.7508\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.8144 - val_mse: 0.8144\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.6747 - val_mse: 0.6747\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.7319 - val_mse: 0.7319\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.7249 - val_mse: 0.7249\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.7528 - val_mse: 0.7528\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.6715 - val_mse: 0.6715\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.7504 - val_mse: 0.7504\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.6625 - val_mse: 0.6625\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.6919 - val_mse: 0.6919\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.7820 - val_mse: 0.7820\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.5872 - val_mse: 0.5872\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.7607 - val_mse: 0.7607\n",
            "Epoch 24/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.6965 - val_mse: 0.6965\n",
            "Epoch 25/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.6334 - val_mse: 0.6334\n",
            "Epoch 26/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.7042 - val_mse: 0.7042\n",
            "Epoch 27/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.6914 - val_mse: 0.6914\n",
            "Epoch 28/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.6395 - val_mse: 0.6395\n",
            "Epoch 29/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.7124 - val_mse: 0.7124\n",
            "Epoch 30/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.6570 - val_mse: 0.6570\n",
            "Epoch 31/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.6144 - val_mse: 0.6144\n",
            "Epoch 32/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.6678 - val_mse: 0.6678\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 139370\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143418-4on7wk1x/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143418-4on7wk1x/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.66784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.66784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.58719\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mautumn-sweep-606\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/4on7wk1x\u001b[0m\n",
            "2021-08-19 14:34:56,143 - wandb.wandb_agent - INFO - Cleaning up finished run: 4on7wk1x\n",
            "2021-08-19 14:34:56,740 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:34:56,740 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.13194413283978673\n",
            "\tdropout_rate2: 0.22128990170170998\n",
            "\tdropout_rate3: 0.14682454580503124\n",
            "\thidden1: 20\n",
            "\thidden2: 92\n",
            "\thidden3: 82\n",
            "\tlearning_rate: 0.014135202196311945\n",
            "\ttime_step: 29\n",
            "2021-08-19 14:34:56,742 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.13194413283978673 --dropout_rate2=0.22128990170170998 --dropout_rate3=0.14682454580503124 --hidden1=20 --hidden2=92 --hidden3=82 --learning_rate=0.014135202196311945 --time_step=29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mleafy-sweep-607\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/mpug6x9y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143459-mpug6x9y\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:35:01.442716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:01,752 - wandb.wandb_agent - INFO - Running runs: ['mpug6x9y']\n",
            "2021-08-19 14:35:01.449908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:01.450525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:01.451773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:01.452424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:01.452973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:02.106822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:02.107516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:02.108107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:02.108656: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:35:02.108704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:35:02.137840: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:35:02.137873: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:35:02.137907: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:35:02.274547: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:35:02.275220: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:35:02.461709: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:35:06.468674: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:03 - loss: 8.7240 - mse: 8.72402021-08-19 14:35:07.096197: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:35:07.096241: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 4.5033 - mse: 4.5033 2021-08-19 14:35:07.365233: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:35:07.365820: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:35:07.517370: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 692 callback api events and 689 activity events. \n",
            "2021-08-19 14:35:07.526808: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:35:07.540785: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07\n",
            "\n",
            "2021-08-19 14:35:07.550470: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:35:07.566525: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07\n",
            "\n",
            "2021-08-19 14:35:07.569455: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:35:07.570359: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143459-mpug6x9y/files/train/plugins/profile/2021_08_19_14_35_07/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 3.1220 - mse: 3.1220WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_begin` time: 0.0427s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0582s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 27ms/step - loss: 0.6434 - mse: 0.6434 - val_loss: 2.2384 - val_mse: 2.2384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4224 - mse: 0.4224 - val_loss: 2.8206 - val_mse: 2.8206\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4192 - mse: 0.4192 - val_loss: 2.0950 - val_mse: 2.0950\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.2933 - mse: 0.2933 - val_loss: 1.3089 - val_mse: 1.3089\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.0974 - mse: 0.0974 - val_loss: 1.2676 - val_mse: 1.2676\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.0707 - mse: 0.0707 - val_loss: 1.2243 - val_mse: 1.2243\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0666 - mse: 0.0666 - val_loss: 1.0590 - val_mse: 1.0590\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0551 - mse: 0.0551 - val_loss: 0.9564 - val_mse: 0.9564\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 1.0063 - val_mse: 1.0063\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0574 - mse: 0.0574 - val_loss: 0.7495 - val_mse: 0.7495\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0523 - mse: 0.0523 - val_loss: 1.2983 - val_mse: 1.2983\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0549 - mse: 0.0549 - val_loss: 0.9670 - val_mse: 0.9670\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.9262 - val_mse: 0.9262\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.7503 - val_mse: 0.7503\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.9759 - val_mse: 0.9759\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.9536 - val_mse: 0.9536\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.7946 - val_mse: 0.7946\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.6360 - val_mse: 0.6360\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.7600 - val_mse: 0.7600\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.6948 - val_mse: 0.6948\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.6982 - val_mse: 0.6982\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.8734 - val_mse: 0.8734\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.5396 - val_mse: 0.5396\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.7438 - val_mse: 0.7438\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.6730 - val_mse: 0.6730\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.6457 - val_mse: 0.6457\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.7571 - val_mse: 0.7571\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.6102 - val_mse: 0.6102\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.6025 - val_mse: 0.6025\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.6572 - val_mse: 0.6572\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.6109 - val_mse: 0.6109\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.6181 - val_mse: 0.6181\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.6215 - val_mse: 0.6215\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 139616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143459-mpug6x9y/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143459-mpug6x9y/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.0094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.62152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.62152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383730\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.53957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▆▆▄▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▆▆▄▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▆█▆▃▃▃▃▂▂▂▃▂▂▂▂▂▂▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▆█▆▃▃▃▃▂▂▂▃▂▂▂▂▂▂▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mleafy-sweep-607\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/mpug6x9y\u001b[0m\n",
            "2021-08-19 14:35:37,365 - wandb.wandb_agent - INFO - Cleaning up finished run: mpug6x9y\n",
            "2021-08-19 14:35:37,827 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:35:37,827 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.28700659577084686\n",
            "\tdropout_rate2: 0.2266225321513159\n",
            "\tdropout_rate3: 0.15675505513280974\n",
            "\thidden1: 72\n",
            "\thidden2: 50\n",
            "\thidden3: 120\n",
            "\tlearning_rate: 0.01683127125875637\n",
            "\ttime_step: 42\n",
            "2021-08-19 14:35:37,829 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.28700659577084686 --dropout_rate2=0.2266225321513159 --dropout_rate3=0.15675505513280974 --hidden1=72 --hidden2=50 --hidden3=120 --learning_rate=0.01683127125875637 --time_step=42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcurious-sweep-608\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/spl7fnxc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143540-spl7fnxc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:35:42.609419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:42.616516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:42.617187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:42,839 - wandb.wandb_agent - INFO - Running runs: ['spl7fnxc']\n",
            "2021-08-19 14:35:42.618397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:42.619002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:42.619578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:43.267465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:43.268190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:43.268799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:35:43.269349: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:35:43.269411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:35:43.300137: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:35:43.300170: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:35:43.300203: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:35:43.434860: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:35:43.435022: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:35:43.622091: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:35:47.575179: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:56 - loss: 10.7755 - mse: 10.77552021-08-19 14:35:48.225965: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:35:48.226010: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 31s - loss: 9.6750 - mse: 9.6750   2021-08-19 14:35:48.490190: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:35:48.490856: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:35:48.642036: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 858 callback api events and 855 activity events. \n",
            "2021-08-19 14:35:48.652091: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:35:48.669179: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48\n",
            "\n",
            "2021-08-19 14:35:48.680038: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:35:48.696299: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48\n",
            "\n",
            "2021-08-19 14:35:48.699045: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:35:48.699627: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143540-spl7fnxc/files/train/plugins/profile/2021_08_19_14_35_48/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 24s - loss: 9.9735 - mse: 9.9735WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_begin` time: 0.0415s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0606s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 28ms/step - loss: 0.8503 - mse: 0.8503 - val_loss: 2.6940 - val_mse: 2.6940\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.3890 - mse: 0.3890 - val_loss: 2.3975 - val_mse: 2.3975\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.3961 - mse: 0.3961 - val_loss: 2.3325 - val_mse: 2.3325\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.3886 - mse: 0.3886 - val_loss: 2.2758 - val_mse: 2.2758\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.3893 - mse: 0.3893 - val_loss: 2.4327 - val_mse: 2.4327\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3969 - mse: 0.3969 - val_loss: 2.6108 - val_mse: 2.6108\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.3873 - mse: 0.3873 - val_loss: 2.3616 - val_mse: 2.3616\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3226 - mse: 0.3226 - val_loss: 2.6762 - val_mse: 2.6762\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.1343 - mse: 0.1343 - val_loss: 1.4160 - val_mse: 1.4160\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.8594 - val_mse: 0.8594\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.9447 - val_mse: 0.9447\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 1.0080 - val_mse: 1.0080\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 1.0216 - val_mse: 1.0216\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.7353 - val_mse: 0.7353\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.9264 - val_mse: 0.9264\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.8840 - val_mse: 0.8840\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.8703 - val_mse: 0.8703\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.9046 - val_mse: 0.9046\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.9455 - val_mse: 0.9455\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.8505 - val_mse: 0.8505\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.9630 - val_mse: 0.9630\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.8636 - val_mse: 0.8636\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.7617 - val_mse: 0.7617\n",
            "Epoch 24/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.7250 - val_mse: 0.7250\n",
            "Epoch 25/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.8820 - val_mse: 0.8820\n",
            "Epoch 26/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.8265 - val_mse: 0.8265\n",
            "Epoch 27/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.8652 - val_mse: 0.8652\n",
            "Epoch 28/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.8504 - val_mse: 0.8504\n",
            "Epoch 29/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.8536 - val_mse: 0.8536\n",
            "Epoch 30/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.8496 - val_mse: 0.8496\n",
            "Epoch 31/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.7485 - val_mse: 0.7485\n",
            "Epoch 32/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.7125 - val_mse: 0.7125\n",
            "Epoch 33/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.8523 - val_mse: 0.8523\n",
            "Epoch 34/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.8718 - val_mse: 0.8718\n",
            "Epoch 35/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.7220 - val_mse: 0.7220\n",
            "Epoch 36/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.8372 - val_mse: 0.8372\n",
            "Epoch 37/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.8692 - val_mse: 0.8692\n",
            "Epoch 38/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.7716 - val_mse: 0.7716\n",
            "Epoch 39/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.7584 - val_mse: 0.7584\n",
            "Epoch 40/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.7711 - val_mse: 0.7711\n",
            "Epoch 41/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.7828 - val_mse: 0.7828\n",
            "Epoch 42/200\n",
            "81/81 [==============================] - 1s 10ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.7458 - val_mse: 0.7458\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 139869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143540-spl7fnxc/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143540-spl7fnxc/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0074\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.0074\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.74579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.74579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383782\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.71251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▄▄▄▄▄▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▄▄▄▄▄▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▇▇▇▇█▇█▃▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse █▇▇▇▇█▇█▃▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcurious-sweep-608\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/spl7fnxc\u001b[0m\n",
            "2021-08-19 14:37:14,255 - wandb.wandb_agent - INFO - Cleaning up finished run: spl7fnxc\n",
            "2021-08-19 14:37:14,688 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:37:14,688 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.24134184456579516\n",
            "\tdropout_rate2: 0.06665138439623981\n",
            "\tdropout_rate3: 0.46655997293569357\n",
            "\thidden1: 88\n",
            "\thidden2: 74\n",
            "\thidden3: 55\n",
            "\tlearning_rate: 0.009435162933352512\n",
            "\ttime_step: 6\n",
            "2021-08-19 14:37:14,690 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.24134184456579516 --dropout_rate2=0.06665138439623981 --dropout_rate3=0.46655997293569357 --hidden1=88 --hidden2=74 --hidden3=55 --learning_rate=0.009435162933352512 --time_step=6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbrisk-sweep-609\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/a7lh31b3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143717-a7lh31b3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:37:19.464896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:19.475009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:19.475956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:19,700 - wandb.wandb_agent - INFO - Running runs: ['a7lh31b3']\n",
            "2021-08-19 14:37:19.477677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:19.478608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:19.479489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:20.140506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:20.141155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:20.141755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:37:20.142285: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:37:20.142334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:37:20.171741: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:37:20.171776: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:37:20.171816: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:37:20.306084: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:37:20.306248: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:37:20.494642: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:37:24.497758: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:04 - loss: 10.4019 - mse: 10.40192021-08-19 14:37:25.139571: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:37:25.139612: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 6.7779 - mse: 6.7779   2021-08-19 14:37:25.401546: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:37:25.402001: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:37:25.552600: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 381 callback api events and 378 activity events. \n",
            "2021-08-19 14:37:25.560073: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:37:25.569216: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25\n",
            "\n",
            "2021-08-19 14:37:25.575848: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:37:25.590780: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25\n",
            "\n",
            "2021-08-19 14:37:25.593730: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:37:25.594249: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143717-a7lh31b3/files/train/plugins/profile/2021_08_19_14_37_25/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 5.0059 - mse: 5.0059WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_begin` time: 0.0414s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_end` time: 0.0558s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 0.5936 - mse: 0.5936 - val_loss: 0.7108 - val_mse: 0.7108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.2137 - mse: 0.2137 - val_loss: 0.7460 - val_mse: 0.7460\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1959 - mse: 0.1959 - val_loss: 0.5550 - val_mse: 0.5550\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1535 - mse: 0.1535 - val_loss: 0.6262 - val_mse: 0.6262\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1448 - mse: 0.1448 - val_loss: 0.8379 - val_mse: 0.8379\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1168 - mse: 0.1168 - val_loss: 0.7395 - val_mse: 0.7395\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.6194 - val_mse: 0.6194\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.7316 - val_mse: 0.7316\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.5959 - val_mse: 0.5959\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.6650 - val_mse: 0.6650\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0640 - mse: 0.0640 - val_loss: 0.5555 - val_mse: 0.5555\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0565 - mse: 0.0565 - val_loss: 0.6021 - val_mse: 0.6021\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.5076 - val_mse: 0.5076\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.4910 - val_mse: 0.4910\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.5396 - val_mse: 0.5396\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.5333 - val_mse: 0.5333\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.4606 - val_mse: 0.4606\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.5330 - val_mse: 0.5330\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.5012 - val_mse: 0.5012\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.5173 - val_mse: 0.5173\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.5408 - val_mse: 0.5408\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.4465 - val_mse: 0.4465\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.4305 - val_mse: 0.4305\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.4459 - val_mse: 0.4459\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.4243 - val_mse: 0.4243\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.3970 - val_mse: 0.3970\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.4271 - val_mse: 0.4271\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.3996 - val_mse: 0.3996\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.4352 - val_mse: 0.4352\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.3929 - val_mse: 0.3929\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.4208 - val_mse: 0.4208\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.4443 - val_mse: 0.4443\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.4114 - val_mse: 0.4114\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.3844 - val_mse: 0.3844\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.4354 - val_mse: 0.4354\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.3999 - val_mse: 0.3999\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.4557 - val_mse: 0.4557\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.4010 - val_mse: 0.4010\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.4623 - val_mse: 0.4623\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.4637 - val_mse: 0.4637\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.5030 - val_mse: 0.5030\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.4141 - val_mse: 0.4141\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.4414 - val_mse: 0.4414\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.3675 - val_mse: 0.3675\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.4017 - val_mse: 0.4017\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.4016 - val_mse: 0.4016\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.3469 - val_mse: 0.3469\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.4087 - val_mse: 0.4087\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.4099 - val_mse: 0.4099\n",
            "Epoch 50/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.4370 - val_mse: 0.4370\n",
            "Epoch 51/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.4113 - val_mse: 0.4113\n",
            "Epoch 52/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.3860 - val_mse: 0.3860\n",
            "Epoch 53/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.4067 - val_mse: 0.4067\n",
            "Epoch 54/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.3791 - val_mse: 0.3791\n",
            "Epoch 55/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.3942 - val_mse: 0.3942\n",
            "Epoch 56/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.4252 - val_mse: 0.4252\n",
            "Epoch 57/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.4522 - val_mse: 0.4522\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 140167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143717-a7lh31b3/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143717-a7lh31b3/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.4522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.4522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.34695\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▆▇▄█▇▆▄▄▄▃▄▃▃▃▄▂▂▂▁▁▂▂▂▁▂▁▁▂▃▂▁▂▂▂▂▂▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▆▇▄█▇▆▄▄▄▃▄▃▃▃▄▂▂▂▁▁▂▂▂▁▂▁▁▂▃▂▁▂▂▂▂▂▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbrisk-sweep-609\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/a7lh31b3\u001b[0m\n",
            "2021-08-19 14:38:10,561 - wandb.wandb_agent - INFO - Cleaning up finished run: a7lh31b3\n",
            "2021-08-19 14:38:11,286 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:38:11,286 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.4843436393639955\n",
            "\tdropout_rate2: 0.1456815608464262\n",
            "\tdropout_rate3: 0.165578838450125\n",
            "\thidden1: 64\n",
            "\thidden2: 106\n",
            "\thidden3: 116\n",
            "\tlearning_rate: 0.02779661895675908\n",
            "\ttime_step: 8\n",
            "2021-08-19 14:38:11,288 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.4843436393639955 --dropout_rate2=0.1456815608464262 --dropout_rate3=0.165578838450125 --hidden1=64 --hidden2=106 --hidden3=116 --learning_rate=0.02779661895675908 --time_step=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgentle-sweep-610\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/pd505zwy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143813-pd505zwy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:38:16.078688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.086013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.086654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16,297 - wandb.wandb_agent - INFO - Running runs: ['pd505zwy']\n",
            "2021-08-19 14:38:16.087880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.088504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.089060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.723473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.724124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.724720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:16.725234: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:38:16.725285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:38:16.755577: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:38:16.755611: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:38:16.755646: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:38:16.897756: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:38:16.897931: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:38:17.068907: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:38:21.106818: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:05 - loss: 9.1428 - mse: 9.14282021-08-19 14:38:21.735033: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:38:21.735151: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 11.5865 - mse: 11.58652021-08-19 14:38:21.994475: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:38:21.994910: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:38:22.164357: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 392 callback api events and 389 activity events. \n",
            "2021-08-19 14:38:22.171751: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:38:22.180782: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22\n",
            "\n",
            "2021-08-19 14:38:22.187177: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:38:22.201176: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22\n",
            "\n",
            "2021-08-19 14:38:22.203967: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:38:22.204543: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143813-pd505zwy/files/train/plugins/profile/2021_08_19_14_38_22/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 9.9222 - mse: 9.9222  WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_begin` time: 0.0413s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_end` time: 0.0586s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 25ms/step - loss: 0.7994 - mse: 0.7994 - val_loss: 2.2448 - val_mse: 2.2448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3830 - mse: 0.3830 - val_loss: 2.2661 - val_mse: 2.2661\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2996 - mse: 0.2996 - val_loss: 1.6693 - val_mse: 1.6693\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1980 - mse: 0.1980 - val_loss: 1.3550 - val_mse: 1.3550\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1345 - mse: 0.1345 - val_loss: 1.1504 - val_mse: 1.1504\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 1.4771 - val_mse: 1.4771\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2067 - mse: 0.2067 - val_loss: 1.5247 - val_mse: 1.5247\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1421 - mse: 0.1421 - val_loss: 1.1491 - val_mse: 1.1491\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 1.3381 - val_mse: 1.3381\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0593 - mse: 0.0593 - val_loss: 1.0507 - val_mse: 1.0507\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0603 - mse: 0.0603 - val_loss: 1.1882 - val_mse: 1.1882\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0453 - mse: 0.0453 - val_loss: 1.1195 - val_mse: 1.1195\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.8911 - val_mse: 0.8911\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.9277 - val_mse: 0.9277\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.8847 - val_mse: 0.8847\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.8984 - val_mse: 0.8984\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.8673 - val_mse: 0.8673\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 1.0012 - val_mse: 1.0012\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.7726 - val_mse: 0.7726\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.5784 - val_mse: 0.5784\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.7095 - val_mse: 0.7095\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.7053 - val_mse: 0.7053\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.7447 - val_mse: 0.7447\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.7946 - val_mse: 0.7946\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.8337 - val_mse: 0.8337\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.7245 - val_mse: 0.7245\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.7915 - val_mse: 0.7915\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.6816 - val_mse: 0.6816\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.6661 - val_mse: 0.6661\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.8616 - val_mse: 0.8616\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 140540\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143813-pd505zwy/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143813-pd505zwy/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01691\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01691\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.86165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.86165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383919\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.57844\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▄▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▄▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ██▆▄▃▅▅▃▄▃▄▃▂▂▂▂▂▃▂▁▂▂▂▂▂▂▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ██▆▄▃▅▅▃▄▃▄▃▂▂▂▂▂▃▂▁▂▂▂▂▂▂▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgentle-sweep-610\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/pd505zwy\u001b[0m\n",
            "2021-08-19 14:38:46,804 - wandb.wandb_agent - INFO - Cleaning up finished run: pd505zwy\n",
            "2021-08-19 14:38:47,389 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:38:47,389 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.2233804521931157\n",
            "\tdropout_rate2: 0.09282446776110143\n",
            "\tdropout_rate3: 0.08322080960810532\n",
            "\thidden1: 87\n",
            "\thidden2: 52\n",
            "\thidden3: 110\n",
            "\tlearning_rate: 0.033507827216679636\n",
            "\ttime_step: 12\n",
            "2021-08-19 14:38:47,391 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.2233804521931157 --dropout_rate2=0.09282446776110143 --dropout_rate3=0.08322080960810532 --hidden1=87 --hidden2=52 --hidden3=110 --learning_rate=0.033507827216679636 --time_step=12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfiery-sweep-611\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/nxzmtrlq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143849-nxzmtrlq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:38:52.047988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.055123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.055742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52,401 - wandb.wandb_agent - INFO - Running runs: ['nxzmtrlq']\n",
            "2021-08-19 14:38:52.056893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.057504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.058051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.720318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.720976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.721589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:38:52.722113: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:38:52.722166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:38:52.757609: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:38:52.757642: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:38:52.757682: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:38:52.889930: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:38:52.890102: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:38:53.062325: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:38:57.045357: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 10.3726 - mse: 10.37262021-08-19 14:38:57.694532: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:38:57.694575: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 33s - loss: 9.7962 - mse: 9.7962   2021-08-19 14:38:57.956932: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:38:57.957640: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:38:58.105509: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 467 callback api events and 464 activity events. \n",
            "2021-08-19 14:38:58.112399: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:38:58.122953: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58\n",
            "\n",
            "2021-08-19 14:38:58.130481: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:38:58.145112: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58\n",
            "\n",
            "2021-08-19 14:38:58.148085: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:38:58.148721: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143849-nxzmtrlq/files/train/plugins/profile/2021_08_19_14_38_58/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 10.0907 - mse: 10.0907WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_begin` time: 0.0417s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0600s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.7517 - mse: 0.7517 - val_loss: 2.3105 - val_mse: 2.3105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3755 - mse: 0.3755 - val_loss: 2.2251 - val_mse: 2.2251\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3946 - mse: 0.3946 - val_loss: 2.7397 - val_mse: 2.7397\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2945 - mse: 0.2945 - val_loss: 2.1527 - val_mse: 2.1527\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0794 - mse: 0.0794 - val_loss: 0.8621 - val_mse: 0.8621\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 1.1161 - val_mse: 1.1161\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.8824 - val_mse: 0.8824\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.8393 - val_mse: 0.8393\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.9090 - val_mse: 0.9090\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 1.0547 - val_mse: 1.0547\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.9150 - val_mse: 0.9150\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.8212 - val_mse: 0.8212\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.8764 - val_mse: 0.8764\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.9343 - val_mse: 0.9343\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.7014 - val_mse: 0.7014\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.9816 - val_mse: 0.9816\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.8292 - val_mse: 0.8292\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.7551 - val_mse: 0.7551\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.8087 - val_mse: 0.8087\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.6934 - val_mse: 0.6934\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.6527 - val_mse: 0.6527\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.6936 - val_mse: 0.6936\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.7025 - val_mse: 0.7025\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.6891 - val_mse: 0.6891\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.7801 - val_mse: 0.7801\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.7779 - val_mse: 0.7779\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.6369 - val_mse: 0.6369\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.7207 - val_mse: 0.7207\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.7057 - val_mse: 0.7057\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.5719 - val_mse: 0.5719\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.6929 - val_mse: 0.6929\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.8395 - val_mse: 0.8395\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.6682 - val_mse: 0.6682\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.6719 - val_mse: 0.6719\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.6860 - val_mse: 0.6860\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.5314 - val_mse: 0.5314\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.7878 - val_mse: 0.7878\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.6186 - val_mse: 0.6186\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.6650 - val_mse: 0.6650\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.6551 - val_mse: 0.6551\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.5906 - val_mse: 0.5906\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.5260 - val_mse: 0.5260\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.5250 - val_mse: 0.5250\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.6760 - val_mse: 0.6760\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.5996 - val_mse: 0.5996\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.6503 - val_mse: 0.6503\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.5348 - val_mse: 0.5348\n",
            "Epoch 48/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.6294 - val_mse: 0.6294\n",
            "Epoch 49/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.6668 - val_mse: 0.6668\n",
            "Epoch 50/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.5395 - val_mse: 0.5395\n",
            "Epoch 51/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.6870 - val_mse: 0.6870\n",
            "Epoch 52/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.6970 - val_mse: 0.6970\n",
            "Epoch 53/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.5532 - val_mse: 0.5532\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 140778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143849-nxzmtrlq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143849-nxzmtrlq/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.00567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.00567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.5532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.5532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629383969\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.52498\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▄▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▄▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▇▆█▂▃▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▇▆█▂▃▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfiery-sweep-611\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/nxzmtrlq\u001b[0m\n",
            "2021-08-19 14:39:38,191 - wandb.wandb_agent - INFO - Cleaning up finished run: nxzmtrlq\n",
            "2021-08-19 14:39:38,594 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:39:38,594 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.48747551626366176\n",
            "\tdropout_rate2: 0.053560087619365905\n",
            "\tdropout_rate3: 0.056372355957574546\n",
            "\thidden1: 89\n",
            "\thidden2: 109\n",
            "\thidden3: 93\n",
            "\tlearning_rate: 0.057575280455152526\n",
            "\ttime_step: 53\n",
            "2021-08-19 14:39:38,596 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.48747551626366176 --dropout_rate2=0.053560087619365905 --dropout_rate3=0.056372355957574546 --hidden1=89 --hidden2=109 --hidden3=93 --learning_rate=0.057575280455152526 --time_step=53\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstilted-sweep-612\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/qbk2pnub\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_143941-qbk2pnub\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:39:43.229137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43,605 - wandb.wandb_agent - INFO - Running runs: ['qbk2pnub']\n",
            "2021-08-19 14:39:43.236547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.237139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.238257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.238887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.239464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.869614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.870258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.870891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:39:43.871422: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:39:43.871474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:39:43.900325: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:39:43.900359: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:39:43.900420: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:39:44.054240: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:39:44.054414: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:39:44.225552: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:39:48.211691: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:57 - loss: 10.2226 - mse: 10.22262021-08-19 14:39:48.862701: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:39:48.862744: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 34s - loss: 11.2185 - mse: 11.2185 2021-08-19 14:39:49.135235: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:39:49.136156: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:39:49.288222: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1030 callback api events and 1027 activity events. \n",
            "2021-08-19 14:39:49.299943: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:39:49.318709: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49\n",
            "\n",
            "2021-08-19 14:39:49.331846: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:39:49.350132: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49\n",
            "\n",
            "2021-08-19 14:39:49.353149: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:39:49.354411: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_143941-qbk2pnub/files/train/plugins/profile/2021_08_19_14_39_49/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 26s - loss: 8.1337 - mse: 8.1337  WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0102s vs `on_train_batch_begin` time: 0.0417s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0102s vs `on_train_batch_end` time: 0.0683s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 31ms/step - loss: 0.8935 - mse: 0.8935 - val_loss: 2.4872 - val_mse: 2.4872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3925 - mse: 0.3925 - val_loss: 2.2086 - val_mse: 2.2086\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3709 - mse: 0.3709 - val_loss: 2.2586 - val_mse: 2.2586\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 13ms/step - loss: 0.4052 - mse: 0.4052 - val_loss: 3.2844 - val_mse: 3.2844\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.4011 - mse: 0.4011 - val_loss: 2.0138 - val_mse: 2.0138\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.4360 - mse: 0.4360 - val_loss: 2.7064 - val_mse: 2.7064\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.4209 - mse: 0.4209 - val_loss: 3.5009 - val_mse: 3.5009\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3877 - mse: 0.3877 - val_loss: 3.4435 - val_mse: 3.4435\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.3863 - mse: 0.3863 - val_loss: 2.7460 - val_mse: 2.7460\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3790 - mse: 0.3790 - val_loss: 2.7808 - val_mse: 2.7808\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3876 - mse: 0.3876 - val_loss: 2.5666 - val_mse: 2.5666\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3709 - mse: 0.3709 - val_loss: 2.4925 - val_mse: 2.4925\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3932 - mse: 0.3932 - val_loss: 2.0094 - val_mse: 2.0094\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3706 - mse: 0.3706 - val_loss: 2.6784 - val_mse: 2.6784\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3736 - mse: 0.3736 - val_loss: 2.4581 - val_mse: 2.4581\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3879 - mse: 0.3879 - val_loss: 2.3254 - val_mse: 2.3254\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.3718 - mse: 0.3718 - val_loss: 2.3948 - val_mse: 2.3948\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3846 - mse: 0.3846 - val_loss: 2.8365 - val_mse: 2.8365\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.3727 - mse: 0.3727 - val_loss: 2.5112 - val_mse: 2.5112\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3806 - mse: 0.3806 - val_loss: 2.3333 - val_mse: 2.3333\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3777 - mse: 0.3777 - val_loss: 2.4245 - val_mse: 2.4245\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3661 - mse: 0.3661 - val_loss: 2.1869 - val_mse: 2.1869\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.3702 - mse: 0.3702 - val_loss: 2.3195 - val_mse: 2.3195\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 141131\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_143941-qbk2pnub/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_143941-qbk2pnub/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.37017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.37017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.31946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.31946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629384012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.00944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▃▂▂▇▁▄██▄▅▄▃▁▄▃▂▃▅▃▃▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▃▂▂▇▁▄██▄▅▄▃▁▄▃▂▃▅▃▃▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mstilted-sweep-612\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/qbk2pnub\u001b[0m\n",
            "2021-08-19 14:40:34,422 - wandb.wandb_agent - INFO - Cleaning up finished run: qbk2pnub\n",
            "2021-08-19 14:40:34,850 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-19 14:40:34,851 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.11038807365538472\n",
            "\tdropout_rate2: 0.10243461751542099\n",
            "\tdropout_rate3: 0.4371205232009681\n",
            "\thidden1: 97\n",
            "\thidden2: 32\n",
            "\thidden3: 30\n",
            "\tlearning_rate: 0.013434487946048006\n",
            "\ttime_step: 23\n",
            "2021-08-19 14:40:34,852 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.11038807365538472 --dropout_rate2=0.10243461751542099 --dropout_rate3=0.4371205232009681 --hidden1=97 --hidden2=32 --hidden3=30 --learning_rate=0.013434487946048006 --time_step=23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-613\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/sweeps/47ty4gib\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kade/gamestop_prediction/runs/90etin4v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210819_144037-90etin4v\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-19 14:40:39.496630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:39,862 - wandb.wandb_agent - INFO - Running runs: ['90etin4v']\n",
            "2021-08-19 14:40:39.503869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:39.504495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:39.505788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:39.506442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:39.507017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:40.134163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:40.134976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:40.135642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-19 14:40:40.136253: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-19 14:40:40.136324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-19 14:40:40.184071: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:40:40.184110: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-19 14:40:40.184242: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-19 14:40:40.351312: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:40:40.351517: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-19 14:40:40.541257: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-19 14:40:44.713445: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:17 - loss: 9.6770 - mse: 9.67702021-08-19 14:40:45.345240: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-19 14:40:45.345289: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 6.3205 - mse: 6.3205 2021-08-19 14:40:45.608195: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-19 14:40:45.608774: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-19 14:40:45.760338: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 601 callback api events and 598 activity events. \n",
            "2021-08-19 14:40:45.768977: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-19 14:40:45.780931: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45\n",
            "\n",
            "2021-08-19 14:40:45.789202: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.trace.json.gz\n",
            "2021-08-19 14:40:45.804051: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45\n",
            "\n",
            "2021-08-19 14:40:45.806786: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.memory_profile.json.gz\n",
            "2021-08-19 14:40:45.807331: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210819_144037-90etin4v/files/train/plugins/profile/2021_08_19_14_40_45/4f63fde49e81.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 4.5213 - mse: 4.5213WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_begin` time: 0.0417s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0561s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.8186 - mse: 0.8186 - val_loss: 2.4330 - val_mse: 2.4330\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5762 - mse: 0.5762 - val_loss: 2.3236 - val_mse: 2.3236\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5412 - mse: 0.5412 - val_loss: 2.5399 - val_mse: 2.5399\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4871 - mse: 0.4871 - val_loss: 2.4624 - val_mse: 2.4624\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4547 - mse: 0.4547 - val_loss: 2.6033 - val_mse: 2.6033\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4411 - mse: 0.4411 - val_loss: 2.4680 - val_mse: 2.4680\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4024 - mse: 0.4024 - val_loss: 2.5579 - val_mse: 2.5579\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3899 - mse: 0.3899 - val_loss: 2.5152 - val_mse: 2.5152\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3724 - mse: 0.3724 - val_loss: 2.4581 - val_mse: 2.4581\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3758 - mse: 0.3758 - val_loss: 2.4400 - val_mse: 2.4400\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3463 - mse: 0.3463 - val_loss: 2.3901 - val_mse: 2.3901\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3126 - mse: 0.3126 - val_loss: 1.7547 - val_mse: 1.7547\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1007 - mse: 0.1007 - val_loss: 1.0846 - val_mse: 1.0846\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0564 - mse: 0.0564 - val_loss: 0.8457 - val_mse: 0.8457\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.8440 - val_mse: 0.8440\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.8507 - val_mse: 0.8507\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.7943 - val_mse: 0.7943\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.7805 - val_mse: 0.7805\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.7113 - val_mse: 0.7113\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.8326 - val_mse: 0.8326\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.8343 - val_mse: 0.8343\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.8149 - val_mse: 0.8149\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.7735 - val_mse: 0.7735\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.7171 - val_mse: 0.7171\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.7349 - val_mse: 0.7349\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.7740 - val_mse: 0.7740\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.7451 - val_mse: 0.7451\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.7403 - val_mse: 0.7403\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.8384 - val_mse: 0.8384\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 141334\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210819_144037-90etin4v/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210819_144037-90etin4v/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.83837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.83837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629384064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.71133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss █▆▆▅▅▅▄▄▄▄▄▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse █▆▆▅▅▅▄▄▄▄▄▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ▇▇█▇█▇██▇▇▇▅▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ▇▇█▇█▇██▇▇▇▅▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msunny-sweep-613\u001b[0m: \u001b[34mhttps://wandb.ai/kade/gamestop_prediction/runs/90etin4v\u001b[0m\n",
            "2021-08-19 14:41:15,411 - wandb.wandb_agent - INFO - Cleaning up finished run: 90etin4v\n",
            "2021-08-19 14:41:15,540 - wandb.wandb_agent - INFO - Agent received command: exit\n",
            "2021-08-19 14:41:15,540 - wandb.wandb_agent - INFO - Received exit command. Killing runs and quitting.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AmdEHkKNm5n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}