{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gamestop_sweep.ipynb",
      "provenance": [],
      "mount_file_id": "1a83B0yCBlvhZMEo4JR4YwX8O5z3qbWji",
      "authorship_tag": "ABX9TyPCqHEuAUDAYl47cl0Zmtew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haebuk/kuggle/blob/main/gamestop_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB0meTHJItzV",
        "outputId": "62fd1e45-5d5b-48db-a811-bfe2f25d0bc2"
      },
      "source": [
        "%pip install wandb -qqq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.6 MB 8.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133 kB 51.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97 kB 8.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 58.3 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cev3goNIwrD",
        "outputId": "d4b7d8a2-dd28-4af9-c1f7-f505bf68e415"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6CSy3R4Hp44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70afb995-0eea-4a7e-87c2-5775e6273532"
      },
      "source": [
        "import wandb \n",
        "from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "PATH = '/content/drive/MyDrive/input/'\n",
        "df = pd.read_csv(PATH + 'GME_scaled.csv')\n",
        "\n",
        "default_configs = {\n",
        "    'learning_rate': 0.001,\n",
        "    'dropout_rate1': 0.2,\n",
        "    'dropout_rate2': 0.2,\n",
        "    'hidden1': 50,\n",
        "    'hidden2': 50,\n",
        "    'time_step': 60,\n",
        "}\n",
        "wandb.init(project='gamestop_prediction', entity='kuggle', config=default_configs)\n",
        "config = wandb.config\n",
        "\n",
        "X = df.drop('date', axis=1).values\n",
        "y = df['open_price'].values\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "time_step = config.time_step\n",
        "X_train, y_train, X_test, y_test = [], [], [], []\n",
        "for i in range(time_step,len(X_tr)): # train\n",
        "    X_train.append(X_tr[i-time_step:i,:])\n",
        "    y_train.append(y_tr[i])\n",
        "for i in range(time_step, len(X_te)): # test\n",
        "    X_test.append(X_te[i-time_step:i,:])\n",
        "    y_test.append(y_te[i])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "# LSTM input shape Ï°∞Í±¥(batch_size, timestep, feature_num)Ïóê ÎßûÍ≤å reshape\n",
        "X_train = np.reshape(X_train, (-1,X_train.shape[1],2))\n",
        "X_test = np.reshape(X_test, (-1,X_test.shape[1],2))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "LSTM(config.hidden1, return_sequences=True, input_shape=(config.time_step, 2)),\n",
        "Dropout(config.dropout_rate1),\n",
        "LSTM(config.hidden2, return_sequences=True),\n",
        "Dropout(config.dropout_rate2),\n",
        "Dense(1),\n",
        "])\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "            loss='mse',\n",
        "            metrics=['mse'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=200, callbacks=[WandbCallback(), early_stopping])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">jumping-firebrand-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/kuggle/gamestop_prediction\" target=\"_blank\">https://wandb.ai/kuggle/gamestop_prediction</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/kuggle/gamestop_prediction/runs/mmlhq6sr\" target=\"_blank\">https://wandb.ai/kuggle/gamestop_prediction/runs/mmlhq6sr</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210820_124504-mmlhq6sr</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "81/81 [==============================] - 9s 22ms/step - loss: 1.1477 - mse: 1.1477 - val_loss: 1.9787 - val_mse: 1.9787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.1779 - mse: 0.1779 - val_loss: 1.1785 - val_mse: 1.1785\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.1022 - mse: 0.1022 - val_loss: 1.3924 - val_mse: 1.3924\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0797 - mse: 0.0797 - val_loss: 1.0705 - val_mse: 1.0705\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0829 - mse: 0.0829 - val_loss: 1.1751 - val_mse: 1.1751\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0728 - mse: 0.0728 - val_loss: 1.0129 - val_mse: 1.0129\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 1.1800 - val_mse: 1.1800\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0638 - mse: 0.0638 - val_loss: 0.8345 - val_mse: 0.8345\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0629 - mse: 0.0629 - val_loss: 0.8629 - val_mse: 0.8629\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0589 - mse: 0.0589 - val_loss: 0.7820 - val_mse: 0.7820\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0690 - mse: 0.0690 - val_loss: 0.8060 - val_mse: 0.8060\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0682 - mse: 0.0682 - val_loss: 0.8778 - val_mse: 0.8778\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0657 - mse: 0.0657 - val_loss: 0.7644 - val_mse: 0.7644\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0692 - mse: 0.0692 - val_loss: 0.8193 - val_mse: 0.8193\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0599 - mse: 0.0599 - val_loss: 0.8815 - val_mse: 0.8815\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0538 - mse: 0.0538 - val_loss: 0.8280 - val_mse: 0.8280\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0625 - mse: 0.0625 - val_loss: 0.8336 - val_mse: 0.8336\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0580 - mse: 0.0580 - val_loss: 0.7927 - val_mse: 0.7927\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0557 - mse: 0.0557 - val_loss: 0.8796 - val_mse: 0.8796\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0593 - mse: 0.0593 - val_loss: 0.8668 - val_mse: 0.8668\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0592 - mse: 0.0592 - val_loss: 0.8304 - val_mse: 0.8304\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0571 - mse: 0.0571 - val_loss: 0.6827 - val_mse: 0.6827\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0545 - mse: 0.0545 - val_loss: 0.8222 - val_mse: 0.8222\n",
            "Epoch 24/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.8441 - val_mse: 0.8441\n",
            "Epoch 25/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0542 - mse: 0.0542 - val_loss: 0.7120 - val_mse: 0.7120\n",
            "Epoch 26/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0611 - mse: 0.0611 - val_loss: 0.8711 - val_mse: 0.8711\n",
            "Epoch 27/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0538 - mse: 0.0538 - val_loss: 0.7842 - val_mse: 0.7842\n",
            "Epoch 28/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0564 - mse: 0.0564 - val_loss: 0.8151 - val_mse: 0.8151\n",
            "Epoch 29/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.7331 - val_mse: 0.7331\n",
            "Epoch 30/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0533 - mse: 0.0533 - val_loss: 0.8066 - val_mse: 0.8066\n",
            "Epoch 31/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.8056 - val_mse: 0.8056\n",
            "Epoch 32/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.7881 - val_mse: 0.7881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4d10212490>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHDzW4x4IfXJ"
      },
      "source": [
        "with open('train.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import wandb \n",
        "from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "PATH = '/content/drive/MyDrive/input/'\n",
        "df = pd.read_csv(PATH + 'GME_scaled.csv')\n",
        "\n",
        "default_configs = {\n",
        "    'learning_rate': 0.001,\n",
        "    'dropout_rate1': 0.2,\n",
        "    'dropout_rate2': 0.2,\n",
        "    'dropout_rate3': 0.2,\n",
        "    'hidden1': 50,\n",
        "    'hidden2': 50,\n",
        "    'hidden3': 50,\n",
        "    'time_step': 60,\n",
        "}\n",
        "wandb.init(project='gamestop_prediction', config=default_configs, magic=True)\n",
        "config = wandb.config\n",
        "\n",
        "X = df.drop('date', axis=1).values\n",
        "y = df['open_price'].values\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "time_step = config.time_step\n",
        "X_train, y_train, X_test, y_test = [], [], [], []\n",
        "for i in range(time_step,len(X_tr)): # train\n",
        "    X_train.append(X_tr[i-time_step:i,:])\n",
        "    y_train.append(y_tr[i])\n",
        "for i in range(time_step, len(X_te)): # test\n",
        "    X_test.append(X_te[i-time_step:i,:])\n",
        "    y_test.append(y_te[i])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "# LSTM input shape Ï°∞Í±¥(batch_size, timestep, feature_num)Ïóê ÎßûÍ≤å reshape\n",
        "X_train = np.reshape(X_train, (-1,X_train.shape[1],2))\n",
        "X_test = np.reshape(X_test, (-1,X_test.shape[1],2))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "LSTM(config.hidden1, return_sequences=True, input_shape=(config.time_step, 2)),\n",
        "Dropout(config.dropout_rate1),\n",
        "LSTM(config.hidden2, return_sequences=True),\n",
        "Dropout(config.dropout_rate2),\n",
        "LSTM(config.hidden3),\n",
        "Dropout(config.dropout_rate3),\n",
        "Dense(1),\n",
        "])\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "            loss='mse',\n",
        "            metrics=['mse'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=200, callbacks=[WandbCallback(), early_stopping])\n",
        "    \"\"\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmZV6rSNJK8P",
        "outputId": "a60b58e2-da37-4c9d-947e-35391e6f3d09"
      },
      "source": [
        "!head train.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "import wandb \n",
            "from wandb.keras import WandbCallback\n",
            "import tensorflow as tf\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from keras.models import Sequential\n",
            "from keras.layers import LSTM, Dense, Dropout\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oG_tPIuJLnN",
        "outputId": "ae9974d1-63ef-4c97-9d3c-e98c4dc7eb5d"
      },
      "source": [
        "!wandb agent kuggle/gamestop_prediction/1ng3wx9s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
            "2021-08-20 12:49:24,429 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2021-08-20 12:49:24,601 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:49:24,601 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.44896520548391455\n",
            "\tdropout_rate2: 0.5276440928019034\n",
            "\tdropout_rate3: 0.5344824982636621\n",
            "\thidden1: 72\n",
            "\thidden2: 12\n",
            "\thidden3: 13\n",
            "\tlearning_rate: 0.04783929267673129\n",
            "\ttime_step: 77\n",
            "2021-08-20 12:49:24,603 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.44896520548391455 --dropout_rate2=0.5276440928019034 --dropout_rate3=0.5344824982636621 --hidden1=72 --hidden2=12 --hidden3=13 --learning_rate=0.04783929267673129 --time_step=77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/c9mwp56z\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_124927-c9mwp56z\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:49:29.226065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.241872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.242567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29,610 - wandb.wandb_agent - INFO - Running runs: ['c9mwp56z']\n",
            "2021-08-20 12:49:29.243918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.244618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.245264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.863793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.864418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.865025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:49:29.865543: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:49:29.865602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:49:29.906732: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:49:29.906764: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:49:29.906865: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:49:30.090776: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:49:30.090956: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:49:30.258411: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:49:34.167011: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/80 [..............................] - ETA: 5:44 - loss: 10.7979 - mse: 10.79792021-08-20 12:49:34.776794: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:49:34.776839: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/80 [..............................] - ETA: 30s - loss: 8.4602 - mse: 8.4602   2021-08-20 12:49:35.029213: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:49:35.030434: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:49:35.183670: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1336 callback api events and 1333 activity events. \n",
            "2021-08-20 12:49:35.200296: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:49:35.229363: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35\n",
            "\n",
            "2021-08-20 12:49:35.245557: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:49:35.277033: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35\n",
            "\n",
            "2021-08-20 12:49:35.280266: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:49:35.280908: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_124927-c9mwp56z/files/train/plugins/profile/2021_08_20_12_49_35/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/80 [>.............................] - ETA: 25s - loss: 6.4502 - mse: 6.4502WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0105s vs `on_train_batch_begin` time: 0.0380s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0105s vs `on_train_batch_end` time: 0.0701s). Check your callbacks.\n",
            "79/80 [============================>.] - ETA: 0s - loss: 1.2430 - mse: 1.2430\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "80/80 [==============================] - 7s 30ms/step - loss: 1.2401 - mse: 1.2401 - val_loss: 3.1451 - val_mse: 3.1451\n",
            "Epoch 2/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.5039 - mse: 0.5039 - val_loss: 2.7763 - val_mse: 2.7763\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.4062 - mse: 0.4062 - val_loss: 2.6387 - val_mse: 2.6387\n",
            "Epoch 4/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3575 - mse: 0.3575 - val_loss: 2.5604 - val_mse: 2.5604\n",
            "Epoch 5/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3487 - mse: 0.3487 - val_loss: 2.9879 - val_mse: 2.9879\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3541 - mse: 0.3541 - val_loss: 2.5915 - val_mse: 2.5915\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3467 - mse: 0.3467 - val_loss: 2.6742 - val_mse: 2.6742\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3453 - mse: 0.3453 - val_loss: 2.6229 - val_mse: 2.6229\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3477 - mse: 0.3477 - val_loss: 2.5597 - val_mse: 2.5597\n",
            "Epoch 10/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3474 - mse: 0.3474 - val_loss: 2.7729 - val_mse: 2.7729\n",
            "Epoch 11/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3477 - mse: 0.3477 - val_loss: 2.6219 - val_mse: 2.6219\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3447 - mse: 0.3447 - val_loss: 2.4426 - val_mse: 2.4426\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3471 - mse: 0.3471 - val_loss: 2.5629 - val_mse: 2.5629\n",
            "Epoch 14/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3479 - mse: 0.3479 - val_loss: 2.5932 - val_mse: 2.5932\n",
            "Epoch 15/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3459 - mse: 0.3459 - val_loss: 2.7122 - val_mse: 2.7122\n",
            "Epoch 16/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 2.7229 - val_mse: 2.7229\n",
            "Epoch 17/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3468 - mse: 0.3468 - val_loss: 2.6251 - val_mse: 2.6251\n",
            "Epoch 18/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3472 - mse: 0.3472 - val_loss: 2.7038 - val_mse: 2.7038\n",
            "Epoch 19/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3460 - mse: 0.3460 - val_loss: 2.4167 - val_mse: 2.4167\n",
            "Epoch 20/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3488 - mse: 0.3488 - val_loss: 2.6912 - val_mse: 2.6912\n",
            "Epoch 21/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3446 - mse: 0.3446 - val_loss: 2.7971 - val_mse: 2.7971\n",
            "Epoch 22/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3478 - mse: 0.3478 - val_loss: 2.5669 - val_mse: 2.5669\n",
            "Epoch 23/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3453 - mse: 0.3453 - val_loss: 2.4210 - val_mse: 2.4210\n",
            "Epoch 24/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3474 - mse: 0.3474 - val_loss: 2.6537 - val_mse: 2.6537\n",
            "Epoch 25/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3454 - mse: 0.3454 - val_loss: 2.7058 - val_mse: 2.7058\n",
            "Epoch 26/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 2.5181 - val_mse: 2.5181\n",
            "Epoch 27/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3454 - mse: 0.3454 - val_loss: 2.6486 - val_mse: 2.6486\n",
            "Epoch 28/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3469 - mse: 0.3469 - val_loss: 2.7366 - val_mse: 2.7366\n",
            "Epoch 29/200\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3488 - mse: 0.3488 - val_loss: 2.7160 - val_mse: 2.7160\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_124927-c9mwp56z/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_124927-c9mwp56z/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.34877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.34877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.71596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.71596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629463801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.41667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdevoted-sweep-1\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/c9mwp56z\u001b[0m\n",
            "2021-08-20 12:50:20,373 - wandb.wandb_agent - INFO - Cleaning up finished run: c9mwp56z\n",
            "2021-08-20 12:50:20,562 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:50:20,562 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.5189695986908369\n",
            "\tdropout_rate2: 0.361281036579307\n",
            "\tdropout_rate3: 0.5683445729907265\n",
            "\thidden1: 55\n",
            "\thidden2: 11\n",
            "\thidden3: 55\n",
            "\tlearning_rate: 0.167523668413378\n",
            "\ttime_step: 43\n",
            "2021-08-20 12:50:20,565 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.5189695986908369 --dropout_rate2=0.361281036579307 --dropout_rate3=0.5683445729907265 --hidden1=55 --hidden2=11 --hidden3=55 --learning_rate=0.167523668413378 --time_step=43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msilvery-sweep-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/9is3nuiq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125023-9is3nuiq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:50:25.155815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.169227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.170142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25,573 - wandb.wandb_agent - INFO - Running runs: ['9is3nuiq']\n",
            "2021-08-20 12:50:25.171599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.172449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.173261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.786164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.786831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.787384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:50:25.787932: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:50:25.787990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:50:25.819549: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:50:25.819684: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:50:25.819768: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:50:25.962697: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:50:25.962877: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:50:26.128884: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:50:30.041350: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:49 - loss: 9.1688 - mse: 9.16882021-08-20 12:50:30.649853: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:50:30.649897: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 31s - loss: 21.2697 - mse: 21.26972021-08-20 12:50:30.906530: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:50:30.907228: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:50:31.059747: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 892 callback api events and 889 activity events. \n",
            "2021-08-20 12:50:31.070837: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:50:31.087335: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31\n",
            "\n",
            "2021-08-20 12:50:31.099134: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:50:31.116560: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31\n",
            "\n",
            "2021-08-20 12:50:31.119551: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:50:31.120103: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125023-9is3nuiq/files/train/plugins/profile/2021_08_20_12_50_31/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 24s - loss: 14.5478 - mse: 14.5478WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_begin` time: 0.0404s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n",
            "81/81 [==============================] - 6s 27ms/step - loss: 1.5324 - mse: 1.5324 - val_loss: 2.4572 - val_mse: 2.4572\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4499 - mse: 0.4499 - val_loss: 2.1729 - val_mse: 2.1729\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.3916 - mse: 0.3916 - val_loss: 2.7103 - val_mse: 2.7103\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4030 - mse: 0.4030 - val_loss: 2.7502 - val_mse: 2.7502\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4352 - mse: 0.4352 - val_loss: 2.8309 - val_mse: 2.8309\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4375 - mse: 0.4375 - val_loss: 1.9855 - val_mse: 1.9855\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4537 - mse: 0.4537 - val_loss: 2.8782 - val_mse: 2.8782\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4655 - mse: 0.4655 - val_loss: 3.8973 - val_mse: 3.8973\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4609 - mse: 0.4609 - val_loss: 2.2513 - val_mse: 2.2513\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4287 - mse: 0.4287 - val_loss: 3.1935 - val_mse: 3.1935\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.5081 - mse: 0.5081 - val_loss: 2.0222 - val_mse: 2.0222\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4989 - mse: 0.4989 - val_loss: 1.9701 - val_mse: 1.9701\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.5468 - mse: 0.5468 - val_loss: 2.1379 - val_mse: 2.1379\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.5097 - mse: 0.5097 - val_loss: 2.1176 - val_mse: 2.1176\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 0.4358 - mse: 0.4358 - val_loss: 3.0608 - val_mse: 3.0608\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4523 - mse: 0.4523 - val_loss: 2.5415 - val_mse: 2.5415\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4866 - mse: 0.4866 - val_loss: 3.7415 - val_mse: 3.7415\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4898 - mse: 0.4898 - val_loss: 2.4173 - val_mse: 2.4173\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.5346 - mse: 0.5346 - val_loss: 2.0074 - val_mse: 2.0074\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4813 - mse: 0.4813 - val_loss: 2.5775 - val_mse: 2.5775\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4757 - mse: 0.4757 - val_loss: 2.7147 - val_mse: 2.7147\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 9ms/step - loss: 0.4449 - mse: 0.4449 - val_loss: 2.1913 - val_mse: 2.1913\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 820\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125023-9is3nuiq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125023-9is3nuiq/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.44495\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.44495\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.19132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.19132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629463847\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 1.97014\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñà‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñà‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msilvery-sweep-2\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/9is3nuiq\u001b[0m\n",
            "2021-08-20 12:50:56,098 - wandb.wandb_agent - INFO - Cleaning up finished run: 9is3nuiq\n",
            "2021-08-20 12:50:56,315 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:50:56,315 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.18843569173211555\n",
            "\tdropout_rate2: 0.1385143059276163\n",
            "\tdropout_rate3: 0.23371682206684985\n",
            "\thidden1: 105\n",
            "\thidden2: 44\n",
            "\thidden3: 85\n",
            "\tlearning_rate: 0.17452292028413366\n",
            "\ttime_step: 105\n",
            "2021-08-20 12:50:56,317 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.18843569173211555 --dropout_rate2=0.1385143059276163 --dropout_rate3=0.23371682206684985 --hidden1=105 --hidden2=44 --hidden3=85 --learning_rate=0.17452292028413366 --time_step=105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-sweep-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/1f9zlbn2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125058-1f9zlbn2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:51:00.908776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:01,324 - wandb.wandb_agent - INFO - Running runs: ['1f9zlbn2']\n",
            "2021-08-20 12:51:00.922065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:00.922707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:00.923871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:00.924473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:00.925062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:01.537296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:01.537957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:01.538573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:51:01.539119: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:51:01.539178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:51:01.566799: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:51:01.566832: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:51:01.566877: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:51:01.701091: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:51:01.701257: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:51:01.869974: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:51:05.828864: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/79 [..............................] - ETA: 5:45 - loss: 9.2492 - mse: 9.24922021-08-20 12:51:06.452809: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:51:06.452851: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/79 [..............................] - ETA: 31s - loss: 52.6231 - mse: 52.62312021-08-20 12:51:06.712153: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:51:06.713914: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:51:06.862117: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1727 callback api events and 1724 activity events. \n",
            "2021-08-20 12:51:06.881300: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:51:06.910065: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06\n",
            "\n",
            "2021-08-20 12:51:06.929546: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:51:06.952345: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06\n",
            "\n",
            "2021-08-20 12:51:06.955592: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:51:06.956284: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125058-1f9zlbn2/files/train/plugins/profile/2021_08_20_12_51_06/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_begin` time: 0.0389s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0716s). Check your callbacks.\n",
            "79/79 [==============================] - 7s 36ms/step - loss: 5.8006 - mse: 5.8006 - val_loss: 2.7148 - val_mse: 2.7148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4358 - mse: 0.4358 - val_loss: 2.9945 - val_mse: 2.9945\n",
            "Epoch 3/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4785 - mse: 0.4785 - val_loss: 2.3451 - val_mse: 2.3451\n",
            "Epoch 4/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4321 - mse: 0.4321 - val_loss: 3.8091 - val_mse: 3.8091\n",
            "Epoch 5/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4587 - mse: 0.4587 - val_loss: 2.0622 - val_mse: 2.0622\n",
            "Epoch 6/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4343 - mse: 0.4343 - val_loss: 1.9153 - val_mse: 1.9153\n",
            "Epoch 7/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5300 - mse: 0.5300 - val_loss: 4.7468 - val_mse: 4.7468\n",
            "Epoch 8/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4553 - mse: 0.4553 - val_loss: 2.0450 - val_mse: 2.0450\n",
            "Epoch 9/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5287 - mse: 0.5287 - val_loss: 2.2368 - val_mse: 2.2368\n",
            "Epoch 10/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4794 - mse: 0.4794 - val_loss: 3.1884 - val_mse: 3.1884\n",
            "Epoch 11/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6448 - mse: 0.6448 - val_loss: 4.4550 - val_mse: 4.4550\n",
            "Epoch 12/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5979 - mse: 0.5979 - val_loss: 3.1212 - val_mse: 3.1212\n",
            "Epoch 13/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4409 - mse: 0.4409 - val_loss: 2.0613 - val_mse: 2.0613\n",
            "Epoch 14/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4511 - mse: 0.4511 - val_loss: 1.8377 - val_mse: 1.8377\n",
            "Epoch 15/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4757 - mse: 0.4757 - val_loss: 3.0089 - val_mse: 3.0089\n",
            "Epoch 16/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5946 - mse: 0.5946 - val_loss: 3.1215 - val_mse: 3.1215\n",
            "Epoch 17/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5488 - mse: 0.5488 - val_loss: 4.0745 - val_mse: 4.0745\n",
            "Epoch 18/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6930 - mse: 0.6930 - val_loss: 4.1845 - val_mse: 4.1845\n",
            "Epoch 19/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5963 - mse: 0.5963 - val_loss: 3.7835 - val_mse: 3.7835\n",
            "Epoch 20/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5673 - mse: 0.5673 - val_loss: 2.1532 - val_mse: 2.1532\n",
            "Epoch 21/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4849 - mse: 0.4849 - val_loss: 1.8291 - val_mse: 1.8291\n",
            "Epoch 22/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4845 - mse: 0.4845 - val_loss: 1.8257 - val_mse: 1.8257\n",
            "Epoch 23/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5936 - mse: 0.5936 - val_loss: 3.2177 - val_mse: 3.2177\n",
            "Epoch 24/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6168 - mse: 0.6168 - val_loss: 2.4327 - val_mse: 2.4327\n",
            "Epoch 25/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5306 - mse: 0.5306 - val_loss: 2.5654 - val_mse: 2.5654\n",
            "Epoch 26/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4972 - mse: 0.4972 - val_loss: 4.8871 - val_mse: 4.8871\n",
            "Epoch 27/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5502 - mse: 0.5502 - val_loss: 1.5984 - val_mse: 1.5984\n",
            "Epoch 28/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5754 - mse: 0.5754 - val_loss: 4.8701 - val_mse: 4.8701\n",
            "Epoch 29/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5006 - mse: 0.5006 - val_loss: 3.7974 - val_mse: 3.7974\n",
            "Epoch 30/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 1.1672 - mse: 1.1672 - val_loss: 2.6605 - val_mse: 2.6605\n",
            "Epoch 31/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5470 - mse: 0.5470 - val_loss: 2.5914 - val_mse: 2.5914\n",
            "Epoch 32/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5376 - mse: 0.5376 - val_loss: 2.1813 - val_mse: 2.1813\n",
            "Epoch 33/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6101 - mse: 0.6101 - val_loss: 2.9529 - val_mse: 2.9529\n",
            "Epoch 34/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 3.9580 - val_mse: 3.9580\n",
            "Epoch 35/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5359 - mse: 0.5359 - val_loss: 1.3404 - val_mse: 1.3404\n",
            "Epoch 36/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5918 - mse: 0.5918 - val_loss: 1.6599 - val_mse: 1.6599\n",
            "Epoch 37/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5904 - mse: 0.5904 - val_loss: 2.9005 - val_mse: 2.9005\n",
            "Epoch 38/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4658 - mse: 0.4658 - val_loss: 2.1086 - val_mse: 2.1086\n",
            "Epoch 39/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5905 - mse: 0.5905 - val_loss: 1.6331 - val_mse: 1.6331\n",
            "Epoch 40/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5145 - mse: 0.5145 - val_loss: 2.9658 - val_mse: 2.9658\n",
            "Epoch 41/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.5416 - mse: 0.5416 - val_loss: 3.3913 - val_mse: 3.3913\n",
            "Epoch 42/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6410 - mse: 0.6410 - val_loss: 2.5194 - val_mse: 2.5194\n",
            "Epoch 43/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6353 - mse: 0.6353 - val_loss: 1.3676 - val_mse: 1.3676\n",
            "Epoch 44/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.6255 - mse: 0.6255 - val_loss: 2.3171 - val_mse: 2.3171\n",
            "Epoch 45/200\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.4868 - mse: 0.4868 - val_loss: 2.6180 - val_mse: 2.6180\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1026\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125058-1f9zlbn2/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125058-1f9zlbn2/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.48675\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.48675\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.61799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.61799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629463929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 1.34038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfresh-sweep-3\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/1f9zlbn2\u001b[0m\n",
            "2021-08-20 12:52:33,101 - wandb.wandb_agent - INFO - Cleaning up finished run: 1f9zlbn2\n",
            "2021-08-20 12:52:33,311 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:52:33,311 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3940321134457593\n",
            "\tdropout_rate2: 0.31813462568764006\n",
            "\tdropout_rate3: 0.14267496273060853\n",
            "\thidden1: 49\n",
            "\thidden2: 76\n",
            "\thidden3: 63\n",
            "\tlearning_rate: 0.020652779738792582\n",
            "\ttime_step: 81\n",
            "2021-08-20 12:52:33,313 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3940321134457593 --dropout_rate2=0.31813462568764006 --dropout_rate3=0.14267496273060853 --hidden1=49 --hidden2=76 --hidden3=63 --learning_rate=0.020652779738792582 --time_step=81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melated-sweep-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/736460v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125235-736460v3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:52:37.940866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:38,327 - wandb.wandb_agent - INFO - Running runs: ['736460v3']\n",
            "2021-08-20 12:52:37.948812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:37.949409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:37.950814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:37.951392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:37.951949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:38.554932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:38.555815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:38.556607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:52:38.557361: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:52:38.557412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:52:38.590911: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:52:38.590946: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:52:38.590984: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:52:38.724238: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:52:38.724404: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:52:38.893123: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:52:42.880285: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/80 [..............................] - ETA: 5:51 - loss: 9.3447 - mse: 9.34472021-08-20 12:52:43.514103: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:52:43.514167: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/80 [..............................] - ETA: 31s - loss: 4.8825 - mse: 4.8825 2021-08-20 12:52:43.769209: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:52:43.770270: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:52:43.917231: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1403 callback api events and 1400 activity events. \n",
            "2021-08-20 12:52:43.933026: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:52:43.959064: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43\n",
            "\n",
            "2021-08-20 12:52:43.975049: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:52:43.997333: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43\n",
            "\n",
            "2021-08-20 12:52:44.001262: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:52:44.002729: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125235-736460v3/files/train/plugins/profile/2021_08_20_12_52_43/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/80 [>.............................] - ETA: 25s - loss: 9.2697 - mse: 9.2697WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0118s vs `on_train_batch_begin` time: 0.0391s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0118s vs `on_train_batch_end` time: 0.0690s). Check your callbacks.\n",
            "80/80 [==============================] - 7s 32ms/step - loss: 0.8596 - mse: 0.8596 - val_loss: 3.0055 - val_mse: 3.0055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4282 - mse: 0.4282 - val_loss: 2.5848 - val_mse: 2.5848\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4179 - mse: 0.4179 - val_loss: 2.9760 - val_mse: 2.9760\n",
            "Epoch 4/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4357 - mse: 0.4357 - val_loss: 2.8243 - val_mse: 2.8243\n",
            "Epoch 5/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4166 - mse: 0.4166 - val_loss: 2.4782 - val_mse: 2.4782\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4044 - mse: 0.4044 - val_loss: 2.9454 - val_mse: 2.9454\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4170 - mse: 0.4170 - val_loss: 2.7368 - val_mse: 2.7368\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 2.3848 - val_mse: 2.3848\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3930 - mse: 0.3930 - val_loss: 2.5217 - val_mse: 2.5217\n",
            "Epoch 10/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3885 - mse: 0.3885 - val_loss: 2.9328 - val_mse: 2.9328\n",
            "Epoch 11/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3970 - mse: 0.3970 - val_loss: 3.0081 - val_mse: 3.0081\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3905 - mse: 0.3905 - val_loss: 2.6366 - val_mse: 2.6366\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3816 - mse: 0.3816 - val_loss: 3.3104 - val_mse: 3.3104\n",
            "Epoch 14/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3777 - mse: 0.3777 - val_loss: 2.6043 - val_mse: 2.6043\n",
            "Epoch 15/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3801 - mse: 0.3801 - val_loss: 3.0307 - val_mse: 3.0307\n",
            "Epoch 16/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3766 - mse: 0.3766 - val_loss: 2.4052 - val_mse: 2.4052\n",
            "Epoch 17/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3724 - mse: 0.3724 - val_loss: 3.0753 - val_mse: 3.0753\n",
            "Epoch 18/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3667 - mse: 0.3667 - val_loss: 2.3794 - val_mse: 2.3794\n",
            "Epoch 19/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3698 - mse: 0.3698 - val_loss: 2.4269 - val_mse: 2.4269\n",
            "Epoch 20/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3668 - mse: 0.3668 - val_loss: 2.8672 - val_mse: 2.8672\n",
            "Epoch 21/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3640 - mse: 0.3640 - val_loss: 2.5550 - val_mse: 2.5550\n",
            "Epoch 22/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3691 - mse: 0.3691 - val_loss: 3.1472 - val_mse: 3.1472\n",
            "Epoch 23/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3601 - mse: 0.3601 - val_loss: 2.7158 - val_mse: 2.7158\n",
            "Epoch 24/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3677 - mse: 0.3677 - val_loss: 2.6424 - val_mse: 2.6424\n",
            "Epoch 25/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3516 - mse: 0.3516 - val_loss: 2.3761 - val_mse: 2.3761\n",
            "Epoch 26/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3605 - mse: 0.3605 - val_loss: 3.0023 - val_mse: 3.0023\n",
            "Epoch 27/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3600 - mse: 0.3600 - val_loss: 2.6667 - val_mse: 2.6667\n",
            "Epoch 28/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3563 - mse: 0.3563 - val_loss: 3.0343 - val_mse: 3.0343\n",
            "Epoch 29/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3541 - mse: 0.3541 - val_loss: 2.6626 - val_mse: 2.6626\n",
            "Epoch 30/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3493 - mse: 0.3493 - val_loss: 2.7376 - val_mse: 2.7376\n",
            "Epoch 31/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3506 - mse: 0.3506 - val_loss: 2.6789 - val_mse: 2.6789\n",
            "Epoch 32/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3481 - mse: 0.3481 - val_loss: 2.6803 - val_mse: 2.6803\n",
            "Epoch 33/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3476 - mse: 0.3476 - val_loss: 2.7152 - val_mse: 2.7152\n",
            "Epoch 34/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3471 - mse: 0.3471 - val_loss: 2.4696 - val_mse: 2.4696\n",
            "Epoch 35/200\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3580 - mse: 0.3580 - val_loss: 2.4978 - val_mse: 2.4978\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1343\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125235-736460v3/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125235-736460v3/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.358\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.358\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.4978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.4978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.37612\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33melated-sweep-4\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/736460v3\u001b[0m\n",
            "2021-08-20 12:53:29,137 - wandb.wandb_agent - INFO - Cleaning up finished run: 736460v3\n",
            "2021-08-20 12:53:29,348 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:53:29,348 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.1320526992776815\n",
            "\tdropout_rate2: 0.10320927342535861\n",
            "\tdropout_rate3: 0.37971502832086085\n",
            "\thidden1: 44\n",
            "\thidden2: 36\n",
            "\thidden3: 39\n",
            "\tlearning_rate: 0.03450088599200962\n",
            "\ttime_step: 85\n",
            "2021-08-20 12:53:29,350 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.1320526992776815 --dropout_rate2=0.10320927342535861 --dropout_rate3=0.37971502832086085 --hidden1=44 --hidden2=36 --hidden3=39 --learning_rate=0.03450088599200962 --time_step=85\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mflowing-sweep-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/rm228yd7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125331-rm228yd7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:53:34.016310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34,359 - wandb.wandb_agent - INFO - Running runs: ['rm228yd7']\n",
            "2021-08-20 12:53:34.028906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.029632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.030807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.031410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.032023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.658599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.659271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.659849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:53:34.660367: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:53:34.660420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:53:34.690146: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:53:34.690181: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:53:34.690223: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:53:34.820708: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:53:34.820881: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:53:34.997812: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:53:38.971926: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/80 [..............................] - ETA: 5:50 - loss: 10.8787 - mse: 10.87872021-08-20 12:53:39.591466: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:53:39.591517: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/80 [..............................] - ETA: 30s - loss: 5.9375 - mse: 5.9375   2021-08-20 12:53:39.848193: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:53:39.849402: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:53:39.998081: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1448 callback api events and 1445 activity events. \n",
            "2021-08-20 12:53:40.013749: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:53:40.040447: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40\n",
            "\n",
            "2021-08-20 12:53:40.062626: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:53:40.087410: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40\n",
            "\n",
            "2021-08-20 12:53:40.090788: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:53:40.091442: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125331-rm228yd7/files/train/plugins/profile/2021_08_20_12_53_40/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/80 [>.............................] - ETA: 25s - loss: 4.9071 - mse: 4.9071WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0121s vs `on_train_batch_begin` time: 0.0394s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0121s vs `on_train_batch_end` time: 0.0675s). Check your callbacks.\n",
            "80/80 [==============================] - 7s 32ms/step - loss: 0.9098 - mse: 0.9098 - val_loss: 3.0992 - val_mse: 3.0992\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.5781 - mse: 0.5781 - val_loss: 2.8049 - val_mse: 2.8049\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.4709 - mse: 0.4709 - val_loss: 2.8161 - val_mse: 2.8161\n",
            "Epoch 4/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.4151 - mse: 0.4151 - val_loss: 2.8051 - val_mse: 2.8051\n",
            "Epoch 5/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3988 - mse: 0.3988 - val_loss: 2.6293 - val_mse: 2.6293\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3555 - mse: 0.3555 - val_loss: 2.7817 - val_mse: 2.7817\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3514 - mse: 0.3514 - val_loss: 2.8626 - val_mse: 2.8626\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 2.7276 - val_mse: 2.7276\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3470 - mse: 0.3470 - val_loss: 2.5199 - val_mse: 2.5199\n",
            "Epoch 10/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3471 - mse: 0.3471 - val_loss: 2.7747 - val_mse: 2.7747\n",
            "Epoch 11/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3470 - mse: 0.3470 - val_loss: 2.6983 - val_mse: 2.6983\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3456 - mse: 0.3456 - val_loss: 2.6367 - val_mse: 2.6367\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3460 - mse: 0.3460 - val_loss: 2.6053 - val_mse: 2.6053\n",
            "Epoch 14/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3450 - mse: 0.3450 - val_loss: 2.8011 - val_mse: 2.8011\n",
            "Epoch 15/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3468 - mse: 0.3468 - val_loss: 2.7558 - val_mse: 2.7558\n",
            "Epoch 16/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3466 - mse: 0.3466 - val_loss: 2.8571 - val_mse: 2.8571\n",
            "Epoch 17/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 2.6532 - val_mse: 2.6532\n",
            "Epoch 18/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3471 - mse: 0.3471 - val_loss: 2.7014 - val_mse: 2.7014\n",
            "Epoch 19/200\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 2.8627 - val_mse: 2.8627\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125331-rm228yd7/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125331-rm228yd7/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.34644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.34644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.86265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.86265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.5199\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mflowing-sweep-5\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/rm228yd7\u001b[0m\n",
            "2021-08-20 12:54:25,126 - wandb.wandb_agent - INFO - Cleaning up finished run: rm228yd7\n",
            "2021-08-20 12:54:25,338 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:54:25,338 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.2641887161778378\n",
            "\tdropout_rate2: 0.14039829091321343\n",
            "\tdropout_rate3: 0.5065521070787138\n",
            "\thidden1: 35\n",
            "\thidden2: 35\n",
            "\thidden3: 110\n",
            "\tlearning_rate: 0.026078497211126282\n",
            "\ttime_step: 7\n",
            "2021-08-20 12:54:25,339 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.2641887161778378 --dropout_rate2=0.14039829091321343 --dropout_rate3=0.5065521070787138 --hidden1=35 --hidden2=35 --hidden3=110 --learning_rate=0.026078497211126282 --time_step=7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mproud-sweep-6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/ho9vt765\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125427-ho9vt765\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:54:30.010246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30,345 - wandb.wandb_agent - INFO - Running runs: ['ho9vt765']\n",
            "2021-08-20 12:54:30.017787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.018394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.019537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.020180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.020739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.645619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.646266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.646865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:54:30.647389: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:54:30.647445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:54:30.674911: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:54:30.674945: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:54:30.674988: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:54:30.803687: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:54:30.803849: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:54:30.973688: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:54:34.993108: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:03 - loss: 10.3386 - mse: 10.33862021-08-20 12:54:35.618450: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:54:35.618501: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 7.0144 - mse: 7.0144   2021-08-20 12:54:35.870196: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:54:35.870680: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:54:36.017536: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 406 callback api events and 403 activity events. \n",
            "2021-08-20 12:54:36.024397: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:54:36.033882: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36\n",
            "\n",
            "2021-08-20 12:54:36.040670: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:54:36.054999: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36\n",
            "\n",
            "2021-08-20 12:54:36.057885: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:54:36.058388: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125427-ho9vt765/files/train/plugins/profile/2021_08_20_12_54_36/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 14.4837 - mse: 14.4837WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_begin` time: 0.0387s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0559s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 1.5536 - mse: 1.5536 - val_loss: 2.3366 - val_mse: 2.3366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4229 - mse: 0.4229 - val_loss: 1.4278 - val_mse: 1.4278\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.2068 - mse: 0.2068 - val_loss: 1.0215 - val_mse: 1.0215\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1404 - mse: 0.1404 - val_loss: 0.8648 - val_mse: 0.8648\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.1201 - mse: 0.1201 - val_loss: 0.9539 - val_mse: 0.9539\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.8484 - val_mse: 0.8484\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0849 - mse: 0.0849 - val_loss: 0.7744 - val_mse: 0.7744\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0671 - mse: 0.0671 - val_loss: 0.5715 - val_mse: 0.5715\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0588 - mse: 0.0588 - val_loss: 0.8807 - val_mse: 0.8807\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.6605 - val_mse: 0.6605\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.6708 - val_mse: 0.6708\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.7488 - val_mse: 0.7488\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.7060 - val_mse: 0.7060\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.7138 - val_mse: 0.7138\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.7673 - val_mse: 0.7673\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.7976 - val_mse: 0.7976\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.9298 - val_mse: 0.9298\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.8692 - val_mse: 0.8692\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125427-ho9vt765/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125427-ho9vt765/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.86918\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.86918\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464086\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.57155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mproud-sweep-6\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/ho9vt765\u001b[0m\n",
            "2021-08-20 12:54:55,729 - wandb.wandb_agent - INFO - Cleaning up finished run: ho9vt765\n",
            "2021-08-20 12:54:56,066 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:54:56,066 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.5734116418685484\n",
            "\tdropout_rate2: 0.24256710863012398\n",
            "\tdropout_rate3: 0.1313343097378029\n",
            "\thidden1: 34\n",
            "\thidden2: 86\n",
            "\thidden3: 85\n",
            "\tlearning_rate: 0.17139624785238985\n",
            "\ttime_step: 88\n",
            "2021-08-20 12:54:56,068 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.5734116418685484 --dropout_rate2=0.24256710863012398 --dropout_rate3=0.1313343097378029 --hidden1=34 --hidden2=86 --hidden3=85 --learning_rate=0.17139624785238985 --time_step=88\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbumbling-sweep-7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/gn4ya7hc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125458-gn4ya7hc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:55:00.713415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:01,074 - wandb.wandb_agent - INFO - Running runs: ['gn4ya7hc']\n",
            "2021-08-20 12:55:00.722482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:00.723099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:00.724209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:00.724836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:00.725385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:01.329370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:01.330047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:01.330624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:01.331154: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:55:01.331205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:55:01.366624: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:55:01.366674: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:55:01.366720: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:55:01.499557: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:55:01.499738: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:55:01.666266: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:55:05.617754: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/80 [..............................] - ETA: 5:48 - loss: 9.5357 - mse: 9.53572021-08-20 12:55:06.240981: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:55:06.241023: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/80 [..............................] - ETA: 31s - loss: 78.0929 - mse: 78.09292021-08-20 12:55:06.499990: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:55:06.501009: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:55:06.648689: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1498 callback api events and 1495 activity events. \n",
            "2021-08-20 12:55:06.663905: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:55:06.690899: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06\n",
            "\n",
            "2021-08-20 12:55:06.707740: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:55:06.732039: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06\n",
            "\n",
            "2021-08-20 12:55:06.735042: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:55:06.735681: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125458-gn4ya7hc/files/train/plugins/profile/2021_08_20_12_55_06/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/80 [>.............................] - ETA: 25s - loss: 59.3339 - mse: 59.3339WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0126s vs `on_train_batch_begin` time: 0.0394s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0126s vs `on_train_batch_end` time: 0.0701s). Check your callbacks.\n",
            "80/80 [==============================] - 7s 33ms/step - loss: 7.2079 - mse: 7.2079 - val_loss: 2.5535 - val_mse: 2.5535\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4213 - mse: 0.4213 - val_loss: 2.0003 - val_mse: 2.0003\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4056 - mse: 0.4056 - val_loss: 1.8976 - val_mse: 1.8976\n",
            "Epoch 4/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4158 - mse: 0.4158 - val_loss: 2.1884 - val_mse: 2.1884\n",
            "Epoch 5/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4109 - mse: 0.4109 - val_loss: 2.8848 - val_mse: 2.8848\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3868 - mse: 0.3868 - val_loss: 3.0008 - val_mse: 3.0008\n",
            "Epoch 7/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4125 - mse: 0.4125 - val_loss: 3.2377 - val_mse: 3.2377\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.5147 - mse: 0.5147 - val_loss: 1.6879 - val_mse: 1.6879\n",
            "Epoch 9/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4651 - mse: 0.4651 - val_loss: 2.8398 - val_mse: 2.8398\n",
            "Epoch 10/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4383 - mse: 0.4383 - val_loss: 2.2254 - val_mse: 2.2254\n",
            "Epoch 11/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4964 - mse: 0.4964 - val_loss: 2.5662 - val_mse: 2.5662\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4555 - mse: 0.4555 - val_loss: 2.3998 - val_mse: 2.3998\n",
            "Epoch 13/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4609 - mse: 0.4609 - val_loss: 3.3113 - val_mse: 3.3113\n",
            "Epoch 14/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.6358 - mse: 0.6358 - val_loss: 3.4092 - val_mse: 3.4092\n",
            "Epoch 15/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4574 - mse: 0.4574 - val_loss: 2.0902 - val_mse: 2.0902\n",
            "Epoch 16/200\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.6182 - mse: 0.6182 - val_loss: 2.6988 - val_mse: 2.6988\n",
            "Epoch 17/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.6257 - mse: 0.6257 - val_loss: 1.7739 - val_mse: 1.7739\n",
            "Epoch 18/200\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4505 - mse: 0.4505 - val_loss: 3.4372 - val_mse: 3.4372\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125458-gn4ya7hc/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125458-gn4ya7hc/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.45045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.45045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 3.43716\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 3.43716\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 1.68786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÅ‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÅ‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbumbling-sweep-7\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/gn4ya7hc\u001b[0m\n",
            "2021-08-20 12:55:36,619 - wandb.wandb_agent - INFO - Cleaning up finished run: gn4ya7hc\n",
            "2021-08-20 12:55:36,841 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:55:36,841 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.3046700940116883\n",
            "\tdropout_rate2: 0.07795413560717984\n",
            "\tdropout_rate3: 0.5251637376509847\n",
            "\thidden1: 25\n",
            "\thidden2: 47\n",
            "\thidden3: 69\n",
            "\tlearning_rate: 0.061889374734611625\n",
            "\ttime_step: 29\n",
            "2021-08-20 12:55:36,843 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.3046700940116883 --dropout_rate2=0.07795413560717984 --dropout_rate3=0.5251637376509847 --hidden1=25 --hidden2=47 --hidden3=69 --learning_rate=0.061889374734611625 --time_step=29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrural-sweep-8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/9wrha184\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125539-9wrha184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:55:41.450888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:41,854 - wandb.wandb_agent - INFO - Running runs: ['9wrha184']\n",
            "2021-08-20 12:55:41.457896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:41.458517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:41.459732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:41.460327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:41.460896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:42.066498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:42.067183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:42.067755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:55:42.068269: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:55:42.068320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:55:42.101676: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:55:42.101710: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:55:42.101749: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:55:42.238999: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:55:42.239172: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:55:42.408573: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:55:46.404946: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 9.7005 - mse: 9.70052021-08-20 12:55:47.026844: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:55:47.026886: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 14.4114 - mse: 14.41142021-08-20 12:55:47.282613: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:55:47.283266: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:55:47.432683: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 706 callback api events and 703 activity events. \n",
            "2021-08-20 12:55:47.443383: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:55:47.459040: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47\n",
            "\n",
            "2021-08-20 12:55:47.470574: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:55:47.489835: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47\n",
            "\n",
            "2021-08-20 12:55:47.493238: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:55:47.493852: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125539-9wrha184/files/train/plugins/profile/2021_08_20_12_55_47/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 13.7050 - mse: 13.7050WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0109s vs `on_train_batch_begin` time: 0.0400s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0109s vs `on_train_batch_end` time: 0.0608s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 1.5457 - mse: 1.5457 - val_loss: 2.1255 - val_mse: 2.1255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5938 - mse: 0.5938 - val_loss: 3.8239 - val_mse: 3.8239\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5144 - mse: 0.5144 - val_loss: 2.3154 - val_mse: 2.3154\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4032 - mse: 0.4032 - val_loss: 2.6365 - val_mse: 2.6365\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4126 - mse: 0.4126 - val_loss: 2.3911 - val_mse: 2.3911\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3635 - mse: 0.3635 - val_loss: 2.4454 - val_mse: 2.4454\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3730 - mse: 0.3730 - val_loss: 2.2563 - val_mse: 2.2563\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3620 - mse: 0.3620 - val_loss: 2.3763 - val_mse: 2.3763\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3639 - mse: 0.3639 - val_loss: 3.0987 - val_mse: 3.0987\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3959 - mse: 0.3959 - val_loss: 2.4372 - val_mse: 2.4372\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3656 - mse: 0.3656 - val_loss: 2.2201 - val_mse: 2.2201\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125539-9wrha184/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125539-9wrha184/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.36559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.36559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.22015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.22015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.12552\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrural-sweep-8\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/9wrha184\u001b[0m\n",
            "2021-08-20 12:56:12,288 - wandb.wandb_agent - INFO - Cleaning up finished run: 9wrha184\n",
            "2021-08-20 12:56:12,528 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:56:12,528 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.16374540874063503\n",
            "\tdropout_rate2: 0.10643356097227366\n",
            "\tdropout_rate3: 0.4484676109604525\n",
            "\thidden1: 20\n",
            "\thidden2: 41\n",
            "\thidden3: 119\n",
            "\tlearning_rate: 0.011424170988253327\n",
            "\ttime_step: 17\n",
            "2021-08-20 12:56:12,530 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.16374540874063503 --dropout_rate2=0.10643356097227366 --dropout_rate3=0.4484676109604525 --hidden1=20 --hidden2=41 --hidden3=119 --learning_rate=0.011424170988253327 --time_step=17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprime-sweep-9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/q98awyjg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125615-q98awyjg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:56:17.291971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17,539 - wandb.wandb_agent - INFO - Running runs: ['q98awyjg']\n",
            "2021-08-20 12:56:17.298917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.299534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.300673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.301260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.301821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.902142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.902833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.903603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:17.904183: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:56:17.904257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:56:17.937922: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:56:17.937959: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:56:17.938003: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:56:18.069484: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:56:18.069724: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:56:18.247258: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:56:22.196740: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:56 - loss: 9.6776 - mse: 9.67762021-08-20 12:56:22.799350: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:56:22.799390: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 5.1990 - mse: 5.1990 2021-08-20 12:56:23.054473: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:56:23.055214: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:56:23.205768: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 539 callback api events and 536 activity events. \n",
            "2021-08-20 12:56:23.213390: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:56:23.224416: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23\n",
            "\n",
            "2021-08-20 12:56:23.232628: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:56:23.248047: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23\n",
            "\n",
            "2021-08-20 12:56:23.250949: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:56:23.251477: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125615-q98awyjg/files/train/plugins/profile/2021_08_20_12_56_23/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 9.9022 - mse: 9.9022WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_begin` time: 0.0403s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0563s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 26ms/step - loss: 0.9848 - mse: 0.9848 - val_loss: 0.9620 - val_mse: 0.9620\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.1998 - mse: 0.1998 - val_loss: 1.2218 - val_mse: 1.2218\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1291 - mse: 0.1291 - val_loss: 0.7655 - val_mse: 0.7655\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1111 - mse: 0.1111 - val_loss: 0.7353 - val_mse: 0.7353\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0917 - mse: 0.0917 - val_loss: 0.7957 - val_mse: 0.7957\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.7756 - val_mse: 0.7756\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0768 - mse: 0.0768 - val_loss: 0.6857 - val_mse: 0.6857\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 0.7487 - val_mse: 0.7487\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.8933 - val_mse: 0.8933\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0632 - mse: 0.0632 - val_loss: 0.9103 - val_mse: 0.9103\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0582 - mse: 0.0582 - val_loss: 0.8361 - val_mse: 0.8361\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0519 - mse: 0.0519 - val_loss: 0.5883 - val_mse: 0.5883\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.5677 - val_mse: 0.5677\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0469 - mse: 0.0469 - val_loss: 0.6904 - val_mse: 0.6904\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.7907 - val_mse: 0.7907\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.6650 - val_mse: 0.6650\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.4629 - val_mse: 0.4629\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.6241 - val_mse: 0.6241\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.5861 - val_mse: 0.5861\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.7089 - val_mse: 0.7089\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.7009 - val_mse: 0.7009\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.6253 - val_mse: 0.6253\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.5414 - val_mse: 0.5414\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.5498 - val_mse: 0.5498\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.5569 - val_mse: 0.5569\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.5226 - val_mse: 0.5226\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.4953 - val_mse: 0.4953\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125615-q98awyjg/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125615-q98awyjg/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01766\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01766\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.4953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.4953\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464199\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.46289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mprime-sweep-9\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/q98awyjg\u001b[0m\n",
            "2021-08-20 12:56:48,087 - wandb.wandb_agent - INFO - Cleaning up finished run: q98awyjg\n",
            "2021-08-20 12:56:48,343 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:56:48,343 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.0740468236040662\n",
            "\tdropout_rate2: 0.09559677153530843\n",
            "\tdropout_rate3: 0.329936753684429\n",
            "\thidden1: 40\n",
            "\thidden2: 41\n",
            "\thidden3: 99\n",
            "\tlearning_rate: 0.012188889022110147\n",
            "\ttime_step: 5\n",
            "2021-08-20 12:56:48,345 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.0740468236040662 --dropout_rate2=0.09559677153530843 --dropout_rate3=0.329936753684429 --hidden1=40 --hidden2=41 --hidden3=99 --learning_rate=0.012188889022110147 --time_step=5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-sweep-10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/njgcasww\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125650-njgcasww\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:56:52.992801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:52.999833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.000424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53,350 - wandb.wandb_agent - INFO - Running runs: ['njgcasww']\n",
            "2021-08-20 12:56:53.001623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.002213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.002781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.611030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.611727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.612317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:56:53.612844: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:56:53.612894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:56:53.644789: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:56:53.644820: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:56:53.644853: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:56:53.777781: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:56:53.777948: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:56:53.952294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:56:57.871582: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:54 - loss: 10.4264 - mse: 10.42642021-08-20 12:56:58.499303: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:56:58.499352: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 8.5492 - mse: 8.5492   2021-08-20 12:56:58.744076: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:56:58.744508: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:56:58.896796: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 373 callback api events and 370 activity events. \n",
            "2021-08-20 12:56:58.903190: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:56:58.912124: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58\n",
            "\n",
            "2021-08-20 12:56:58.918823: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:56:58.932837: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58\n",
            "\n",
            "2021-08-20 12:56:58.936129: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:56:58.936674: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125650-njgcasww/files/train/plugins/profile/2021_08_20_12_56_58/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 5.7641 - mse: 5.7641WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_begin` time: 0.0389s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0586s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 23ms/step - loss: 1.3676 - mse: 1.3676 - val_loss: 2.2928 - val_mse: 2.2928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.2473 - mse: 0.2473 - val_loss: 0.9047 - val_mse: 0.9047\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.7424 - val_mse: 0.7424\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0768 - mse: 0.0768 - val_loss: 0.6698 - val_mse: 0.6698\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0800 - mse: 0.0800 - val_loss: 0.6113 - val_mse: 0.6113\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0678 - mse: 0.0678 - val_loss: 0.6754 - val_mse: 0.6754\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0681 - mse: 0.0681 - val_loss: 0.5157 - val_mse: 0.5157\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0563 - mse: 0.0563 - val_loss: 0.5514 - val_mse: 0.5514\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.4467 - val_mse: 0.4467\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0528 - mse: 0.0528 - val_loss: 0.5909 - val_mse: 0.5909\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0453 - mse: 0.0453 - val_loss: 0.4223 - val_mse: 0.4223\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.4441 - val_mse: 0.4441\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.4092 - val_mse: 0.4092\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.5427 - val_mse: 0.5427\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.4671 - val_mse: 0.4671\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.3925 - val_mse: 0.3925\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.5779 - val_mse: 0.5779\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.4937 - val_mse: 0.4937\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.4785 - val_mse: 0.4785\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.4851 - val_mse: 0.4851\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.4536 - val_mse: 0.4536\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.4820 - val_mse: 0.4820\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.5312 - val_mse: 0.5312\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.4875 - val_mse: 0.4875\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.5607 - val_mse: 0.5607\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.5955 - val_mse: 0.5955\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125650-njgcasww/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125650-njgcasww/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.59551\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.59551\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.39253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcomfy-sweep-10\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/njgcasww\u001b[0m\n",
            "2021-08-20 12:57:39,392 - wandb.wandb_agent - INFO - Cleaning up finished run: njgcasww\n",
            "2021-08-20 12:57:39,666 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:57:39,666 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.08535670889676483\n",
            "\tdropout_rate2: 0.11563759798277783\n",
            "\tdropout_rate3: 0.38324059780283193\n",
            "\thidden1: 21\n",
            "\thidden2: 118\n",
            "\thidden3: 118\n",
            "\tlearning_rate: 0.012141864674450751\n",
            "\ttime_step: 15\n",
            "2021-08-20 12:57:39,668 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.08535670889676483 --dropout_rate2=0.11563759798277783 --dropout_rate3=0.38324059780283193 --hidden1=21 --hidden2=118 --hidden3=118 --learning_rate=0.012141864674450751 --time_step=15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvivid-sweep-11\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/v3rk5flu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125742-v3rk5flu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:57:44.294910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44,675 - wandb.wandb_agent - INFO - Running runs: ['v3rk5flu']\n",
            "2021-08-20 12:57:44.301846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.302457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.303556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.304171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.304747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.917742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.918374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.918972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:57:44.919494: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:57:44.919555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:57:44.947850: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:57:44.947885: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:57:44.947927: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:57:45.090761: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:57:45.090949: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:57:45.256800: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:57:49.252421: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 10.0406 - mse: 10.04062021-08-20 12:57:49.879461: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:57:49.879513: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.2510 - mse: 5.2510   2021-08-20 12:57:50.143526: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:57:50.144090: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:57:50.288790: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 514 callback api events and 511 activity events. \n",
            "2021-08-20 12:57:50.296940: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:57:50.308397: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50\n",
            "\n",
            "2021-08-20 12:57:50.316182: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:57:50.331403: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50\n",
            "\n",
            "2021-08-20 12:57:50.334339: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:57:50.334848: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125742-v3rk5flu/files/train/plugins/profile/2021_08_20_12_57_50/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 7.8643 - mse: 7.8643WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_begin` time: 0.0409s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 0.7957 - mse: 0.7957 - val_loss: 1.3105 - val_mse: 1.3105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1409 - mse: 0.1409 - val_loss: 1.1222 - val_mse: 1.1222\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0958 - mse: 0.0958 - val_loss: 0.7591 - val_mse: 0.7591\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0771 - mse: 0.0771 - val_loss: 0.6551 - val_mse: 0.6551\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0813 - mse: 0.0813 - val_loss: 1.0740 - val_mse: 1.0740\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0702 - mse: 0.0702 - val_loss: 0.7309 - val_mse: 0.7309\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.5532 - val_mse: 0.5532\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0541 - mse: 0.0541 - val_loss: 0.7022 - val_mse: 0.7022\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0528 - mse: 0.0528 - val_loss: 0.6026 - val_mse: 0.6026\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0510 - mse: 0.0510 - val_loss: 0.7876 - val_mse: 0.7876\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0493 - mse: 0.0493 - val_loss: 0.6931 - val_mse: 0.6931\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.8614 - val_mse: 0.8614\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.5925 - val_mse: 0.5925\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.5644 - val_mse: 0.5644\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.6698 - val_mse: 0.6698\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.6828 - val_mse: 0.6828\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.6390 - val_mse: 0.6390\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2725\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125742-v3rk5flu/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125742-v3rk5flu/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.0292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.63903\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.63903\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464280\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.55321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mvivid-sweep-11\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/v3rk5flu\u001b[0m\n",
            "2021-08-20 12:58:10,057 - wandb.wandb_agent - INFO - Cleaning up finished run: v3rk5flu\n",
            "2021-08-20 12:58:10,277 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:58:10,277 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.05881709768476594\n",
            "\tdropout_rate2: 0.315820140970418\n",
            "\tdropout_rate3: 0.23060115179549417\n",
            "\thidden1: 34\n",
            "\thidden2: 75\n",
            "\thidden3: 119\n",
            "\tlearning_rate: 0.0889863677332161\n",
            "\ttime_step: 17\n",
            "2021-08-20 12:58:10,279 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.05881709768476594 --dropout_rate2=0.315820140970418 --dropout_rate3=0.23060115179549417 --hidden1=34 --hidden2=75 --hidden3=119 --learning_rate=0.0889863677332161 --time_step=17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mancient-sweep-12\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/jwdqxzlg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125812-jwdqxzlg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:58:15,287 - wandb.wandb_agent - INFO - Running runs: ['jwdqxzlg']\n",
            "2021-08-20 12:58:14.882009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:14.888853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:14.889441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:14.890805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:14.891410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:14.892060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:15.489790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:15.490411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:15.490983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:15.491490: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:58:15.491545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:58:15.519590: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:58:15.519628: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:58:15.519682: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:58:15.652588: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:58:15.652765: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:58:15.820399: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:58:19.715781: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:54 - loss: 9.8328 - mse: 9.83282021-08-20 12:58:20.356315: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:58:20.356376: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 51.4157 - mse: 51.41572021-08-20 12:58:20.622486: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:58:20.623049: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:58:20.772276: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 544 callback api events and 541 activity events. \n",
            "2021-08-20 12:58:20.780596: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:58:20.792959: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20\n",
            "\n",
            "2021-08-20 12:58:20.801355: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:58:20.817785: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20\n",
            "\n",
            "2021-08-20 12:58:20.822064: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:58:20.822798: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125812-jwdqxzlg/files/train/plugins/profile/2021_08_20_12_58_20/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 42.5591 - mse: 42.5591WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_begin` time: 0.0420s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0589s). Check your callbacks.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4528 - val_mse: 2.4528\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4186 - mse: 0.4186 - val_loss: 2.4921 - val_mse: 2.4921\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4436 - mse: 0.4436 - val_loss: 2.2101 - val_mse: 2.2101\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4256 - mse: 0.4256 - val_loss: 2.2313 - val_mse: 2.2313\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4267 - mse: 0.4267 - val_loss: 2.4185 - val_mse: 2.4185\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3990 - mse: 0.3990 - val_loss: 2.2569 - val_mse: 2.2569\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4090 - mse: 0.4090 - val_loss: 2.4128 - val_mse: 2.4128\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4088 - mse: 0.4088 - val_loss: 3.2111 - val_mse: 3.2111\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4090 - mse: 0.4090 - val_loss: 2.8919 - val_mse: 2.8919\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3961 - mse: 0.3961 - val_loss: 2.3294 - val_mse: 2.3294\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4271 - mse: 0.4271 - val_loss: 2.1834 - val_mse: 2.1834\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4085 - mse: 0.4085 - val_loss: 2.3370 - val_mse: 2.3370\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3881 - mse: 0.3881 - val_loss: 2.3848 - val_mse: 2.3848\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3779 - mse: 0.3779 - val_loss: 2.9575 - val_mse: 2.9575\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3904 - mse: 0.3904 - val_loss: 2.5285 - val_mse: 2.5285\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3831 - mse: 0.3831 - val_loss: 2.2888 - val_mse: 2.2888\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4099 - mse: 0.4099 - val_loss: 2.1398 - val_mse: 2.1398\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4456 - mse: 0.4456 - val_loss: 2.7501 - val_mse: 2.7501\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4285 - mse: 0.4285 - val_loss: 2.4103 - val_mse: 2.4103\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4621 - mse: 0.4621 - val_loss: 2.2132 - val_mse: 2.2132\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4087 - mse: 0.4087 - val_loss: 2.4949 - val_mse: 2.4949\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4520 - mse: 0.4520 - val_loss: 3.8642 - val_mse: 3.8642\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4791 - mse: 0.4791 - val_loss: 2.9181 - val_mse: 2.9181\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.4176 - mse: 0.4176 - val_loss: 2.3828 - val_mse: 2.3828\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4092 - mse: 0.4092 - val_loss: 2.8437 - val_mse: 2.8437\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4176 - mse: 0.4176 - val_loss: 2.7015 - val_mse: 2.7015\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4043 - mse: 0.4043 - val_loss: 2.4221 - val_mse: 2.4221\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2896\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125812-jwdqxzlg/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125812-jwdqxzlg/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.4043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.4043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.42209\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.42209\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.13977\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mancient-sweep-12\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/jwdqxzlg\u001b[0m\n",
            "2021-08-20 12:58:45,773 - wandb.wandb_agent - INFO - Cleaning up finished run: jwdqxzlg\n",
            "2021-08-20 12:58:46,024 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:58:46,024 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.09130896346703185\n",
            "\tdropout_rate2: 0.08816882845964741\n",
            "\tdropout_rate3: 0.5418770002484968\n",
            "\thidden1: 101\n",
            "\thidden2: 33\n",
            "\thidden3: 112\n",
            "\tlearning_rate: 0.03465024880203915\n",
            "\ttime_step: 5\n",
            "2021-08-20 12:58:46,026 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.09130896346703185 --dropout_rate2=0.08816882845964741 --dropout_rate3=0.5418770002484968 --hidden1=101 --hidden2=33 --hidden3=112 --learning_rate=0.03465024880203915 --time_step=5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-sweep-13\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/roeankp3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125848-roeankp3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:58:50.703678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:50.710793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:50.711395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:51,035 - wandb.wandb_agent - INFO - Running runs: ['roeankp3']\n",
            "2021-08-20 12:58:50.712595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:50.713217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:50.713789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:51.321636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:51.322325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:51.322949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:58:51.323461: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:58:51.323513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:58:51.359847: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:58:51.359882: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:58:51.359915: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:58:51.489760: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:58:51.489934: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:58:51.663788: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:58:55.642442: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:59 - loss: 10.7276 - mse: 10.72762021-08-20 12:58:56.270417: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:58:56.270467: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 5.9703 - mse: 5.9703   2021-08-20 12:58:56.519682: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:58:56.520144: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:58:56.674105: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 365 callback api events and 362 activity events. \n",
            "2021-08-20 12:58:56.681498: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:58:56.691211: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56\n",
            "\n",
            "2021-08-20 12:58:56.698536: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:58:56.714289: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56\n",
            "\n",
            "2021-08-20 12:58:56.717462: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:58:56.718049: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125848-roeankp3/files/train/plugins/profile/2021_08_20_12_58_56/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 7.2702 - mse: 7.2702WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_begin` time: 0.0396s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0595s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 1.0489 - mse: 1.0489 - val_loss: 2.8191 - val_mse: 2.8191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.5293 - mse: 0.5293 - val_loss: 1.9974 - val_mse: 1.9974\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3429 - mse: 0.3429 - val_loss: 1.1394 - val_mse: 1.1394\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1963 - mse: 0.1963 - val_loss: 1.0405 - val_mse: 1.0405\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.1125 - mse: 0.1125 - val_loss: 0.7340 - val_mse: 0.7340\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0778 - mse: 0.0778 - val_loss: 0.6576 - val_mse: 0.6576\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.6495 - val_mse: 0.6495\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.4004 - val_mse: 0.4004\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.6079 - val_mse: 0.6079\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.6484 - val_mse: 0.6484\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.7502 - val_mse: 0.7502\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.6524 - val_mse: 0.6524\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.6220 - val_mse: 0.6220\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.6517 - val_mse: 0.6517\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.5651 - val_mse: 0.5651\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.6649 - val_mse: 0.6649\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.6704 - val_mse: 0.6704\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.6782 - val_mse: 0.6782\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125848-roeankp3/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125848-roeankp3/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.67824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.67824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464346\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.40043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33meffortless-sweep-13\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/roeankp3\u001b[0m\n",
            "2021-08-20 12:59:21,520 - wandb.wandb_agent - INFO - Cleaning up finished run: roeankp3\n",
            "2021-08-20 12:59:21,847 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:59:21,847 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.2672977248593362\n",
            "\tdropout_rate2: 0.08686397249447309\n",
            "\tdropout_rate3: 0.4125788593643521\n",
            "\thidden1: 120\n",
            "\thidden2: 72\n",
            "\thidden3: 104\n",
            "\tlearning_rate: 0.02380239315157554\n",
            "\ttime_step: 21\n",
            "2021-08-20 12:59:21,849 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.2672977248593362 --dropout_rate2=0.08686397249447309 --dropout_rate3=0.4125788593643521 --hidden1=120 --hidden2=72 --hidden3=104 --learning_rate=0.02380239315157554 --time_step=21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-sweep-14\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/3a7ypw34\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_125924-3a7ypw34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 12:59:26,853 - wandb.wandb_agent - INFO - Running runs: ['3a7ypw34']\n",
            "2021-08-20 12:59:26.473423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:26.480352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:26.480993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:26.482241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:26.482874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:26.483413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:27.088318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:27.089021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:27.089617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 12:59:27.090139: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 12:59:27.090189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 12:59:27.118584: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:59:27.118620: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 12:59:27.118669: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 12:59:27.257471: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:59:27.257681: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 12:59:27.421372: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 12:59:31.411273: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:00 - loss: 10.6833 - mse: 10.68332021-08-20 12:59:32.031364: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 12:59:32.031417: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 9.4081 - mse: 9.4081   2021-08-20 12:59:32.280266: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 12:59:32.280857: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 12:59:32.433508: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 571 callback api events and 568 activity events. \n",
            "2021-08-20 12:59:32.442620: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 12:59:32.456412: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32\n",
            "\n",
            "2021-08-20 12:59:32.465874: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 12:59:32.482473: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32\n",
            "\n",
            "2021-08-20 12:59:32.485729: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 12:59:32.486348: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_125924-3a7ypw34/files/train/plugins/profile/2021_08_20_12_59_32/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 12.9097 - mse: 12.9097WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_begin` time: 0.0392s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_end` time: 0.0597s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 1.0785 - mse: 1.0785 - val_loss: 2.7750 - val_mse: 2.7750\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.5314 - mse: 0.5314 - val_loss: 2.5391 - val_mse: 2.5391\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4859 - mse: 0.4859 - val_loss: 2.5296 - val_mse: 2.5296\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4707 - mse: 0.4707 - val_loss: 2.2859 - val_mse: 2.2859\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4734 - mse: 0.4734 - val_loss: 2.6001 - val_mse: 2.6001\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4407 - mse: 0.4407 - val_loss: 2.6933 - val_mse: 2.6933\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4292 - mse: 0.4292 - val_loss: 2.5245 - val_mse: 2.5245\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4328 - mse: 0.4328 - val_loss: 2.5473 - val_mse: 2.5473\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4121 - mse: 0.4121 - val_loss: 2.2679 - val_mse: 2.2679\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3893 - mse: 0.3893 - val_loss: 2.4077 - val_mse: 2.4077\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3773 - mse: 0.3773 - val_loss: 2.4945 - val_mse: 2.4945\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3689 - mse: 0.3689 - val_loss: 2.3562 - val_mse: 2.3562\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3594 - mse: 0.3594 - val_loss: 2.3207 - val_mse: 2.3207\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3532 - mse: 0.3532 - val_loss: 2.3497 - val_mse: 2.3497\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3446 - mse: 0.3446 - val_loss: 2.6118 - val_mse: 2.6118\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3468 - mse: 0.3468 - val_loss: 2.4921 - val_mse: 2.4921\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3440 - mse: 0.3440 - val_loss: 2.5973 - val_mse: 2.5973\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3436 - mse: 0.3436 - val_loss: 2.3986 - val_mse: 2.3986\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3494 - mse: 0.3494 - val_loss: 2.6337 - val_mse: 2.6337\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_125924-3a7ypw34/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_125924-3a7ypw34/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.34936\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.34936\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.63374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.63374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.2679\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÜ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÜ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwarm-sweep-14\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/3a7ypw34\u001b[0m\n",
            "2021-08-20 12:59:57,320 - wandb.wandb_agent - INFO - Cleaning up finished run: 3a7ypw34\n",
            "2021-08-20 12:59:57,687 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 12:59:57,687 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.05507032789767777\n",
            "\tdropout_rate2: 0.1287704463098196\n",
            "\tdropout_rate3: 0.389751726530389\n",
            "\thidden1: 62\n",
            "\thidden2: 19\n",
            "\thidden3: 118\n",
            "\tlearning_rate: 0.0532556656068591\n",
            "\ttime_step: 23\n",
            "2021-08-20 12:59:57,688 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.05507032789767777 --dropout_rate2=0.1287704463098196 --dropout_rate3=0.389751726530389 --hidden1=62 --hidden2=19 --hidden3=118 --learning_rate=0.0532556656068591 --time_step=23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlogical-sweep-15\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/l5eh69gs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130000-l5eh69gs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:00:02.335744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.342512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.343148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02,697 - wandb.wandb_agent - INFO - Running runs: ['l5eh69gs']\n",
            "2021-08-20 13:00:02.344418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.345081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.345631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.948311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.949048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.949628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:02.950155: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:00:02.950207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:00:02.984400: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:00:02.984526: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:00:02.984609: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:00:03.134324: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:00:03.134485: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:00:03.313167: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:00:07.241502: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:55 - loss: 10.1680 - mse: 10.16802021-08-20 13:00:07.872703: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:00:07.872758: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 33s - loss: 24.6873 - mse: 24.6873 2021-08-20 13:00:08.131265: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:00:08.132051: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:00:08.280482: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 622 callback api events and 619 activity events. \n",
            "2021-08-20 13:00:08.290223: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:00:08.303452: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08\n",
            "\n",
            "2021-08-20 13:00:08.312704: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:00:08.329988: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08\n",
            "\n",
            "2021-08-20 13:00:08.332977: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:00:08.333522: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130000-l5eh69gs/files/train/plugins/profile/2021_08_20_13_00_08/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 16.6111 - mse: 16.6111WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0072s vs `on_train_batch_begin` time: 0.0406s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_train_batch_end` time: 0.0616s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 26ms/step - loss: 1.2182 - mse: 1.2182 - val_loss: 2.1705 - val_mse: 2.1705\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4738 - mse: 0.4738 - val_loss: 2.5702 - val_mse: 2.5702\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4914 - mse: 0.4914 - val_loss: 2.2649 - val_mse: 2.2649\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4424 - mse: 0.4424 - val_loss: 2.4281 - val_mse: 2.4281\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4639 - mse: 0.4639 - val_loss: 2.3637 - val_mse: 2.3637\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4167 - mse: 0.4167 - val_loss: 2.6125 - val_mse: 2.6125\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4175 - mse: 0.4175 - val_loss: 2.1246 - val_mse: 2.1246\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4197 - mse: 0.4197 - val_loss: 2.3701 - val_mse: 2.3701\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4147 - mse: 0.4147 - val_loss: 3.2473 - val_mse: 3.2473\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4194 - mse: 0.4194 - val_loss: 2.1169 - val_mse: 2.1169\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4229 - mse: 0.4229 - val_loss: 2.2988 - val_mse: 2.2988\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4232 - mse: 0.4232 - val_loss: 2.9998 - val_mse: 2.9998\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4106 - mse: 0.4106 - val_loss: 2.4436 - val_mse: 2.4436\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4134 - mse: 0.4134 - val_loss: 2.3099 - val_mse: 2.3099\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4111 - mse: 0.4111 - val_loss: 2.5586 - val_mse: 2.5586\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3936 - mse: 0.3936 - val_loss: 2.4691 - val_mse: 2.4691\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4037 - mse: 0.4037 - val_loss: 3.0194 - val_mse: 3.0194\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4175 - mse: 0.4175 - val_loss: 2.7541 - val_mse: 2.7541\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4152 - mse: 0.4152 - val_loss: 2.1774 - val_mse: 2.1774\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.5163 - mse: 0.5163 - val_loss: 2.2444 - val_mse: 2.2444\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130000-l5eh69gs/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130000-l5eh69gs/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.51634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.51634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 2.2444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 2.2444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464421\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 2.11686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlogical-sweep-15\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/l5eh69gs\u001b[0m\n",
            "2021-08-20 13:00:33,183 - wandb.wandb_agent - INFO - Cleaning up finished run: l5eh69gs\n",
            "2021-08-20 13:00:33,430 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:00:33,430 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.07498808317443736\n",
            "\tdropout_rate2: 0.0534015150932514\n",
            "\tdropout_rate3: 0.44099310787406987\n",
            "\thidden1: 76\n",
            "\thidden2: 60\n",
            "\thidden3: 109\n",
            "\tlearning_rate: 0.03009862213030308\n",
            "\ttime_step: 23\n",
            "2021-08-20 13:00:33,432 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.07498808317443736 --dropout_rate2=0.0534015150932514 --dropout_rate3=0.44099310787406987 --hidden1=76 --hidden2=60 --hidden3=109 --learning_rate=0.03009862213030308 --time_step=23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdauntless-sweep-16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/xiznev8k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130035-xiznev8k\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:00:38.155514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38,442 - wandb.wandb_agent - INFO - Running runs: ['xiznev8k']\n",
            "2021-08-20 13:00:38.162614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.163225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.164559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.165284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.165934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.799838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.800517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.801138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:00:38.801667: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:00:38.801714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:00:38.832533: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:00:38.832564: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:00:38.832602: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:00:38.965950: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:00:38.966115: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:00:39.142558: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:00:43.180098: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:04 - loss: 11.3514 - mse: 11.35142021-08-20 13:00:43.794040: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:00:43.794090: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 30s - loss: 10.1507 - mse: 10.1507 2021-08-20 13:00:44.044406: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:00:44.045043: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:00:44.193942: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 608 callback api events and 605 activity events. \n",
            "2021-08-20 13:00:44.208058: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:00:44.220869: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44\n",
            "\n",
            "2021-08-20 13:00:44.229296: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:00:44.245411: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44\n",
            "\n",
            "2021-08-20 13:00:44.248322: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:00:44.249023: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130035-xiznev8k/files/train/plugins/profile/2021_08_20_13_00_44/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 10.7793 - mse: 10.7793WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_begin` time: 0.0395s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0586s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.9908 - mse: 0.9908 - val_loss: 2.2495 - val_mse: 2.2495\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5100 - mse: 0.5100 - val_loss: 3.0526 - val_mse: 3.0526\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.5338 - mse: 0.5338 - val_loss: 2.4910 - val_mse: 2.4910\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.4615 - mse: 0.4615 - val_loss: 2.3589 - val_mse: 2.3589\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3885 - mse: 0.3885 - val_loss: 1.4830 - val_mse: 1.4830\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3943 - mse: 0.3943 - val_loss: 1.8145 - val_mse: 1.8145\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4517 - mse: 0.4517 - val_loss: 2.4934 - val_mse: 2.4934\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3985 - mse: 0.3985 - val_loss: 2.4218 - val_mse: 2.4218\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3894 - mse: 0.3894 - val_loss: 2.2532 - val_mse: 2.2532\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3785 - mse: 0.3785 - val_loss: 2.6616 - val_mse: 2.6616\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3799 - mse: 0.3799 - val_loss: 2.6635 - val_mse: 2.6635\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3581 - mse: 0.3581 - val_loss: 2.2746 - val_mse: 2.2746\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3582 - mse: 0.3582 - val_loss: 2.5799 - val_mse: 2.5799\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3348 - mse: 0.3348 - val_loss: 1.9261 - val_mse: 1.9261\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2324 - mse: 0.2324 - val_loss: 1.8852 - val_mse: 1.8852\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3676\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130035-xiznev8k/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130035-xiznev8k/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.2324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.2324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 1.88521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 1.88521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 1.48302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdauntless-sweep-16\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/xiznev8k\u001b[0m\n",
            "2021-08-20 13:01:03,862 - wandb.wandb_agent - INFO - Cleaning up finished run: xiznev8k\n",
            "2021-08-20 13:01:04,119 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:01:04,119 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.24579721919918257\n",
            "\tdropout_rate2: 0.2578051491251325\n",
            "\tdropout_rate3: 0.5132987205744705\n",
            "\thidden1: 54\n",
            "\thidden2: 97\n",
            "\thidden3: 100\n",
            "\tlearning_rate: 0.0041042067448244296\n",
            "\ttime_step: 8\n",
            "2021-08-20 13:01:04,122 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.24579721919918257 --dropout_rate2=0.2578051491251325 --dropout_rate3=0.5132987205744705 --hidden1=54 --hidden2=97 --hidden3=100 --learning_rate=0.0041042067448244296 --time_step=8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclassic-sweep-17\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/92jauxf2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130106-92jauxf2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:01:08.702922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:09,132 - wandb.wandb_agent - INFO - Running runs: ['92jauxf2']\n",
            "2021-08-20 13:01:08.710077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:08.710722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:08.711829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:08.712421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:08.712990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:09.334111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:09.334814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:09.335391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:09.335908: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:01:09.335956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:01:09.366549: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:01:09.366584: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:01:09.366681: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:01:09.502485: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:01:09.503190: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:01:09.666193: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:01:13.663154: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 10.2309 - mse: 10.23092021-08-20 13:01:14.280000: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:01:14.280050: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 8.6386 - mse: 8.6386   2021-08-20 13:01:14.532575: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:01:14.533037: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:01:14.679976: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 413 callback api events and 410 activity events. \n",
            "2021-08-20 13:01:14.686579: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:01:14.696296: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14\n",
            "\n",
            "2021-08-20 13:01:14.703487: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:01:14.718131: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14\n",
            "\n",
            "2021-08-20 13:01:14.721122: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:01:14.721661: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130106-92jauxf2/files/train/plugins/profile/2021_08_20_13_01_14/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 6.5208 - mse: 6.5208WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_begin` time: 0.0400s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 0.0564s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 0.6155 - mse: 0.6155 - val_loss: 0.8888 - val_mse: 0.8888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.6777 - val_mse: 0.6777\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1685 - mse: 0.1685 - val_loss: 0.6279 - val_mse: 0.6279\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1386 - mse: 0.1386 - val_loss: 0.5454 - val_mse: 0.5454\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1312 - mse: 0.1312 - val_loss: 0.7750 - val_mse: 0.7750\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1281 - mse: 0.1281 - val_loss: 0.4497 - val_mse: 0.4497\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1325 - mse: 0.1325 - val_loss: 0.9648 - val_mse: 0.9648\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1230 - mse: 0.1230 - val_loss: 0.5954 - val_mse: 0.5954\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.5256 - val_mse: 0.5256\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1018 - mse: 0.1018 - val_loss: 0.4267 - val_mse: 0.4267\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.4605 - val_mse: 0.4605\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.4734 - val_mse: 0.4734\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0918 - mse: 0.0918 - val_loss: 0.6295 - val_mse: 0.6295\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0820 - mse: 0.0820 - val_loss: 0.4327 - val_mse: 0.4327\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.5176 - val_mse: 0.5176\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.6124 - val_mse: 0.6124\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0767 - mse: 0.0767 - val_loss: 0.5321 - val_mse: 0.5321\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0707 - mse: 0.0707 - val_loss: 0.4844 - val_mse: 0.4844\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 0s 6ms/step - loss: 0.0621 - mse: 0.0621 - val_loss: 0.5120 - val_mse: 0.5120\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0657 - mse: 0.0657 - val_loss: 0.6641 - val_mse: 0.6641\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130106-92jauxf2/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130106-92jauxf2/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0657\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.0657\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.66406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.66406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.42667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mclassic-sweep-17\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/92jauxf2\u001b[0m\n",
            "2021-08-20 13:01:39,609 - wandb.wandb_agent - INFO - Cleaning up finished run: 92jauxf2\n",
            "2021-08-20 13:01:39,883 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:01:39,883 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.19361563828000855\n",
            "\tdropout_rate2: 0.1692573863859591\n",
            "\tdropout_rate3: 0.46877328101650406\n",
            "\thidden1: 30\n",
            "\thidden2: 102\n",
            "\thidden3: 107\n",
            "\tlearning_rate: 0.019878239199489372\n",
            "\ttime_step: 10\n",
            "2021-08-20 13:01:39,885 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.19361563828000855 --dropout_rate2=0.1692573863859591 --dropout_rate3=0.46877328101650406 --hidden1=30 --hidden2=102 --hidden3=107 --learning_rate=0.019878239199489372 --time_step=10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-sweep-18\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/k0a39xrq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130142-k0a39xrq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:01:44.548576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:44,892 - wandb.wandb_agent - INFO - Running runs: ['k0a39xrq']\n",
            "2021-08-20 13:01:44.555918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:44.556512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:44.557946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:44.558545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:44.559138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:45.179664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:45.180293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:45.180878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:01:45.181384: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:01:45.181443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:01:45.214899: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:01:45.214930: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:01:45.215023: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:01:45.347221: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:01:45.347387: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:01:45.521921: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:01:49.469637: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:57 - loss: 9.6341 - mse: 9.63412021-08-20 13:01:50.113253: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:01:50.113304: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 33s - loss: 6.4705 - mse: 6.4705 2021-08-20 13:01:50.370715: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:01:50.371437: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:01:50.520539: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 448 callback api events and 445 activity events. \n",
            "2021-08-20 13:01:50.527542: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:01:50.537374: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50\n",
            "\n",
            "2021-08-20 13:01:50.544693: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:01:50.559962: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50\n",
            "\n",
            "2021-08-20 13:01:50.563285: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:01:50.563805: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130142-k0a39xrq/files/train/plugins/profile/2021_08_20_13_01_50/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 9.4655 - mse: 9.4655WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_begin` time: 0.0409s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0599s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 1.0785 - mse: 1.0785 - val_loss: 1.6406 - val_mse: 1.6406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2210 - mse: 0.2210 - val_loss: 0.9075 - val_mse: 0.9075\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1737 - mse: 0.1737 - val_loss: 0.4821 - val_mse: 0.4821\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1081 - mse: 0.1081 - val_loss: 0.8602 - val_mse: 0.8602\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.6187 - val_mse: 0.6187\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0697 - mse: 0.0697 - val_loss: 0.5011 - val_mse: 0.5011\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0716 - mse: 0.0716 - val_loss: 0.7377 - val_mse: 0.7377\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0573 - mse: 0.0573 - val_loss: 0.6571 - val_mse: 0.6571\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.5619 - val_mse: 0.5619\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0460 - mse: 0.0460 - val_loss: 0.6370 - val_mse: 0.6370\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.6030 - val_mse: 0.6030\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.6195 - val_mse: 0.6195\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.6504 - val_mse: 0.6504\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130142-k0a39xrq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130142-k0a39xrq/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02816\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02816\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.65037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.65037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.48211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrevived-sweep-18\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/k0a39xrq\u001b[0m\n",
            "2021-08-20 13:02:15,382 - wandb.wandb_agent - INFO - Cleaning up finished run: k0a39xrq\n",
            "2021-08-20 13:02:15,656 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:02:15,656 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.06095456991024786\n",
            "\tdropout_rate2: 0.30385875021690123\n",
            "\tdropout_rate3: 0.4422743274693636\n",
            "\thidden1: 40\n",
            "\thidden2: 55\n",
            "\thidden3: 67\n",
            "\tlearning_rate: 0.011677692306676802\n",
            "\ttime_step: 14\n",
            "2021-08-20 13:02:15,657 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.06095456991024786 --dropout_rate2=0.30385875021690123 --dropout_rate3=0.4422743274693636 --hidden1=40 --hidden2=55 --hidden3=67 --learning_rate=0.011677692306676802 --time_step=14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-sweep-19\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/bkktca8f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130218-bkktca8f\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:02:20.362188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.369202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.369811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20,666 - wandb.wandb_agent - INFO - Running runs: ['bkktca8f']\n",
            "2021-08-20 13:02:20.370972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.371559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.372135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.984273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.985221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.986042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:20.986810: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:02:20.986862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:02:21.022402: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:02:21.022440: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:02:21.022481: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:02:21.155409: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:02:21.155581: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:02:21.331383: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:02:25.322454: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:01 - loss: 9.1176 - mse: 9.11762021-08-20 13:02:25.952915: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:02:25.952968: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 4.7944 - mse: 4.7944 2021-08-20 13:02:26.202447: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:02:26.203174: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:02:26.348628: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 493 callback api events and 490 activity events. \n",
            "2021-08-20 13:02:26.355739: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:02:26.367299: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26\n",
            "\n",
            "2021-08-20 13:02:26.374792: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:02:26.389698: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26\n",
            "\n",
            "2021-08-20 13:02:26.392618: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:02:26.393140: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130218-bkktca8f/files/train/plugins/profile/2021_08_20_13_02_26/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 6.1406 - mse: 6.1406WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_begin` time: 0.0395s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.8639 - mse: 0.8639 - val_loss: 1.1140 - val_mse: 1.1140\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1894 - mse: 0.1894 - val_loss: 0.7691 - val_mse: 0.7691\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1715 - mse: 0.1715 - val_loss: 0.6855 - val_mse: 0.6855\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1499 - mse: 0.1499 - val_loss: 0.7862 - val_mse: 0.7862\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1419 - mse: 0.1419 - val_loss: 0.6680 - val_mse: 0.6680\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1105 - mse: 0.1105 - val_loss: 0.8062 - val_mse: 0.8062\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.7504 - val_mse: 0.7504\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0914 - mse: 0.0914 - val_loss: 0.7785 - val_mse: 0.7785\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0800 - mse: 0.0800 - val_loss: 0.5721 - val_mse: 0.5721\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.6691 - val_mse: 0.6691\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0644 - mse: 0.0644 - val_loss: 0.7215 - val_mse: 0.7215\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.8280 - val_mse: 0.8280\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.6230 - val_mse: 0.6230\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.6734 - val_mse: 0.6734\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.5807 - val_mse: 0.5807\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.6596 - val_mse: 0.6596\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.6950 - val_mse: 0.6950\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.5168 - val_mse: 0.5168\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.6609 - val_mse: 0.6609\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.6357 - val_mse: 0.6357\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.7043 - val_mse: 0.7043\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.5560 - val_mse: 0.5560\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.6385 - val_mse: 0.6385\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.5431 - val_mse: 0.5431\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.6434 - val_mse: 0.6434\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.5564 - val_mse: 0.5564\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.6532 - val_mse: 0.6532\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.5409 - val_mse: 0.5409\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130218-bkktca8f/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130218-bkktca8f/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.54094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.54094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464563\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.51684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlemon-sweep-19\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/bkktca8f\u001b[0m\n",
            "2021-08-20 13:02:51,166 - wandb.wandb_agent - INFO - Cleaning up finished run: bkktca8f\n",
            "2021-08-20 13:02:51,394 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:02:51,395 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.07470749486549831\n",
            "\tdropout_rate2: 0.39088241644243954\n",
            "\tdropout_rate3: 0.5533370032395305\n",
            "\thidden1: 44\n",
            "\thidden2: 58\n",
            "\thidden3: 67\n",
            "\tlearning_rate: 0.03859567384457785\n",
            "\ttime_step: 10\n",
            "2021-08-20 13:02:51,396 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.07470749486549831 --dropout_rate2=0.39088241644243954 --dropout_rate3=0.5533370032395305 --hidden1=44 --hidden2=58 --hidden3=67 --learning_rate=0.03859567384457785 --time_step=10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-sweep-20\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/tgys8koi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130253-tgys8koi\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:02:55.989774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:56,401 - wandb.wandb_agent - INFO - Running runs: ['tgys8koi']\n",
            "2021-08-20 13:02:55.997050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:55.997665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:55.998888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:55.999475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:56.000126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:56.606389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:56.607081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:56.607692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:02:56.608213: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:02:56.608263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:02:56.643838: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:02:56.643871: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:02:56.643967: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:02:56.789443: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:02:56.789619: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:02:56.957015: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:03:00.970670: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:02 - loss: 11.0930 - mse: 11.09302021-08-20 13:03:01.592950: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:03:01.593001: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 7.6967 - mse: 7.6967   2021-08-20 13:03:01.844638: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:03:01.850280: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:03:02.001167: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 443 callback api events and 440 activity events. \n",
            "2021-08-20 13:03:02.008109: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:03:02.017983: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02\n",
            "\n",
            "2021-08-20 13:03:02.025809: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:03:02.041004: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02\n",
            "\n",
            "2021-08-20 13:03:02.043977: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:03:02.044520: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130253-tgys8koi/files/train/plugins/profile/2021_08_20_13_03_02/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 8.3949 - mse: 8.3949WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_begin` time: 0.0395s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0588s). Check your callbacks.\n",
            "82/82 [==============================] - ETA: 0s - loss: 1.0424 - mse: 1.0424\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 1.0424 - mse: 1.0424 - val_loss: 2.4635 - val_mse: 2.4635\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.5917 - mse: 0.5917 - val_loss: 2.7080 - val_mse: 2.7080\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.6030 - mse: 0.6030 - val_loss: 2.2228 - val_mse: 2.2228\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4640 - mse: 0.4640 - val_loss: 2.2744 - val_mse: 2.2744\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4168 - mse: 0.4168 - val_loss: 2.2676 - val_mse: 2.2676\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4085 - mse: 0.4085 - val_loss: 2.3127 - val_mse: 2.3127\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3841 - mse: 0.3841 - val_loss: 2.5289 - val_mse: 2.5289\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3629 - mse: 0.3629 - val_loss: 2.3293 - val_mse: 2.3293\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.3693 - mse: 0.3693 - val_loss: 2.3809 - val_mse: 2.3809\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3613 - mse: 0.3613 - val_loss: 2.5669 - val_mse: 2.5669\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3730 - mse: 0.3730 - val_loss: 2.4824 - val_mse: 2.4824\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2389 - mse: 0.2389 - val_loss: 1.5820 - val_mse: 1.5820\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 1.2519 - val_mse: 1.2519\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.9281 - val_mse: 0.9281\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 1.0368 - val_mse: 1.0368\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.8415 - val_mse: 0.8415\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.9395 - val_mse: 0.9395\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.9527 - val_mse: 0.9527\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.9126 - val_mse: 0.9126\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.9326 - val_mse: 0.9326\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.9632 - val_mse: 0.9632\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.8748 - val_mse: 0.8748\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.9023 - val_mse: 0.9023\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 1.0148 - val_mse: 1.0148\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.7849 - val_mse: 0.7849\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.7369 - val_mse: 0.7369\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.7345 - val_mse: 0.7345\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.9976 - val_mse: 0.9976\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.9641 - val_mse: 0.9641\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.8182 - val_mse: 0.8182\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.7298 - val_mse: 0.7298\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.7742 - val_mse: 0.7742\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.7359 - val_mse: 0.7359\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.8808 - val_mse: 0.8808\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 1.0049 - val_mse: 1.0049\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.9225 - val_mse: 0.9225\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.9049 - val_mse: 0.9049\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.9035 - val_mse: 0.9035\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.8389 - val_mse: 0.8389\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 1.0648 - val_mse: 1.0648\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.8927 - val_mse: 0.8927\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130253-tgys8koi/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130253-tgys8koi/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.02833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.02833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.89272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.89272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.72978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmorning-sweep-20\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/tgys8koi\u001b[0m\n",
            "2021-08-20 13:03:37,045 - wandb.wandb_agent - INFO - Cleaning up finished run: tgys8koi\n",
            "2021-08-20 13:03:37,324 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:03:37,324 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.09305142846103737\n",
            "\tdropout_rate2: 0.2925743892876117\n",
            "\tdropout_rate3: 0.34619566375909844\n",
            "\thidden1: 24\n",
            "\thidden2: 104\n",
            "\thidden3: 50\n",
            "\tlearning_rate: 0.013016478664623842\n",
            "\ttime_step: 27\n",
            "2021-08-20 13:03:37,325 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.09305142846103737 --dropout_rate2=0.2925743892876117 --dropout_rate3=0.34619566375909844 --hidden1=24 --hidden2=104 --hidden3=50 --learning_rate=0.013016478664623842 --time_step=27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrimson-sweep-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/jq0kjmv8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130339-jq0kjmv8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:03:42.041497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42,331 - wandb.wandb_agent - INFO - Running runs: ['jq0kjmv8']\n",
            "2021-08-20 13:03:42.048489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.049110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.050247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.050881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.051425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.667960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.668601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.669209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:03:42.669739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:03:42.669789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:03:42.699538: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:03:42.699574: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:03:42.699612: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:03:42.837215: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:03:42.837378: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:03:43.003671: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:03:47.059487: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 6:06 - loss: 9.2211 - mse: 9.22112021-08-20 13:03:47.689448: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:03:47.689491: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 32s - loss: 4.9030 - mse: 4.9030 2021-08-20 13:03:47.949129: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:03:47.949965: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:03:48.100056: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 654 callback api events and 651 activity events. \n",
            "2021-08-20 13:03:48.108609: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:03:48.122135: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48\n",
            "\n",
            "2021-08-20 13:03:48.133066: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:03:48.148643: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48\n",
            "\n",
            "2021-08-20 13:03:48.151453: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:03:48.152012: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130339-jq0kjmv8/files/train/plugins/profile/2021_08_20_13_03_48/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 24s - loss: 4.7173 - mse: 4.7173WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_begin` time: 0.0410s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_end` time: 0.0593s). Check your callbacks.\n",
            "82/82 [==============================] - 7s 26ms/step - loss: 0.7804 - mse: 0.7804 - val_loss: 2.5580 - val_mse: 2.5580\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.3540 - mse: 0.3540 - val_loss: 1.2342 - val_mse: 1.2342\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.8657 - val_mse: 0.8657\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1240 - mse: 0.1240 - val_loss: 0.9710 - val_mse: 0.9710\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1067 - mse: 0.1067 - val_loss: 0.7703 - val_mse: 0.7703\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 1.0207 - val_mse: 1.0207\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0860 - mse: 0.0860 - val_loss: 1.0072 - val_mse: 1.0072\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0793 - mse: 0.0793 - val_loss: 0.8397 - val_mse: 0.8397\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.9364 - val_mse: 0.9364\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 1.0491 - val_mse: 1.0491\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0582 - mse: 0.0582 - val_loss: 0.8736 - val_mse: 0.8736\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.8919 - val_mse: 0.8919\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.8578 - val_mse: 0.8578\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.9030 - val_mse: 0.9030\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.9720 - val_mse: 0.9720\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4711\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130339-jq0kjmv8/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130339-jq0kjmv8/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.03616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.03616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.97195\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.97195\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.77032\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcrimson-sweep-21\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/jq0kjmv8\u001b[0m\n",
            "2021-08-20 13:04:07,725 - wandb.wandb_agent - INFO - Cleaning up finished run: jq0kjmv8\n",
            "2021-08-20 13:04:07,980 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:04:07,980 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.09824419961472201\n",
            "\tdropout_rate2: 0.4234025897486593\n",
            "\tdropout_rate3: 0.5098146397818708\n",
            "\thidden1: 31\n",
            "\thidden2: 93\n",
            "\thidden3: 57\n",
            "\tlearning_rate: 0.007495422019370002\n",
            "\ttime_step: 7\n",
            "2021-08-20 13:04:07,981 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.09824419961472201 --dropout_rate2=0.4234025897486593 --dropout_rate3=0.5098146397818708 --hidden1=31 --hidden2=93 --hidden3=57 --learning_rate=0.007495422019370002 --time_step=7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvolcanic-sweep-22\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/h1q8pucz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130410-h1q8pucz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:04:12,992 - wandb.wandb_agent - INFO - Running runs: ['h1q8pucz']\n",
            "2021-08-20 13:04:12.733409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:12.740455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:12.741077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:12.742358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:12.742970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:12.743520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:13.371828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:13.372502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:13.373124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:13.373671: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:04:13.373725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:04:13.405174: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:04:13.405204: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:04:13.405239: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:04:13.535080: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:04:13.535256: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:04:13.698090: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:04:17.648925: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:59 - loss: 10.4981 - mse: 10.49812021-08-20 13:04:18.277228: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:04:18.277276: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 30s - loss: 8.2796 - mse: 8.2796   2021-08-20 13:04:18.530627: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:04:18.531101: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:04:18.679659: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 406 callback api events and 403 activity events. \n",
            "2021-08-20 13:04:18.686155: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:04:18.695990: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18\n",
            "\n",
            "2021-08-20 13:04:18.702773: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:04:18.717675: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18\n",
            "\n",
            "2021-08-20 13:04:18.720633: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:04:18.721160: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130410-h1q8pucz/files/train/plugins/profile/2021_08_20_13_04_18/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 5.7638 - mse: 5.7638WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_begin` time: 0.0403s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0550s). Check your callbacks.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "82/82 [==============================] - 6s 25ms/step - loss: 0.8440 - mse: 0.8440 - val_loss: 0.6070 - val_mse: 0.6070\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.2587 - mse: 0.2587 - val_loss: 0.4967 - val_mse: 0.4967\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.2050 - mse: 0.2050 - val_loss: 0.8589 - val_mse: 0.8589\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1735 - mse: 0.1735 - val_loss: 0.4833 - val_mse: 0.4833\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1742 - mse: 0.1742 - val_loss: 0.5608 - val_mse: 0.5608\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1554 - mse: 0.1554 - val_loss: 0.6176 - val_mse: 0.6176\n",
            "Epoch 7/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1384 - mse: 0.1384 - val_loss: 0.7393 - val_mse: 0.7393\n",
            "Epoch 8/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.1300 - mse: 0.1300 - val_loss: 0.7005 - val_mse: 0.7005\n",
            "Epoch 9/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.1180 - mse: 0.1180 - val_loss: 0.7179 - val_mse: 0.7179\n",
            "Epoch 10/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.4708 - val_mse: 0.4708\n",
            "Epoch 11/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.4512 - val_mse: 0.4512\n",
            "Epoch 12/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0791 - mse: 0.0791 - val_loss: 0.3939 - val_mse: 0.3939\n",
            "Epoch 13/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0714 - mse: 0.0714 - val_loss: 0.3654 - val_mse: 0.3654\n",
            "Epoch 14/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0687 - mse: 0.0687 - val_loss: 0.5415 - val_mse: 0.5415\n",
            "Epoch 15/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0566 - mse: 0.0566 - val_loss: 0.5684 - val_mse: 0.5684\n",
            "Epoch 16/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0526 - mse: 0.0526 - val_loss: 0.5952 - val_mse: 0.5952\n",
            "Epoch 17/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.4458 - val_mse: 0.4458\n",
            "Epoch 18/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.5468 - val_mse: 0.5468\n",
            "Epoch 19/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.4302 - val_mse: 0.4302\n",
            "Epoch 20/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.3862 - val_mse: 0.3862\n",
            "Epoch 21/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.4124 - val_mse: 0.4124\n",
            "Epoch 22/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.3771 - val_mse: 0.3771\n",
            "Epoch 23/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.3283 - val_mse: 0.3283\n",
            "Epoch 24/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.3368 - val_mse: 0.3368\n",
            "Epoch 25/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.3453 - val_mse: 0.3453\n",
            "Epoch 26/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.3287 - val_mse: 0.3287\n",
            "Epoch 27/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.3145 - val_mse: 0.3145\n",
            "Epoch 28/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.3434 - val_mse: 0.3434\n",
            "Epoch 29/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.3171 - val_mse: 0.3171\n",
            "Epoch 30/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.2913 - val_mse: 0.2913\n",
            "Epoch 31/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.2901 - val_mse: 0.2901\n",
            "Epoch 32/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.2448 - val_mse: 0.2448\n",
            "Epoch 33/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.2444 - val_mse: 0.2444\n",
            "Epoch 34/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.2697 - val_mse: 0.2697\n",
            "Epoch 35/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.2787 - val_mse: 0.2787\n",
            "Epoch 36/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.2370 - val_mse: 0.2370\n",
            "Epoch 37/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.2132 - val_mse: 0.2132\n",
            "Epoch 38/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.2268 - val_mse: 0.2268\n",
            "Epoch 39/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.2202 - val_mse: 0.2202\n",
            "Epoch 40/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.2171 - val_mse: 0.2171\n",
            "Epoch 41/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.2213 - val_mse: 0.2213\n",
            "Epoch 42/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.2382 - val_mse: 0.2382\n",
            "Epoch 43/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.2268 - val_mse: 0.2268\n",
            "Epoch 44/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.2549 - val_mse: 0.2549\n",
            "Epoch 45/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.2320 - val_mse: 0.2320\n",
            "Epoch 46/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.2930 - val_mse: 0.2930\n",
            "Epoch 47/200\n",
            "82/82 [==============================] - 1s 6ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.2822 - val_mse: 0.2822\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130410-h1q8pucz/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130410-h1q8pucz/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.01281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.01281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.28222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 0.28222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464685\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.21323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mvolcanic-sweep-22\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/h1q8pucz\u001b[0m\n",
            "2021-08-20 13:04:53,675 - wandb.wandb_agent - INFO - Cleaning up finished run: h1q8pucz\n",
            "2021-08-20 13:04:53,942 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:04:53,942 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.05842714070123656\n",
            "\tdropout_rate2: 0.5250143288964126\n",
            "\tdropout_rate3: 0.5647488743036022\n",
            "\thidden1: 10\n",
            "\thidden2: 109\n",
            "\thidden3: 106\n",
            "\tlearning_rate: 0.008051492007272391\n",
            "\ttime_step: 56\n",
            "2021-08-20 13:04:53,944 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.05842714070123656 --dropout_rate2=0.5250143288964126 --dropout_rate3=0.5647488743036022 --hidden1=10 --hidden2=109 --hidden3=106 --learning_rate=0.008051492007272391 --time_step=56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnorthern-sweep-23\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/g3m8rotu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130456-g3m8rotu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:04:58.479115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:58,953 - wandb.wandb_agent - INFO - Running runs: ['g3m8rotu']\n",
            "2021-08-20 13:04:58.486714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:58.487300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:58.488386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:58.489016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:58.489563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:59.075786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:59.076490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:59.077113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:04:59.077624: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:04:59.077682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:04:59.105815: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:04:59.105845: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:04:59.105885: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:04:59.247063: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:04:59.247249: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:04:59.414476: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:05:03.383983: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/81 [..............................] - ETA: 5:55 - loss: 9.6614 - mse: 9.66142021-08-20 13:05:03.998417: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:05:03.998460: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/81 [..............................] - ETA: 30s - loss: 5.0739 - mse: 5.0739 2021-08-20 13:05:04.256200: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:05:04.257014: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:05:04.405404: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 1066 callback api events and 1063 activity events. \n",
            "2021-08-20 13:05:04.418003: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:05:04.437168: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04\n",
            "\n",
            "2021-08-20 13:05:04.450204: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:05:04.468905: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04\n",
            "\n",
            "2021-08-20 13:05:04.471902: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:05:04.472541: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130456-g3m8rotu/files/train/plugins/profile/2021_08_20_13_05_04/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/81 [>.............................] - ETA: 24s - loss: 6.4437 - mse: 6.4437WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_begin` time: 0.0400s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 0.0619s). Check your callbacks.\n",
            "81/81 [==============================] - 7s 30ms/step - loss: 0.9078 - mse: 0.9078 - val_loss: 2.4981 - val_mse: 2.4981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.5920 - mse: 0.5920 - val_loss: 2.8640 - val_mse: 2.8640\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.5502 - mse: 0.5502 - val_loss: 2.6065 - val_mse: 2.6065\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.5699 - mse: 0.5699 - val_loss: 2.6931 - val_mse: 2.6931\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.5409 - mse: 0.5409 - val_loss: 2.4481 - val_mse: 2.4481\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.5049 - mse: 0.5049 - val_loss: 2.5322 - val_mse: 2.5322\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.4773 - mse: 0.4773 - val_loss: 2.2906 - val_mse: 2.2906\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.4564 - mse: 0.4564 - val_loss: 1.8798 - val_mse: 1.8798\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.2371 - mse: 0.2371 - val_loss: 1.7181 - val_mse: 1.7181\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.1358 - mse: 0.1358 - val_loss: 1.3347 - val_mse: 1.3347\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.1164 - mse: 0.1164 - val_loss: 1.2293 - val_mse: 1.2293\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 1s 11ms/step - loss: 0.0950 - mse: 0.0950 - val_loss: 1.1652 - val_mse: 1.1652\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.1013 - mse: 0.1013 - val_loss: 1.4806 - val_mse: 1.4806\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0815 - mse: 0.0815 - val_loss: 1.3563 - val_mse: 1.3563\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 1.3810 - val_mse: 1.3810\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0655 - mse: 0.0655 - val_loss: 1.1053 - val_mse: 1.1053\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.9870 - val_mse: 0.9870\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.9612 - val_mse: 0.9612\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0493 - mse: 0.0493 - val_loss: 1.3289 - val_mse: 1.3289\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 1.2134 - val_mse: 1.2134\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 1.1085 - val_mse: 1.1085\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 1.1339 - val_mse: 1.1339\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 1.2115 - val_mse: 1.2115\n",
            "Epoch 24/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 1.3002 - val_mse: 1.3002\n",
            "Epoch 25/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 1.1082 - val_mse: 1.1082\n",
            "Epoch 26/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 1.1319 - val_mse: 1.1319\n",
            "Epoch 27/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 1.1069 - val_mse: 1.1069\n",
            "Epoch 28/200\n",
            "81/81 [==============================] - 1s 12ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 1.0908 - val_mse: 1.0908\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5199\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210820_130456-g3m8rotu/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210820_130456-g3m8rotu/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.03204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mse 0.03204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 1.09078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val_mse 1.09078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1629464731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.96117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mse ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_mse ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 12 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mnorthern-sweep-23\u001b[0m: \u001b[34mhttps://wandb.ai/kuggle/gamestop_prediction/runs/g3m8rotu\u001b[0m\n",
            "2021-08-20 13:05:39,655 - wandb.wandb_agent - INFO - Cleaning up finished run: g3m8rotu\n",
            "2021-08-20 13:05:39,911 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-08-20 13:05:39,912 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tdropout_rate1: 0.25316346772682535\n",
            "\tdropout_rate2: 0.5544245182267123\n",
            "\tdropout_rate3: 0.4762884405474954\n",
            "\thidden1: 72\n",
            "\thidden2: 103\n",
            "\thidden3: 48\n",
            "\tlearning_rate: 0.05124451982971986\n",
            "\ttime_step: 15\n",
            "2021-08-20 13:05:39,913 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --dropout_rate1=0.25316346772682535 --dropout_rate2=0.5544245182267123 --dropout_rate3=0.4762884405474954 --hidden1=72 --hidden2=103 --hidden3=48 --learning_rate=0.05124451982971986 --time_step=15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkade\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworthy-sweep-24\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/sweeps/1ng3wx9s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kuggle/gamestop_prediction/runs/isj0tezw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210820_130542-isj0tezw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-08-20 13:05:44.509837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:44.516783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:44.517415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:44,922 - wandb.wandb_agent - INFO - Running runs: ['isj0tezw']\n",
            "2021-08-20 13:05:44.518634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:44.519232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:44.519804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:45.139565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:45.140255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:45.140827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-20 13:05:45.141343: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-20 13:05:45.141399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14886 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-08-20 13:05:45.171020: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:05:45.171060: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2021-08-20 13:05:45.171144: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
            "2021-08-20 13:05:45.321848: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:05:45.322027: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021-08-20 13:05:45.486988: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "Epoch 1/200\n",
            "2021-08-20 13:05:49.388813: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            " 1/82 [..............................] - ETA: 5:53 - loss: 10.7324 - mse: 10.73242021-08-20 13:05:49.993088: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2021-08-20 13:05:49.993127: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            " 2/82 [..............................] - ETA: 31s - loss: 7.7058 - mse: 7.7058   2021-08-20 13:05:50.256059: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-20 13:05:50.256516: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
            "2021-08-20 13:05:50.407853: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 488 callback api events and 485 activity events. \n",
            "2021-08-20 13:05:50.415050: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
            "2021-08-20 13:05:50.425414: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50\n",
            "\n",
            "2021-08-20 13:05:50.432701: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.trace.json.gz\n",
            "2021-08-20 13:05:50.447418: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50\n",
            "\n",
            "2021-08-20 13:05:50.450171: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.memory_profile.json.gz\n",
            "2021-08-20 13:05:50.450687: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50\n",
            "Dumped tool data for xplane.pb to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/wandb/run-20210820_130542-isj0tezw/files/train/plugins/profile/2021_08_20_13_05_50/359ee3b7c60b.kernel_stats.pb\n",
            "\n",
            " 3/82 [>.............................] - ETA: 23s - loss: 5.8759 - mse: 5.8759WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_begin` time: 0.0417s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_end` time: 0.0559s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 24ms/step - loss: 0.9558 - mse: 0.9558 - val_loss: 2.4321 - val_mse: 2.4321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
            "Epoch 2/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.5466 - mse: 0.5466 - val_loss: 2.2614 - val_mse: 2.2614\n",
            "Epoch 3/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.4098 - mse: 0.4098 - val_loss: 2.5856 - val_mse: 2.5856\n",
            "Epoch 4/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3874 - mse: 0.3874 - val_loss: 2.4742 - val_mse: 2.4742\n",
            "Epoch 5/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3831 - mse: 0.3831 - val_loss: 2.2858 - val_mse: 2.2858\n",
            "Epoch 6/200\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 0.3553 - mse: 0.3553 - val_loss: 2.2976 - val_mse: 2.2976\n",
            "Epoch 7/200\n",
            "29/82 [=========>....................] - ETA: 0s - loss: 0.3816 - mse: 0.3816"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AmdEHkKNm5n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}